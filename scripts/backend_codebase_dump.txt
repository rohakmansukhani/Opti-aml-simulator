# Codebase Dump: backend
# Generated from: /Users/rohakmansukhani/Documents/Coding/OptiMoney/sas simulator/backend


================================================================================
FILE: backend/auth.py
================================================================================

import os
from fastapi import Request, HTTPException, status
from jose import jwt, jwk
import requests
from functools import lru_cache

from datetime import datetime, timedelta

# Cache JWKS with 1-hour TTL
_jwks_cache = {"data": None, "expires_at": None}

def get_jwks():
    now = datetime.now()
    if _jwks_cache["data"] and _jwks_cache["expires_at"] and _jwks_cache["expires_at"] > now:
        return _jwks_cache["data"]

    supabase_url = os.environ.get("SUPABASE_URL")
    if not supabase_url:
        raise ValueError("SUPABASE_URL not set")
    
    # JWKS endpoint logic
    jwks_url = f"{supabase_url}/auth/v1/.well-known/jwks.json"
    try:
        response = requests.get(jwks_url)
        response.raise_for_status()
        data = response.json()
        
        # Update Cache
        _jwks_cache["data"] = data
        _jwks_cache["expires_at"] = now + timedelta(hours=1)
        
        return data
    except Exception as e:
        print(f"Failed to fetch JWKS: {e}")
        raise e

async def get_current_user_token(request: Request):
    """
    Validates Supabase JWT using JWKS (Asymmetric Keys).
    Expects: Authorization: Bearer <token>
    Returns: (user_payload, raw_token)
    """
    auth_header = request.headers.get("Authorization")
    if not auth_header:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Missing Authorization Header")

    try:
        scheme, token = auth_header.split(" ")
        if scheme.lower() != "bearer":
            raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Invalid Auth Scheme")
    except ValueError:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Invalid Auth Format")

    # 1. Get Key ID (kid) from header
    try:
        headers = jwt.get_unverified_headers(token)
    except Exception as e:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Invalid Token Headers")
        
    kid = headers.get("kid")
    if not kid:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Missing 'kid' in token")

    # 2. Fetch JWKS from Supabase
    try:
        jwks = get_jwks()
    except Exception as e:
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Auth Provider Unavailable")
    
    # 3. Find matching key
    key_data = next((k for k in jwks["keys"] if k["kid"] == kid), None)
    if not key_data:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Invalid Key ID")
    
    # 4. Verify & Decode
    try:
        # Supabase uses ES256. python-jose handles JWK dicts directly.
        alg = headers.get("alg", "RS256")
        
        payload = jwt.decode(
            token, 
            key_data, # Use the JWK dict directly
            algorithms=[alg],
            options={
                "verify_aud": False,
                "verify_sub": True,
                "verify_iat": True,
                "verify_exp": True
            } 
        )
        return payload, token
    except jwt.ExpiredSignatureError:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, "Token has expired")
    except jwt.JWTClaimsError as e:
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, f"Invalid token claims: {str(e)}")
    except Exception as e:
        # If it's a signature error, let's log the key info (safely)
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, f"Token Verification Failed: {str(e)}")

async def get_current_user(request: Request):
    # Convenience wrapper returning just the payload
    payload, _ = await get_current_user_token(request)
    return payload


================================================================================
FILE: backend/tasks.py
================================================================================

"""
Celery Tasks - Background job processing for long-running simulations

Purpose:
    Run simulations asynchronously to avoid blocking HTTP requests
    
Why Celery?
    - Simulations take 10-45 seconds
    - HTTP requests would timeout
    - User gets instant response, simulation runs in background
    
Flow:
    1. User clicks "Run Simulation"
    2. API creates run record, returns run_id immediately
    3. Celery worker picks up task from Redis queue
    4. Simulation executes in background
    5. User polls /status endpoint for progress
"""

from celery import Celery
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import os
import structlog

# Celery app configuration
app = Celery('sas_simulator')
app.conf.broker_url = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
app.conf.result_backend = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
app.conf.task_serializer = 'json'
app.conf.result_serializer = 'json'
app.conf.accept_content = ['json']
app.conf.timezone = 'UTC'
app.conf.enable_utc = True

logger = structlog.get_logger("celery_tasks")


# Global engine and sessionmaker for reuse across tasks
_engine = None
_SessionFactory = None

def get_db_session():
    """
    Create database session for Celery tasks.
    Reuses the global engine to prevent connection pool exhaustion.
    """
    global _engine, _SessionFactory
    
    if _engine is None:
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            raise ValueError("DATABASE_URL environment variable is not set")
            
        # Create engine with production-ready settings
        _engine = create_engine(
            database_url,
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        _SessionFactory = sessionmaker(autocommit=False, autoflush=False, bind=_engine)
        
    return _SessionFactory()


@app.task(bind=True, name='tasks.run_simulation_background')
def run_simulation_background(
    self, 
    run_id: str, 
    scenarios: list, 
    date_range: dict = None,
    field_mappings: dict = None
):
    """
    Background task for running simulations.
    
    Args:
        self: Celery task instance (for progress updates)
        run_id: Simulation run ID
        scenarios: List of scenario IDs to execute
        date_range: Optional date filtering
        field_mappings: Optional field name mappings
        
    Returns:
        {
            "status": "completed" | "failed",
            "run_id": str,
            "total_alerts": int (if successful),
            "error": str (if failed)
        }
    """
    logger.info(
        "simulation_task_started",
        run_id=run_id,
        scenarios=scenarios,
        task_id=self.request.id
    )
    
    try:
        # Import here to avoid circular dependencies
        from services.simulation_service import SimulationService
        
        # Create database session
        db = get_db_session()
        
        try:
            # Update task state to STARTED
            self.update_state(
                state='STARTED',
                meta={'run_id': run_id, 'progress': 0}
            )
            
            # Execute simulation
            service = SimulationService(db)
            
            # Update progress: 25% - Loading data
            self.update_state(
                state='PROGRESS',
                meta={'run_id': run_id, 'progress': 25, 'stage': 'Loading data'}
            )
            
            # Execute simulation (this is the long-running part)
            result = service.execute_run(
                run_id=run_id,
                scenarios=scenarios,
                date_range=date_range,
                field_mappings=field_mappings
            )
            
            # Update progress: 100% - Complete
            self.update_state(
                state='SUCCESS',
                meta={'run_id': run_id, 'progress': 100, 'stage': 'Complete'}
            )
            
            logger.info(
                "simulation_task_completed",
                run_id=run_id,
                total_alerts=result.get('total_alerts', 0)
            )
            
            return {
                "status": "completed",
                "run_id": run_id,
                "total_alerts": result.get('total_alerts', 0),
                "total_transactions": result.get('total_transactions', 0)
            }
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(
            "simulation_task_failed",
            run_id=run_id,
            error=str(e),
            exc_info=True
        )
        
        # Update task state to FAILURE
        self.update_state(
            state='FAILURE',
            meta={'run_id': run_id, 'error': str(e)}
        )
        
        return {
            "status": "failed",
            "run_id": run_id,
            "error": str(e)
        }


@app.task(name='tasks.cleanup_expired_data')
def cleanup_expired_data():
    """
    Periodic task to cleanup expired TTL data.
    
    Run this via Celery Beat (scheduler):
        celery -A tasks beat --schedule=/tmp/celerybeat-schedule
    """
    logger.info("cleanup_task_started")
    
    try:
        from core.ttl_manager import TTLManager
        
        db = get_db_session()
        
        try:
            result = TTLManager.cleanup_expired(db)
            
            logger.info(
                "cleanup_task_completed",
                transactions_deleted=result['transactions_deleted'],
                customers_deleted=result['customers_deleted']
            )
            
            return result
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error("cleanup_task_failed", error=str(e), exc_info=True)
        raise


# Celery Beat schedule (periodic tasks)
app.conf.beat_schedule = {
    'cleanup-expired-data-daily': {
        'task': 'tasks.cleanup_expired_data',
        'schedule': 86400.0,  # Run every 24 hours
    },
}


================================================================================
FILE: backend/models.py
================================================================================

from sqlalchemy import Column, String, Integer, DateTime, Boolean, DECIMAL, Text, ForeignKey, JSON, Float, Index, ForeignKeyConstraint
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import UUID
import uuid
import datetime
from datetime import datetime as dt, timezone
from database import Base

# Helper for consistent UTC timestamps
def utc_now():
    return dt.now(timezone.utc)

class Transaction(Base):
    """
    Schema-agnostic transaction model.
    All user CSV data is stored in raw_data JSONB.
    """
    __tablename__ = "transactions"
    
    # System columns only
    transaction_id = Column(String, primary_key=True)
    customer_id = Column(String, ForeignKey("customers.customer_id"), nullable=False, index=True)
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id"), nullable=True, index=True)
    created_at = Column(DateTime(timezone=True), default=utc_now)  # UTC
    expires_at = Column(DateTime(timezone=True), nullable=True, index=True)  # UTC (TTL)
    
    # All user CSV data stored here
    raw_data = Column(JSON, nullable=False, default={})

    # Relationships
    customer = relationship("Customer", back_populates="transactions")
    alert_transactions = relationship("AlertTransaction", back_populates="transaction")  # ✅ ADDED

class Customer(Base):
    """
    Schema-agnostic customer model.
    All user CSV data is stored in raw_data JSONB.
    """
    __tablename__ = "customers"
    
    customer_id = Column(String, primary_key=True)
    upload_id = Column(UUID(as_uuid=True), ForeignKey('data_uploads.upload_id'), nullable=True, index=True)
    raw_data = Column(JSON, nullable=False, default={})
    expires_at = Column(DateTime(timezone=True), nullable=True, index=True)
    created_at = Column(DateTime(timezone=True), default=utc_now)

    # Keep other relationships that work:
    transactions = relationship("Transaction", back_populates="customer")
    alerts = relationship("Alert", back_populates="customer")

class Alert(Base):
    __tablename__ = "alerts"

    alert_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    customer_id = Column(String, ForeignKey("customers.customer_id"), nullable=True)  # Nullable for anonymization
    customer_name = Column(String)
    scenario_id = Column(String, ForeignKey("scenarios_config.scenario_id"))
    scenario_name = Column(String)
    scenario_description = Column(Text)
    alert_date = Column(DateTime(timezone=True), nullable=False, index=True)  # UTC
    alert_status = Column(String, default="OPN")
    trigger_details = Column(JSON)
    risk_classification = Column(String)
    risk_score = Column(Integer)
    run_id = Column(String, ForeignKey("simulation_runs.run_id"), nullable=False, index=True)
    excluded = Column(Boolean, default=False)
    exclusion_reason = Column(Text)
    created_at = Column(DateTime(timezone=True), default=utc_now)  # UTC
    updated_at = Column(DateTime(timezone=True), default=utc_now, onupdate=utc_now)

    # Investigation Workflow
    assigned_to = Column(String, nullable=True) # User ID
    investigation_status = Column(String, default='New') # New, In Progress, Closed
    outcome = Column(String, nullable=True) # False Positive, True Positive, Suspicious
    sar_reference = Column(String, nullable=True)
    investigation_notes = Column(Text, nullable=True)
    is_anonymized = Column(Boolean, default=False, index=True)
    anonymized_at = Column(DateTime(timezone=True), nullable=True)

    customer = relationship("Customer", back_populates="alerts")
    simulation_run = relationship("SimulationRun", back_populates="alerts")
    alert_transactions = relationship("AlertTransaction", back_populates="alert")  # ✅ ADDED

class UserProfile(Base):
    __tablename__ = "profiles"
    
    id = Column(String, primary_key=True) # Linked to auth.users.id
    email = Column(String, unique=True, nullable=False)
    full_name = Column(String)
    avatar_url = Column(String)
    organization_id = Column(String)
    role = Column(String, default="analyst") # admin, analyst, viewer
    created_at = Column(DateTime, default=utc_now)
    updated_at = Column(DateTime, default=utc_now)

class ScenarioConfig(Base):
    __tablename__ = "scenarios_config"

    scenario_id = Column(String, primary_key=True)
    user_id = Column(UUID(as_uuid=True), ForeignKey("profiles.id"), index=True, nullable=True) # Owner
    scenario_name = Column(String, nullable=False)
    description = Column(Text)
    priority = Column(String, nullable=True)  # ✅ ADD THIS LINE
    frequency = Column(String)
    transaction_types = Column(JSON)
    channels = Column(JSON)
    direction = Column(String)
    thresholds = Column(JSON)
    lookback_days = Column(Integer)
    aggregation_logic = Column(String)
    enabled = Column(Boolean, default=True)
    refinements = Column(JSON)
    config_json = Column(JSON) 
    field_mappings = Column(JSON, nullable=True)
    updated_at = Column(DateTime, default=utc_now)

class SimulationRun(Base):
    __tablename__ = "simulation_runs"

    run_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(UUID(as_uuid=True), ForeignKey("profiles.id"), index=True, nullable=True)  # UUID type
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id"), nullable=True, index=True)  # Added
    run_type = Column(String)
    scenarios_run = Column(JSON)
    date_range_start = Column(DateTime(timezone=False))
    date_range_end = Column(DateTime(timezone=False))
    total_transactions = Column(Integer, default=0)
    total_alerts = Column(Integer)
    status = Column(String)
    progress_percentage = Column(Integer, default=0)
    created_at = Column(DateTime(timezone=False), default=utc_now)
    completed_at = Column(DateTime(timezone=False))
    metadata_info = Column(JSON, nullable=True)

    alerts = relationship("Alert", back_populates="simulation_run")

class VerifiedEntity(Base):
    __tablename__ = "verified_entities"
    
    entity_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(UUID(as_uuid=True), ForeignKey("profiles.id"), index=True, nullable=True)  # UUID type
    entity_name = Column(String, index=True)
    entity_type = Column(String)
    country = Column(String)
    risk_category = Column(String)
    is_active = Column(Boolean, default=True)
    valid_from = Column(DateTime(timezone=False), default=utc_now)
    valid_to = Column(DateTime(timezone=False), nullable=True)

class AuditLog(Base):
    __tablename__ = "audit_logs"
    
    log_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    timestamp = Column(DateTime, default=utc_now)
    user_id = Column(String)
    action_type = Column(String) # create_refinement, approve_rule, deploy_rule
    target_entity_id = Column(String) # e.g. scenario_id
    details = Column(JSON)
    risk_score_snapshot = Column(Float, nullable=True)
    justification_notes = Column(String, nullable=True)

class AlertExclusionLog(Base):
    __tablename__ = "alert_exclusion_logs"
    
    log_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    alert_id = Column(String, ForeignKey("alerts.alert_id"))
    exclusion_timestamp = Column(DateTime, default=utc_now)
    rule_id = Column(String, nullable=True)
    exclusion_reason = Column(String)
    risk_flags = Column(JSON) # Snapshot of risk indicators at time of exclusion

class CustomerRiskProfile(Base):
    __tablename__ = "customer_risk_profiles"
    
    profile_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    customer_id = Column(String, ForeignKey("customers.customer_id"))
    is_pep = Column(Boolean, default=False)
    high_risk_occupation = Column(Boolean, default=False)
    has_adverse_media = Column(Boolean, default=False)
    previous_sar_count = Column(Integer, default=0)
    account_age_days = Column(Integer, default=0)
    last_updated = Column(DateTime, default=utc_now)
    
    customer = relationship("Customer")

class DataUpload(Base):
    __tablename__ = "data_uploads"
    
    upload_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)  # UUID type
    user_id = Column(UUID(as_uuid=True), ForeignKey("profiles.id"), nullable=True)  # UUID type
    upload_timestamp = Column(DateTime(timezone=True), default=lambda: datetime.datetime.now(datetime.timezone.utc))
    filename = Column(String)
    record_count_transactions = Column(Integer)
    record_count_customers = Column(Integer)
    expires_at = Column(DateTime(timezone=True))  # Timezone-aware
    status = Column(String, default="active")

class Account(Base):
    __tablename__ = "accounts"

    account_id = Column(String, primary_key=True)
    customer_id = Column(String, ForeignKey("customers.customer_id", ondelete="CASCADE"), nullable=False, index=True)
    account_number = Column(String)
    account_type = Column(String)  # Savings, Current, NRI, Loan, FD
    account_status = Column(String, default='Active')  # Active, Dormant, Closed
    currency_code = Column(String, default='GBP')
    
    # Dates
    account_open_date = Column(DateTime(timezone=False), nullable=False, index=True)
    account_close_date = Column(DateTime(timezone=False))
    last_transaction_date = Column(DateTime(timezone=False))
    
    # Risk & Compliance
    risk_rating = Column(Text)  # LOW, MEDIUM, HIGH
    is_pep = Column(Boolean, default=False)
    
    # Balances
    current_balance = Column(DECIMAL(18, 2))
    average_monthly_balance = Column(DECIMAL(18, 2))
    
    # Metadata
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id"), index=True)
    created_at = Column(DateTime, default=utc_now)
    expires_at = Column(DateTime(timezone=True))
    
    # Schema-agnostic storage
    raw_data = Column(JSON, default={}, nullable=False)

class FieldValueIndex(Base):
    __tablename__ = "field_value_index"

    index_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id", ondelete="CASCADE"), nullable=False)
    table_name = Column(String, nullable=False)  # 'transactions' or 'customers'
    field_name = Column(String, nullable=False, index=True)
    field_value = Column(String, nullable=False)
    value_count = Column(Integer, default=1)
    value_percentage = Column(DECIMAL(5, 2))
    first_seen = Column(DateTime, default=utc_now)
    last_seen = Column(DateTime, default=utc_now)

    __table_args__ = (
        Index('idx_field_value_upload_table', 'upload_id', 'table_name'),
        Index('idx_field_value_search', 'field_name', 'field_value'),
        Index('idx_field_value_count', 'field_name', 'value_count'),
    )

class FieldMetadata(Base):
    __tablename__ = "field_metadata"

    metadata_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id", ondelete="CASCADE"), nullable=False)
    table_name = Column(String, nullable=False)
    field_name = Column(String, nullable=False)
    field_type = Column(String, nullable=False)  # 'text', 'numeric', 'date', 'boolean'
    
    # Statistics
    total_records = Column(Integer)
    non_null_count = Column(Integer)
    null_count = Column(Integer)
    distinct_count = Column(Integer)
    
    # Numeric stats
    min_value = Column(DECIMAL(18, 2))
    max_value = Column(DECIMAL(18, 2))
    avg_value = Column(DECIMAL(18, 2))
    
    # Recommendations
    recommended_operators = Column(JSON)
    sample_values = Column(JSON)
    
    created_at = Column(DateTime, default=utc_now)

    __table_args__ = (
        Index('idx_field_metadata_upload', 'upload_id', 'table_name'),
    )

class AlertTransaction(Base):
    __tablename__ = "alert_transactions"

    alert_id = Column(String, ForeignKey("alerts.alert_id", ondelete="CASCADE"), primary_key=True)
    transaction_id = Column(String, primary_key=True) # Part of composite FK
    upload_id = Column(UUID(as_uuid=True), primary_key=True) # Added for Partitioning Support
    
    contribution_percentage = Column(DECIMAL(5, 2))
    
    __table_args__ = (
        ForeignKeyConstraint(
            ['transaction_id', 'upload_id'],
            ['transactions.transaction_id', 'transactions.upload_id'],
            ondelete="CASCADE"
        ),
    )
    is_primary_trigger = Column(Boolean, default=False)
    sequence_order = Column(Integer)

    alert = relationship("Alert", back_populates="alert_transactions")
    transaction = relationship("Transaction", back_populates="alert_transactions")

# Phase 4: Intelligence & Comparison Models

class SimulationComparison(Base):
    __tablename__ = "simulation_comparisons"
    
    comparison_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(UUID(as_uuid=True), ForeignKey("profiles.id", ondelete="CASCADE"), index=True)
    base_run_id = Column(String, ForeignKey("simulation_runs.run_id", ondelete="CASCADE"), nullable=False)
    challenger_run_id = Column(String, ForeignKey("simulation_runs.run_id", ondelete="CASCADE"), nullable=False)
    
    # Metrics
    alerts_delta = Column(Integer) # challenger - base
    efficiency_score = Column(Float) # Efficiency gain/loss
    overlap_count = Column(Integer) # Common alerts
    
    # Store detailed comparison JSON (New Alerts, Dropped Alerts, etc.)
    comparison_details = Column(JSON)
    
    created_at = Column(DateTime, default=utc_now)

class BeneficiaryHistory(Base):
    __tablename__ = "beneficiary_history"
    
    history_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    beneficiary_name = Column(String, index=True) # Normalized name
    beneficiary_account = Column(String, index=True, nullable=True) # Account/IBAN if available
    
    # Let's scope to Data Upload -> User so we don't leak data across tenants
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id", ondelete="CASCADE"), index=True)
    
    # Usage Stats
    total_transactions = Column(Integer, default=0)
    total_amount = Column(DECIMAL(18, 2), default=0)
    first_seen = Column(DateTime, default=utc_now)
    last_seen = Column(DateTime, default=utc_now)
    
    # Risk Indicators (for Stability Checks)
    std_dev_amount = Column(Float, nullable=True)
    avg_days_between_txns = Column(Float, nullable=True)
    
    __table_args__ = (
        Index('idx_beneficiary_upload', 'upload_id', 'beneficiary_name'),
    )

class ComparisonReport(Base):
    """
    Stores advanced aggregation reports (e.g. Rolling Window analysis results).
    """
    __tablename__ = "comparison_reports"
    
    report_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    run_id = Column(String, ForeignKey("simulation_runs.run_id", ondelete="CASCADE"))
    report_type = Column(String) # 'TIME_CLUSTERING', 'ROLLING_WINDOW', 'BENEFICIARY_RISK'
    report_data = Column(JSON)
    created_at = Column(DateTime, default=utc_now)

# Relationships
SimulationRun.comparisons_base = relationship("SimulationComparison", foreign_keys=[SimulationComparison.base_run_id])
SimulationRun.comparisons_challenger = relationship("SimulationComparison", foreign_keys=[SimulationComparison.challenger_run_id])

# Phase 5: Advanced Features Models

class ScenarioVersion(Base):
    __tablename__ = "scenario_versions"
    
    version_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    scenario_id = Column(String, ForeignKey("scenarios_config.scenario_id", ondelete="CASCADE"), nullable=False, index=True)
    version_number = Column(Integer, nullable=False) # 1, 2, 3...
    
    # Snapshot of the config at this version
    config_snapshot = Column(JSON) 
    change_description = Column(String)
    changed_by = Column(UUID(as_uuid=True), ForeignKey("profiles.id"), nullable=True)
    created_at = Column(DateTime, default=utc_now)
    
class DataQualityMetric(Base):
    __tablename__ = "data_quality_metrics"
    
    metric_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    upload_id = Column(UUID(as_uuid=True), ForeignKey("data_uploads.upload_id", ondelete="CASCADE"), nullable=False, index=True)
    
    # Overall Score
    completeness_score = Column(Float) # % of non-null required fields
    validity_score = Column(Float) # % of valid formats
    uniqueness_score = Column(Float) # % of unique IDs
    
    # Detailed Report
    field_level_issues = Column(JSON) # {"transaction_amount": {"nulls": 50, "negatives": 2}}
    row_level_issues = Column(JSON) # Sample of bad rows
    
    created_at = Column(DateTime, default=utc_now)

# Global Relationships
ScenarioConfig.versions = relationship("ScenarioVersion", backref="scenario", order_by="desc(ScenarioVersion.version_number)")
DataUpload.quality_metrics = relationship("DataQualityMetric", uselist=False, backref="upload")


================================================================================
FILE: backend/database.py
================================================================================

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from fastapi import Header, HTTPException
import os
from tempfile import mkdtemp
from dotenv import load_dotenv

load_dotenv()

# Global cache for engines
# Key: db_url, Value: sessionmaker
_engine_cache = {}

Base = declarative_base()

# Enforce Supabase PostgreSQL - No SQLite fallback
DEFAULT_DB_URL = os.getenv("DATABASE_URL", "").strip()
if not DEFAULT_DB_URL:
    raise RuntimeError("DATABASE_URL environment variable is required. Please configure Supabase connection.")

# For initial boot/migrations where no request context exists
def get_default_engine():
    return _get_engine(DEFAULT_DB_URL)

def _get_engine(db_url: str):
    if not db_url:
         raise HTTPException(status_code=500, detail="Database URL not configured.")

    if db_url not in _engine_cache:
        try:
            # PostgreSQL connection with pool_pre_ping for resilience
            engine = create_engine(db_url, pool_pre_ping=True, pool_size=10, max_overflow=20)
            _engine_cache[db_url] = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid Database Connection String: {str(e)}")
            
    return _engine_cache[db_url]

# Service Role support for background tasks (Bypass RLS)
SERVICE_ROLE_URL = os.getenv("DATABASE_URL_SERVICE_ROLE")

def get_service_engine():
    """Returns engine with SERVICE ROLE key - bypasses RLS for system operations."""
    target_url = SERVICE_ROLE_URL or DEFAULT_DB_URL
    
    if "service_engine" not in _engine_cache:
        try:
            engine = create_engine(target_url, pool_pre_ping=True, pool_size=5, max_overflow=10)
            _engine_cache["service_engine"] = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        except Exception as e:
             print(f"Service role engine init failed: {e}")
             return get_default_engine()

    return _engine_cache["service_engine"]

def resolve_db_url(url_or_alias: str) -> str:
    """Helper to resolve 'local' alias to Supabase PostgreSQL."""
    if not url_or_alias or url_or_alias == "default" or url_or_alias == "local":
        return DEFAULT_DB_URL
    return url_or_alias

def get_db(x_db_url: str = Header(None)):
    """
    Dependency that gets DB session based on header.
    """
    if x_db_url is None:
        x_db_url = "local"
        
    target_url = resolve_db_url(x_db_url)
    
    SessionLocal = _get_engine(target_url)
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Expose global engine/SessionLocal for scripts that import them directly
# ensuring they use the default fallback
engine = get_default_engine().kw['bind']
SessionLocal = get_default_engine()


================================================================================
FILE: backend/main.py
================================================================================

from fastapi import FastAPI, Request, Depends, Response
from sqlalchemy.orm import Session
from sqlalchemy import text
from database import engine, Base, get_db
from models import (
    Transaction, Customer, Alert, ScenarioConfig, 
    VerifiedEntity, AuditLog, AlertExclusionLog, 
    UserProfile, CustomerRiskProfile, SimulationRun
)
import os
import time
import structlog
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Structured logging configuration
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger("sas_simulator")

# Create database tables
Base.metadata.create_all(bind=engine)

from api import data, simulation, comparison, rules, risk, dashboard, admin, validation, fields, investigation

# Rate limiter
limiter = Limiter(key_func=get_remote_address, default_limits=["200/minute"])

app = FastAPI(
    title="SAS Sandbox Simulator API",
    version="1.0.0",
    description="Enterprise-grade AML/CFT scenario simulation platform"
)

# Add rate limiter to app state
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

from fastapi.middleware.cors import CORSMiddleware

# Production CORS configuration
# We split by comma and strip whitespace to be extra safe
DEFAULT_ORIGINS = "http://localhost:3000,https://opti-aml-simulator.vercel.app"
ALLOWED_ORIGINS = [
    o.strip() for o in os.getenv("ALLOWED_ORIGINS", DEFAULT_ORIGINS).split(",")
    if o.strip()
]

# Auto-allow Vercel preview/production URLs if they contain the specific patterns
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"], # Ensure custom headers like x-db-url are visible
)

# Request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    logger.info(
        "http_request",
        method=request.method,
        path=request.url.path,
        status_code=response.status_code,
        duration_ms=round(duration * 1000, 2),
        client_ip=get_remote_address(request)
    )
    
    return response

app.include_router(data.router)
app.include_router(simulation.router)
app.include_router(comparison.router)
app.include_router(rules.router)
app.include_router(risk.router)
app.include_router(dashboard.router)
app.include_router(validation.router)
app.include_router(admin.router)  # Admin endpoints for TTL management
app.include_router(fields.router) # Field Discovery & Autocomplete
app.include_router(investigation.router) # Investigation & Traceability



@app.get("/")
async def root():
    return {"message": "SAS Sandbox Simulator API is running", "version": "1.0.0"}

@app.get("/health")
async def health_check(response: Response, db: Session = Depends(get_db)):
    """
    Comprehensive health check endpoint
    
    Checks:
    1. Upstash Redis connection
    2. PostgreSQL database connection
    3. Supabase Auth service
    
    Returns:
        {
            "status": "healthy" | "degraded" | "unhealthy",
            "checks": {
                "redis": {...},
                "database": {...},
                "auth": {...}
            }
        }
    """
    import redis
    from supabase import create_client
    
    checks = {}
    overall_status = "healthy"
    
    # 1. Check Upstash Redis
    try:
        redis_url = os.getenv("REDIS_URL")
        if redis_url:
            r = redis.from_url(redis_url, socket_connect_timeout=5)
            r.ping()
            checks["redis"] = {
                "status": "healthy",
                "message": "Connected to Redis",
                "url": redis_url.split("@")[1] if "@" in redis_url else "localhost"
            }
        else:
            checks["redis"] = {
                "status": "warning",
                "message": "Redis URL not configured"
            }
            overall_status = "degraded"
    except Exception as e:
        checks["redis"] = {
            "status": "unhealthy",
            "message": f"Redis connection failed: {str(e)}"
        }
        overall_status = "unhealthy"
    
    # 2. Check PostgreSQL Database
    try:
        # Simple query to test connection
        db.execute(text("SELECT 1"))
        checks["database"] = {
            "status": "healthy",
            "message": "Database connected",
            "type": "PostgreSQL"
        }
    except Exception as e:
        checks["database"] = {
            "status": "unhealthy",
            "message": f"Database connection failed: {str(e)}"
        }
        overall_status = "unhealthy"
    
    # 3. Check Supabase Auth
    try:
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_ANON_KEY")
        
        if supabase_url and supabase_key:
            supabase = create_client(supabase_url, supabase_key)
            # Test auth service by checking if it's reachable
            # Note: This doesn't actually authenticate, just checks service availability
            checks["auth"] = {
                "status": "healthy",
                "message": "Supabase Auth configured",
                "url": supabase_url
            }
        else:
            checks["auth"] = {
                "status": "warning",
                "message": "Supabase Auth not configured"
            }
            if overall_status == "healthy":
                overall_status = "degraded"
    except Exception as e:
        checks["auth"] = {
            "status": "unhealthy",
            "message": f"Auth service check failed: {str(e)}"
        }
        overall_status = "unhealthy"
    
    # Set appropriate HTTP status code
    if overall_status == "unhealthy":
        response.status_code = 503
    elif overall_status == "degraded":
        response.status_code = 200  # Still 200 OK but with warnings
    
    logger.info(
        "health_check_completed",
        status=overall_status,
        redis=checks.get("redis", {}).get("status"),
        database=checks.get("database", {}).get("status"),
        auth=checks.get("auth", {}).get("status")
    )
    
    return {
        "status": overall_status,
        "service": "sas-sandbox-simulator",
        "version": "1.0.0",
        "environment": os.getenv("ENVIRONMENT", "development"),
        "checks": checks
    }

# Prometheus metrics endpoint
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from fastapi.responses import Response

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint for monitoring"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

from pydantic import BaseModel
class ConnectionRequest(BaseModel):
    db_url: str

@app.post("/api/connect")
async def test_connection(request: ConnectionRequest):
    """
    Validates that a provided database URL can establish a connection.
    Frontend should call this before storing the URL in session/localstorage.
    Only PostgreSQL connections are supported.
    """
    from sqlalchemy import create_engine, text
    import logging
    
    logger = logging.getLogger(__name__)
    
    # Validate URL format - only accept PostgreSQL
    if not request.db_url.startswith("postgresql://") and not request.db_url.startswith("postgres://"):
        return {
            "status": "failed", 
            "message": "Only PostgreSQL databases are supported. URL must start with 'postgresql://' or 'postgres://'"
        }
    
    try:
        # Try connecting with a short timeout
        engine = create_engine(
            request.db_url, 
            connect_args={"connect_timeout": 5}
        )
        with engine.connect() as connection:
            connection.execute(text("SELECT 1"))
        return {"status": "connected", "message": "Connection Successful"}
    except Exception as e:
        # Log detailed error server-side
        logger.error(f"Database connection failed: {str(e)}")
        
        # Return generic error to client
        return {
            "status": "failed", 
            "message": "Unable to connect to database. Please verify your connection string and ensure the database is accessible."
        }



================================================================================
FILE: backend/tasks/cleanup_cron.py
================================================================================

"""
Cleanup Cron Job - Automatically deletes expired data

This script should be run periodically (e.g., every hour) via cron or a task scheduler.
For Supabase, this is configured via pg_cron extension.
"""

import sys
import os

# Add backend to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database import get_default_engine
from sqlalchemy.orm import sessionmaker
from core.ttl_manager import TTLManager
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def run_cleanup():
    """Execute cleanup of expired data"""
    logger.info("Starting TTL cleanup job...")
    
    SessionLocal = get_default_engine()
    db = SessionLocal()
    
    try:
        result = TTLManager.cleanup_expired(db)
        logger.info(f"Cleanup completed: {result}")
        return result
    except Exception as e:
        logger.error(f"Cleanup failed: {e}")
        raise
    finally:
        db.close()


if __name__ == "__main__":
    run_cleanup()


================================================================================
FILE: backend/core/upload_validator.py
================================================================================

"""
Upload Validator - Enforces size limits for CSV/Excel uploads
"""

import pandas as pd
from typing import Dict, Any


class UploadValidator:
    """Validates upload size and provides recommendations for large datasets"""
    
    MAX_RECORDS_TRANSACTIONS = 10000
    MAX_RECORDS_CUSTOMERS = 10000
    
    @classmethod
    def validate_size(cls, df: pd.DataFrame, data_type: str = "transactions") -> Dict[str, Any]:
        """
        Validates if a DataFrame is within acceptable size limits.
        
        Args:
            df: DataFrame to validate
            data_type: Type of data ("transactions" or "customers")
            
        Returns:
            dict with keys:
                - allowed (bool): Whether upload is permitted
                - count (int): Number of records
                - message (str): User-facing message
                - recommendation (str): Action to take if rejected
        """
        record_count = len(df)
        max_records = (cls.MAX_RECORDS_TRANSACTIONS if data_type == "transactions" 
                      else cls.MAX_RECORDS_CUSTOMERS)
        
        if record_count >= max_records:
            return {
                "allowed": False,
                "count": record_count,
                "max_allowed": max_records,
                "message": (
                    f"Dataset too large: {record_count:,} records exceeds limit of {max_records:,}. "
                    f"Please connect your own database for large datasets."
                ),
                "recommendation": "connect_external_db"
            }
        
        return {
            "allowed": True,
            "count": record_count,
            "max_allowed": max_records,
            "message": f"Upload validated: {record_count:,} records"
        }
    
    @classmethod
    def estimate_from_file_size(cls, file_size_bytes: int, data_type: str = "transactions") -> Dict[str, Any]:
        """
        Estimates record count from file size for early validation.
        
        Args:
            file_size_bytes: Size of uploaded file in bytes
            data_type: Type of data
            
        Returns:
            dict with estimated count and pre-validation result
        """
        # Rough estimate: ~200 bytes per transaction record, ~150 bytes per customer record
        bytes_per_record = 200 if data_type == "transactions" else 150
        estimated_count = file_size_bytes // bytes_per_record
        
        max_records = (cls.MAX_RECORDS_TRANSACTIONS if data_type == "transactions" 
                      else cls.MAX_RECORDS_CUSTOMERS)
        
        return {
            "estimated_count": estimated_count,
            "likely_too_large": estimated_count >= max_records,
            "file_size_mb": round(file_size_bytes / (1024 * 1024), 2)
        }


================================================================================
FILE: backend/core/rate_limiting.py
================================================================================

"""
Enhanced Rate Limiting with Cost-Based Budgets

Implements tiered rate limiting based on endpoint cost:
- Expensive operations (simulations): 3/hour
- Moderate operations (uploads): 5/hour  
- Cheap operations (queries): 200/minute
"""

from slowapi import Limiter
from slowapi.util import get_remote_address
from functools import wraps
from typing import Callable
import redis
import os
from datetime import datetime, timedelta

# Initialize Redis for distributed rate limiting
redis_client = redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379"))

# Cost map for different endpoints
ENDPOINT_COSTS = {
    "/api/simulation/run": 100,           # Very expensive
    "/api/comparison/compare": 50,        # Expensive
    "/api/data/upload/transactions": 30,  # Moderate
    "/api/data/upload/customers": 30,     # Moderate
    "/api/scenario-config/create": 20,    # Moderate
    "/api/risk/analyze": 40,              # Expensive
    "/api/dashboard/stats": 5,            # Cheap
    "/api/rules/scenarios": 1,            # Very cheap
}

DAILY_BUDGET = 1000  # Total cost units per user per day

class CostBasedRateLimiter:
    """
    Rate limiter that tracks cumulative cost instead of just request count.
    
    Example:
        - User runs 10 simulations (10 * 100 = 1000 cost) → budget exhausted
        - User makes 200 dashboard queries (200 * 5 = 1000 cost) → budget exhausted
        - Mix: 5 simulations (500) + 100 queries (500) = 1000 → budget exhausted
    """
    
    @staticmethod
    def check_budget(user_id: str, endpoint: str) -> tuple[bool, dict]:
        """
        Check if user has enough budget for this endpoint.
        
        Returns:
            (allowed, info) where info contains current usage and remaining budget
        """
        cost = ENDPOINT_COSTS.get(endpoint, 1)
        key = f"rate_limit:cost:{user_id}:{datetime.utcnow().strftime('%Y-%m-%d')}"
        
        # Get current usage
        current_usage = redis_client.get(key)
        current_usage = int(current_usage) if current_usage else 0
        
        # Check if adding this cost would exceed budget
        new_usage = current_usage + cost
        allowed = new_usage <= DAILY_BUDGET
        
        if allowed:
            # Increment usage
            redis_client.setex(key, timedelta(days=1), new_usage)
        
        return allowed, {
            "current_usage": new_usage if allowed else current_usage,
            "daily_budget": DAILY_BUDGET,
            "remaining_budget": max(0, DAILY_BUDGET - (new_usage if allowed else current_usage)),
            "endpoint_cost": cost,
            "reset_at": (datetime.utcnow() + timedelta(days=1)).replace(hour=0, minute=0, second=0).isoformat()
        }
    
    @staticmethod
    def get_usage_stats(user_id: str) -> dict:
        """Get current usage statistics for a user."""
        key = f"rate_limit:cost:{user_id}:{datetime.utcnow().strftime('%Y-%m-%d')}"
        current_usage = redis_client.get(key)
        current_usage = int(current_usage) if current_usage else 0
        
        return {
            "current_usage": current_usage,
            "daily_budget": DAILY_BUDGET,
            "remaining_budget": max(0, DAILY_BUDGET - current_usage),
            "usage_percentage": (current_usage / DAILY_BUDGET) * 100,
            "reset_at": (datetime.utcnow() + timedelta(days=1)).replace(hour=0, minute=0, second=0).isoformat()
        }


def cost_limited(endpoint: str):
    """
    Decorator to apply cost-based rate limiting to an endpoint.
    
    Usage:
        @router.post("/run")
        @cost_limited("/api/simulation/run")
        async def run_simulation(...):
            pass
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Extract user_id from kwargs (injected by get_current_user dependency)
            current_user = kwargs.get('current_user')
            if not current_user:
                # If no user context, allow (for public endpoints)
                return await func(*args, **kwargs)
            
            user_id = current_user.get('sub') or current_user.get('user_id')
            
            # Check budget
            allowed, info = CostBasedRateLimiter.check_budget(user_id, endpoint)
            
            if not allowed:
                from fastapi import HTTPException
                raise HTTPException(
                    status_code=429,
                    detail={
                        "error": "Rate limit exceeded",
                        "message": f"Daily budget exhausted. Resets at {info['reset_at']}",
                        "usage_info": info
                    }
                )
            
            # Add usage info to response headers
            response = await func(*args, **kwargs)
            
            # If response is a Response object, add headers
            if hasattr(response, 'headers'):
                response.headers['X-RateLimit-Remaining'] = str(info['remaining_budget'])
                response.headers['X-RateLimit-Reset'] = info['reset_at']
            
            return response
        
        return wrapper
    return decorator


# Standard limiter for simple rate limiting
limiter = Limiter(
    key_func=get_remote_address,
    default_limits=["200/minute"],
    storage_uri=os.getenv("REDIS_URL", "memory://")
)


================================================================================
FILE: backend/core/field_mapper.py
================================================================================

import pandas as pd
from typing import Dict, Optional

def apply_field_mappings_to_df(df: pd.DataFrame, mappings: Optional[Dict[str, str]]) -> pd.DataFrame:
    """
    Renames DataFrame columns based on mapping configuration.
    
    Args:
        df: Original DataFrame with user's column names
        mappings: Dict mapping scenario fields -> actual DB columns
                  Example: {'transaction_amount': 'tx_amt'}
                  
    Returns:
        DataFrame with renamed columns.
        If mappings is None or empty, returns original DataFrame.
    """
    if not mappings:
        return df
    
    # The 'mappings' dict is typically: { 'system_field': 'user_column' }
    # e.g. { 'transaction_amount': 'amount_usd', 'customer_id': 'cust_id' }
    # But check the direction. 
    # Usually: Application expects 'transaction_amount'.
    # User provides CSV with 'amount_usd'.
    # So we want to rename 'amount_usd' -> 'transaction_amount'.
    
    # Reverse mapping: User Column -> System Field
    reverse_map = {v: k for k, v in mappings.items()}
    
    # Only rename columns that exist in the DataFrame
    # This prevents errors if a mapping key doesn't exist in the specific DF (e.g. if partial)
    rename_dict = {col: reverse_map[col] for col in df.columns if col in reverse_map}
    
    if rename_dict:
        # Create a copy to avoid modifying original
        df_mapped = df.copy()
        
        # Drop target columns if they already exist (to prevent duplicates)
        target_cols = list(rename_dict.values())
        existing_targets = [col for col in target_cols if col in df_mapped.columns]
        if existing_targets:
            print(f"[FIELD_MAPPER] Dropping existing columns to prevent duplicates: {existing_targets}")
            df_mapped = df_mapped.drop(columns=existing_targets)
        
        # Now safely rename
        df_mapped = df_mapped.rename(columns=rename_dict)
        return df_mapped
        
    return df


================================================================================
FILE: backend/core/ttl_manager.py
================================================================================

"""
TTL Manager - Handles automatic data expiration and cleanup
"""

from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
from sqlalchemy import text
from typing import Optional
import uuid
import json
import structlog

logger = structlog.get_logger("ttl_manager")


class TTLManager:
    """Manages Time-To-Live for uploaded data"""
    
    DEFAULT_TTL_HOURS = 48
    MAX_TTL_HOURS = 168  # 7 days
    
    @staticmethod
    def set_expiry(hours: int = DEFAULT_TTL_HOURS) -> datetime:
        """
        Calculate expiry timestamp (timezone-aware).
        
        Args:
            hours: Number of hours until expiration
            
        Returns:
            timezone-aware datetime object representing expiry time
        """
        return datetime.now(timezone.utc) + timedelta(hours=hours)
    
    @staticmethod
    def create_upload_record(
        db: Session,
        user_id: str,  # UUID as string from JWT
        filename: str,
        txn_count: int,
        cust_count: int,
        schema_snapshot: dict,
        ttl_hours: int = DEFAULT_TTL_HOURS
    ) -> uuid.UUID:
        """
        Create a new upload metadata record.
        
        Returns:
            upload_id (UUID object)
        """
        upload_id = uuid.uuid4()  # Native UUID object
        expires_at = TTLManager.set_expiry(ttl_hours)
        
        # Serialize schema to JSON string
        schema_json = json.dumps(schema_snapshot)
        
        # Insert into data_uploads table
        query = text("""
            INSERT INTO data_uploads 
            (upload_id, user_id, filename, record_count_transactions, 
             record_count_customers, schema_snapshot, expires_at, status)
            VALUES 
            (:upload_id, :user_id, :filename, :txn_count, 
             :cust_count, CAST(:schema AS jsonb), :expires_at, 'active')
        """)
        
        try:
            db.execute(query, {
                "upload_id": upload_id,
                "user_id": user_id,  # Pass as string, cast to UUID in SQL
                "filename": filename,
                "txn_count": txn_count,
                "cust_count": cust_count,
                "schema": schema_json,
                "expires_at": expires_at
            })
            logger.info("upload_record_created", upload_id=str(upload_id), user_id=user_id)
        except Exception as e:
            logger.error("upload_record_creation_failed", error=str(e), upload_id=str(upload_id))
            raise
        
        # REMOVED: db.commit() - Let the calling function handle the transaction
        return upload_id
    
    @staticmethod
    def extend_ttl(db: Session, upload_id: str, additional_hours: int = 24) -> bool:
        """
        Extend TTL for an existing upload.
        
        Args:
            db: Database session
            upload_id: Upload to extend
            additional_hours: Hours to add
            
        Returns:
            bool indicating success
        """
        # Atomic Update to prevent race conditions
        # We update data_uploads first and get the new expiry, then sync other tables
        
        try:
            # Postgres: Update expires_at by adding interval
            update_query = text("""
                UPDATE data_uploads 
                SET expires_at = expires_at + (:hours * interval '1 hour')
                WHERE upload_id = :id
                RETURNING expires_at
            """)
            
            result = db.execute(update_query, {"hours": additional_hours, "id": upload_id}).fetchone()
            
            if not result:
                logger.warning("extend_ttl_failed_not_found", upload_id=upload_id)
                return False
                
            new_expiry = result[0]
            
            # Cap at Max TTL (if new_expiry exceeds limit, force update it back)
            now = datetime.now(timezone.utc)
            max_allowed = now + timedelta(hours=TTLManager.MAX_TTL_HOURS)
            
            if new_expiry.tzinfo is None:
                new_expiry = new_expiry.replace(tzinfo=timezone.utc)
                
            if new_expiry > max_allowed:
                new_expiry = max_allowed
                # Force update to cap
                db.execute(
                    text("UPDATE data_uploads SET expires_at = :cap WHERE upload_id = :id"),
                    {"cap": new_expiry, "id": upload_id}
                )
            
            # Sync child tables
            db.execute(
                text("""
                    UPDATE transactions SET expires_at = :new_expiry WHERE upload_id = :id;
                    UPDATE customers SET expires_at = :new_expiry WHERE upload_id = :id;
                """),
                {"new_expiry": new_expiry, "id": upload_id}
            )
            
            db.commit()
            logger.info("ttl_extended", upload_id=upload_id, new_expiry=new_expiry.isoformat())
            return True
            
        except Exception as e:
            db.rollback()
            logger.error("ttl_extension_error", error=str(e))
            raise e
    
    @staticmethod
    def cleanup_expired(db: Session, dry_run: bool = False) -> dict:
        """
        Delete expired PII data while preserving anonymized alerts.
        
        Critical Flow:
        1. Anonymize alerts BEFORE deleting customers (preserve customer_name)
        2. Delete transactions (raw PII)
        3. Delete customers (FK cascade sets alert.customer_id = NULL)
        4. Mark data_uploads as expired
        
        Args:
            db: Database session
            dry_run: If True, rollback instead of commit (for testing)
        
        Returns:
            dict with cleanup statistics including anonymized alert count
        """
        now = datetime.now(timezone.utc)
        
        logger.info("cleanup_started", timestamp=now.isoformat(), dry_run=dry_run)
        
        # Get expired upload IDs
        expired_uploads = db.execute(
            text("SELECT upload_id FROM data_uploads WHERE expires_at < :now AND status = 'active'"),
            {"now": now}
        ).fetchall()
        
        expired_ids = [str(row[0]) for row in expired_uploads]
        
        if not expired_ids:
            logger.info("cleanup_no_expired_data")
            return {
                "alerts_anonymized": 0,
                "transactions_deleted": 0,
                "customers_deleted": 0,
                "uploads_expired": 0,
                "timestamp": now.isoformat(),
                "dry_run": dry_run
            }
        
        # STEP 1: Anonymize alerts BEFORE deleting customers
        # This preserves customer_name while removing PII linkage
        anonymize_result = db.execute(
            text("""
                UPDATE alerts
                SET 
                    customer_name = 'ANONYMIZED-' || SUBSTRING(customer_id, 1, 8),
                    is_anonymized = true,
                    anonymized_at = :now,
                    trigger_details = jsonb_set(
                        COALESCE(trigger_details, '{}'::jsonb),
                        '{pii_removed}',
                        'true'::jsonb
                    )
                WHERE customer_id IN (
                    SELECT customer_id FROM customers WHERE upload_id = ANY(:ids::uuid[])
                )
                AND is_anonymized = false
            """),
            {"now": now, "ids": expired_ids}
        )
        alerts_anonymized = anonymize_result.rowcount
        
        # STEP 2: Delete transactions (raw PII)
        txn_result = db.execute(
            text("DELETE FROM transactions WHERE upload_id = ANY(:ids::uuid[])"),
            {"ids": expired_ids}
        )
        transactions_deleted = txn_result.rowcount
        
        # STEP 3: Delete customers (FK cascade sets alert.customer_id = NULL)
        cust_result = db.execute(
            text("DELETE FROM customers WHERE upload_id = ANY(:ids::uuid[])"),
            {"ids": expired_ids}
        )
        customers_deleted = cust_result.rowcount
        
        # STEP 4: Mark uploads as expired
        upload_result = db.execute(
            text("UPDATE data_uploads SET status = 'expired' WHERE upload_id = ANY(:ids::uuid[])"),
            {"ids": expired_ids}
        )
        uploads_expired = upload_result.rowcount
        
        result = {
            "alerts_anonymized": alerts_anonymized,
            "transactions_deleted": transactions_deleted,
            "customers_deleted": customers_deleted,
            "uploads_expired": uploads_expired,
            "upload_ids_processed": expired_ids,
            "timestamp": now.isoformat(),
            "dry_run": dry_run
        }
        
        if dry_run:
            db.rollback()
            logger.info("cleanup_dry_run_completed", **result)
        else:
            db.commit()
            logger.info("cleanup_completed", **result)
        
        return result



================================================================================
FILE: backend/core/smart_layer.py
================================================================================

import pandas as pd
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from models import VerifiedEntity, AlertExclusionLog
from datetime import datetime
import uuid

class EventDetector:
    """Detects legitimate events in transactions with Context Awareness"""
    
    # ... keywords same as before ...
    EDUCATION_KEYWORDS = [
        'university', 'tuition', 'education', 'school', 
        'college', 'student fee', 'semester'
    ]
    LOAN_KEYWORDS = ['loan', 'emi', 'mortgage', 'repayment', 'installment', 'financing']
    FIXED_DEPOSIT_KEYWORDS = ['fixed deposit', 'fd', 'term deposit', 'investment', 'fd maturity']
    SALARY_KEYWORDS = ['salary', 'payroll', 'wages', 'compensation', 'monthly income']

    def __init__(self, db: Session):
        self.db = db

    def is_verified_entity(self, entity_name: str, entity_type: str) -> bool:
        """Check if entity is in verified whitelist"""
        if not entity_name:
            return False
            
        # Basic fuzzy match or direct match
        # In prod: use localized search or vector search
        # Here: simple ILIKE
        entity = self.db.query(VerifiedEntity).filter(
            VerifiedEntity.entity_name.ilike(f"%{entity_name}%"),
            VerifiedEntity.entity_type == entity_type,
            VerifiedEntity.is_active == True
        ).first()
        
        return entity is not None

    def detect_event_context(self, narrative: str, amount: float, beneficiary: str) -> Optional[Dict]:
        """
        Detect event type and validate context (Amount, Beneficiary).
        Returns dict with event_type and verification status.
        """
        if not narrative:
            return None
        
        narrative_lower = str(narrative).lower()
        event_type = None
        
        if any(kw in narrative_lower for kw in self.EDUCATION_KEYWORDS):
            event_type = 'education'
        elif any(kw in narrative_lower for kw in self.LOAN_KEYWORDS):
            event_type = 'loan'
        elif any(kw in narrative_lower for kw in self.FIXED_DEPOSIT_KEYWORDS):
            event_type = 'fixed_deposit'
            
        if not event_type:
            return None

        # Context Checks
        is_verified = False
        amount_reasonable = True # Default true until logic added
        
        if event_type == 'education':
            # Check verified university
            is_verified = self.is_verified_entity(beneficiary, 'University')
            # Amount check: Tuition usually < 50k
            if amount > 50000:
                amount_reasonable = False
        
        if event_type == 'loan':
            is_verified = self.is_verified_entity(beneficiary, 'FinancialInstitution')
            
        return {
            "type": event_type,
            "is_verified": is_verified,
            "amount_reasonable": amount_reasonable
        }

class SmartLayerProcessor:
    """Main smart layer orchestrator 2.0"""
    
    def __init__(self, db: Session):
        self.db = db
        self.event_detector = EventDetector(db)
    
    def _get_trigger_window_transactions(
        self,
        alert: pd.Series,
        transactions: pd.DataFrame,
        lookback_days: int = 30
    ) -> pd.DataFrame:
        """
        Filter transactions to only those within the alert's trigger window.
        Uses alert_date and lookback period to get precise window.
        """
        customer_id = alert['customer_id']
        alert_date = pd.to_datetime(alert.get('alert_date', datetime.utcnow()))
        
        # Calculate trigger window
        window_start = alert_date - pd.Timedelta(days=lookback_days)
        window_end = alert_date
        
        # Filter to customer and date range
        trigger_txns = transactions[
            (transactions['customer_id'] == customer_id) &
            (pd.to_datetime(transactions['transaction_date']) >= window_start) &
            (pd.to_datetime(transactions['transaction_date']) <= window_end)
        ]
        
        return trigger_txns
    
    def _write_exclusion_log(
        self,
        alert_id: str,
        rule_id: str,
        exclusion_reason: str,
        risk_flags: Dict
    ):
        """Write AlertExclusionLog entry when excluding an alert"""
        try:
            log_entry = AlertExclusionLog(
                log_id=str(uuid.uuid4()),
                alert_id=alert_id,
                exclusion_timestamp=datetime.utcnow(),
                rule_id=rule_id,
                exclusion_reason=exclusion_reason,
                risk_flags=risk_flags  # Compact snapshot of risk indicators
            )
            self.db.add(log_entry)
            self.db.commit()
        except Exception as e:
            self.db.rollback()
            print(f"Failed to write exclusion log: {e}")
    
    def apply_refinements(
        self,
        alerts: pd.DataFrame,
        transactions: pd.DataFrame,
        refinement_rules: List[Dict],
        lookback_days: int = 30
    ) -> pd.DataFrame:
        """Apply all refinement rules to alerts with context checks (VECTORIZED)"""
        
        if alerts.empty:
            return alerts
            
        if 'excluded' not in alerts.columns:
            alerts['excluded'] = False
        if 'exclusion_reason' not in alerts.columns:
            alerts['exclusion_reason'] = None
        
        # VECTORIZED APPROACH: Process all alerts at once instead of iterrows
        for rule in refinement_rules:
            rule_id = rule.get('rule_id', 'unknown')
            
            if rule['type'] == 'event_based':
                excluded_events = rule.get('excluded_events', [])
                
                # Build keyword pattern for all excluded events
                event_keywords = {
                    'education': ['tuition', 'university', 'college', 'school'],
                    'crypto': ['crypto', 'bitcoin', 'binance', 'coinbase'],
                    'loan': ['loan', 'mortgage', 'credit'],
                }
                
                # Combine all keywords for excluded events
                all_keywords = []
                for event in excluded_events:
                    all_keywords.extend(event_keywords.get(event, []))
                
                if not all_keywords:
                    continue
                
                # Vectorized: Find all transactions matching keywords
                # Optimization: Pre-compile regex
                import re
                keyword_pattern = '|'.join(map(re.escape, all_keywords)) # Escape to prevent regex injection errors
                compiled_regex = re.compile(keyword_pattern, re.IGNORECASE)
                
                matching_txns = transactions[
                    transactions['transaction_narrative'].str.contains(
                        compiled_regex, 
                        na=False,
                        regex=True
                    )
                ].copy()
                
                if matching_txns.empty:
                    continue
                
                # Vectorized: Get customer IDs with matching transactions
                customers_with_matches = set(matching_txns['customer_id'].unique())
                
                # Vectorized: Mark alerts for these customers (not yet excluded)
                mask = (
                    alerts['customer_id'].isin(customers_with_matches) &
                    (~alerts['excluded'])
                )
                
                # For alerts that match, we need to verify context
                # This part still needs row-level logic but only for matched subset
                matched_alerts = alerts[mask].copy()
                
                for idx in matched_alerts.index:
                    alert = alerts.loc[idx]
                    
                    # Get trigger window transactions
                    trigger_txns = self._get_trigger_window_transactions(
                        alert, 
                        transactions, 
                        lookback_days
                    )
                    
                    should_exclude = False
                    exclusion_reason = None
                    risk_flags = {}
                    
                    # Check transactions in trigger window
                    for _, txn in trigger_txns.iterrows():
                        narrative = txn.get('transaction_narrative')
                        amount = txn.get('transaction_amount', 0)
                        beneficiary = txn.get('beneficiary_name')
                        
                        context = self.event_detector.detect_event_context(narrative, amount, beneficiary)
                        
                        if context and context['type'] in excluded_events:
                            # Build risk flags snapshot
                            risk_flags = {
                                'event_type': context['type'],
                                'is_verified': context['is_verified'],
                                'amount_reasonable': context['amount_reasonable'],
                                'beneficiary': beneficiary,
                                'amount': float(amount)
                            }
                            
                            if context['type'] == 'education':
                                if context['is_verified'] and context['amount_reasonable']:
                                    should_exclude = True
                                    exclusion_reason = f"Verified {context['type']} transaction to {beneficiary}"
                            else:
                                should_exclude = True
                                exclusion_reason = f"Legitimate {context['type']} transaction"
                            
                            if should_exclude:
                                break
                    
                    if should_exclude:
                        alerts.at[idx, 'excluded'] = True
                        alerts.at[idx, 'exclusion_reason'] = exclusion_reason
                        
                        # Write exclusion log
                        alert_id = alert.get('alert_id')
                        if alert_id:
                            self._write_exclusion_log(
                                alert_id=alert_id,
                                rule_id=rule_id,
                                exclusion_reason=exclusion_reason,
                                risk_flags=risk_flags
                            )
        
        return alerts


================================================================================
FILE: backend/core/__init__.py
================================================================================



================================================================================
FILE: backend/core/risk_engine.py
================================================================================

from typing import List, Dict, Any, Optional
import pandas as pd
from sqlalchemy.orm import Session
from models import Alert, Customer, Transaction, VerifiedEntity, CustomerRiskProfile
import random
import logging

logger = logging.getLogger(__name__)

class RiskEngine:
    """
    Risk Engine 2.0: Advanced Security & Gap Analysis Module.
    
    This engine is responsible for "Red Teaming" proposed rule refinements.
    It analyzes alerts that would be DROPPED by a new rule and calculates a
    'Risk Score' to determine if safe, legitimate alerts are being suppressed.
    """

    ML_SCENARIOS = {
        "structuring": {
            "name": "Structuring / Smurfing",
            "risk_level": "HIGH",
            "indicators": ["small_amount", "high_frequency"],
            "base_coverage": 0.95
        },
        "education_fraud": {
            "name": "Education / Tuition Fraud",
            "risk_level": "HIGH",
            "indicators": ["education_keywords", "unverified_beneficiary"],
            "base_coverage": 0.85
        },
        "loan_circularity": {
            "name": "Loan / FD Circularity",
            "risk_level": "MEDIUM",
            "indicators": ["loan_keywords", "fd_keywords", "circular_pattern"],
            "base_coverage": 0.80
        }
    }

    def __init__(self, db: Session):
        self.db = db

    def analyze_risk_gap(self, refinements: List[Dict], baseline_run_id: str, user_id: str = None) -> Dict[str, Any]:
        """
        Analyzes the security gap introduced by proposed refinements.

        Args:
            refinements (List[Dict]): A list of refinement rules (e.g., "Exclude Education").
            baseline_run_id (str): The ID of the baseline simulation to compare against.

        Returns:
            Dict containing:
            - risk_score (0-100)
            - risk_level (SAFE, CAUTION, DANGEROUS, CRITICAL)
            - sample_exploits (List of potential exploit vectors)
        """
        
        # 1. Fetch Baseline Alerts
        # We need to know what alerts existed BEFORE the refinement to see what gets dropped.
        baseline_alerts_query = self.db.query(Alert).filter(Alert.run_id == baseline_run_id)
        if baseline_alerts_query.count() == 0:
            return {
                "risk_score": 0, 
                "risk_level": "SAFE", 
                "explanations": ["No baseline alerts to analyze"], 
                "excluded_count": 0, 
                "sample_exploits": []
            }

        baseline_alerts_df = pd.read_sql(baseline_alerts_query.statement, self.db.bind)
        
        excluded_alerts = []
        risk_score_total = 0.0
        
        # 2. Simulate Refinement Impact
        for rule in refinements:
            if rule['type'] == 'event_based':
                excluded_events = rule.get('excluded_events', [])
                
                for _, alert in baseline_alerts_df.iterrows():
                    narrative = alert.get('alert_description', '').lower()
                    
                    # Normalize trigger details
                    details = alert.get('trigger_details', {})
                    if isinstance(details, str):
                        import json
                        try:
                            details = json.loads(details)
                        except:
                            details = {}
                    
                    alert_data = alert.to_dict()
                    alert_data['trigger_details'] = details
                    
                    is_excluded = False
                    exclusion_reason = ""
                    
                    # Check if this alert would be caught by the refinement rule
                    if 'education' in excluded_events and ('tuition' in narrative or 'university' in narrative):
                        is_excluded = True
                        exclusion_reason = "Education Exclusion"
                    
                    if is_excluded:
                        # 3. Calculate Risk of Exclusion
                        # If we exclude this, are we opening a loophole?
                        alert_risk = self._calculate_alert_risk(alert_data, exclusion_reason, user_id=user_id)
                        risk_score_total += alert_risk['score']
                        excluded_alerts.append({
                            "alert_id": str(alert['alert_id']),
                            "reason": exclusion_reason,
                            "risk_analysis": alert_risk
                        })
                        
        # 4. Normalize Score
        # Cap at 100. Logic: Summing individual risks can go high, we need a bounded score.
        normalized_score = min(risk_score_total, 100.0)
        
        risk_level = "SAFE"
        if normalized_score > 60:
            risk_level = "CRITICAL"
        elif normalized_score > 30:
            risk_level = "DANGEROUS"
        elif normalized_score > 0:
            risk_level = "CAUTION"

        return {
            "risk_score": round(normalized_score, 1),
            "risk_level": risk_level,
            "excluded_count": len(excluded_alerts),
            "sample_exploits": self._generate_sample_exploits(excluded_alerts)
        }
    
    def analyze_excluded_alerts(self, excluded_alerts: List[Dict], user_id: str = None) -> Dict[str, Any]:
        """
        Analyzes actual excluded alerts from a simulation run.
        
        Args:
            excluded_alerts (List[Dict]): List of alerts that were excluded
            user_id (str): The ID of the current user
            
        Returns:
            Dict containing:
            - risk_score (0-100)
            - risk_level ('SAFE' | 'CAUTION' | 'DANGEROUS' | 'CRITICAL')
            - sample_exploits (List of potential exploit vectors)
        """
        if not excluded_alerts:
            return {
                "risk_score": 0.0,
                "risk_level": "SAFE",
                "excluded_count": 0,
                "sample_exploits": []
            }
        
        risk_score_total = 0.0
        analyzed_alerts = []
        
        for alert in excluded_alerts:
            exclusion_reason = alert.get('exclusion_reason', 'Unknown')
            alert_risk = self._calculate_alert_risk(alert, exclusion_reason, user_id=user_id)
            risk_score_total += alert_risk['score']
            
            analyzed_alerts.append({
                "alert_id": str(alert.get('alert_id', '')),
                "reason": exclusion_reason,
                "risk_analysis": alert_risk
            })
        
        # Normalize score (cap at 100)
        normalized_score = min(risk_score_total, 100.0)
        
        # Determine risk level
        risk_level = "SAFE"
        if normalized_score > 60:
            risk_level = "CRITICAL"
        elif normalized_score > 30:
            risk_level = "DANGEROUS"
        elif normalized_score > 0:
            risk_level = "CAUTION"
        
        return {
            "risk_score": round(normalized_score, 1),
            "risk_level": risk_level,
            "excluded_count": len(excluded_alerts),
            "sample_exploits": self._generate_sample_exploits(analyzed_alerts)
        }

    def _calculate_alert_risk(self, alert_data: Dict[str, Any], exclusion_reason: str, user_id: str = None) -> Dict[str, Any]:
        """
        Scoring Engine for a single excluded alert.
        
        Factors:
        1. Amount: Higher amount = Higher Risk (if excluded).
        2. Beneficiary: Unverified beneficiaries typically indicate fraud.
        3. Customer: PEPs or those with Adverse Media are high risk.
        """
        score = 0.0
        factors = []
        
        # Factor A: Amount Reasonability
        details = alert_data.get('trigger_details', {})
        amount_str = details.get('aggregated_value', details.get('transaction_amount', '0'))
        try:
            amount = float(amount_str)
        except:
            amount = 0.0

        reason_lower = exclusion_reason.lower()
        
        if "education" in reason_lower:
            # Tuition fees > 50k are suspicious
            if amount > 50000:
                score += 15.0
                factors.append(f"High amount for Education: {amount}")
        elif "crypto" in reason_lower:
             # Crypto > 10k is suspicious
            if amount > 10000:
                score += 20.0
                factors.append(f"High amount for Crypto: {amount}")

        # Factor B: Beneficiary Verification
        ben_name = details.get('beneficiary_name')
        if ben_name:
            expected_type = None
            if "education" in reason_lower: expected_type = 'University'
            elif "crypto" in reason_lower: expected_type = 'CryptoExchange'
            
            if expected_type:
                # Check Database for Whitelist (Scoped to User if provided)
                query = self.db.query(VerifiedEntity).filter(
                    VerifiedEntity.entity_name.ilike(f"%{ben_name}%"),
                    VerifiedEntity.entity_type == expected_type,
                    VerifiedEntity.is_active == True
                )
                
                if user_id:
                    from sqlalchemy import or_
                    # Match user-specific whitelist OR a global one (user_id=None)
                    query = query.filter(or_(VerifiedEntity.user_id == user_id, VerifiedEntity.user_id.is_(None)))
                
                verified = query.first()
                
                if not verified:
                    score += 25.0
                    factors.append(f"Unverified {expected_type} beneficiary: {ben_name}")
                else:
                    score -= 5.0 # Trusted entity reduces risk
            
        # Factor C: Customer Risk Profile
        customer_id = alert_data.get('customer_id')
        if customer_id:
            # Note: CustomerRiskProfile is linked to Customer, which is scoped by upload_id -> user_id
            # However, for defense in depth, we can check if the customer belongs to the user
            # or simply assume the data loader pre-filtered correctly.
            # We'll rely on the existing relationship but filter by profile_id
            profile = self.db.query(CustomerRiskProfile).filter(CustomerRiskProfile.customer_id == customer_id).first()
            if profile:
                if profile.is_pep:
                    score += 25.0
                    factors.append("Customer is PEP")
                if profile.has_adverse_media:
                    score += 20.0
                    factors.append("Adverse Media found")
                if profile.high_risk_occupation:
                    score += 10.0
                    factors.append("High Risk Occupation")
                if profile.previous_sar_count > 0:
                    score += (10.0 * profile.previous_sar_count)
                    factors.append(f"{profile.previous_sar_count} Previous SARs")

        return {"score": max(0.0, score), "factors": factors}

    def _generate_sample_exploits(self, excluded_alerts: List[Dict]) -> List[Dict]:
        """
        Translates raw technical risk data into "Exploit Stories" for the UI.
        e.g., "Attacker could structure funds using 'Education' payments."
        """
        # Sort by risk score descending
        sorted_alerts = sorted(excluded_alerts, key=lambda x: x['risk_analysis']['score'], reverse=True)
        
        exploits = []
        for item in sorted_alerts[:3]: # Take Top 3 Riskiest exclusions
            factors = item['risk_analysis']['factors']
            exploits.append({
                "title": f"Exploit: {factors[0] if factors else 'Generic Gap'}",
                "method": f"Excluded by rule '{item['reason']}'. {', '.join(factors)}",
                "volume_risk": "High"
            })
        return exploits


================================================================================
FILE: backend/core/README.md
================================================================================

# Backend Core Modules

This directory contains the core business logic and processing engines for the SAS Sandbox Simulator.

## 📁 Module Structure

```
core/
├── universal_engine.py    # Main scenario execution engine
├── smart_layer.py          # Alert refinement & exclusion logic
├── risk_engine.py          # Risk scoring & gap analysis
├── event_detector.py       # Transaction context detection
├── ttl_manager.py          # Data lifecycle management
└── config_models.py        # Pydantic configuration schemas
```

## 🔧 Core Components

### 1. Universal Scenario Engine
**File:** `universal_engine.py`

**Purpose:** Executes AML/CFT scenarios against transaction data to generate alerts.

**Key Features:**
- Config-driven execution (no hardcoded rules)
- Supports multiple aggregation types (SUM, COUNT, MAX)
- Dynamic threshold evaluation
- Rolling window calculations (daily, monthly)
- Generates structured alert objects

**Main Class:** `UniversalScenarioEngine`

**Key Methods:**
```python
execute_scenarios(
    scenarios: List[Dict],
    transactions_df: pd.DataFrame,
    customers_df: pd.DataFrame
) -> pd.DataFrame
```

**Flow:**
1. Load scenario configurations
2. Apply filters to transactions
3. Aggregate by customer/time window
4. Evaluate thresholds
5. Generate alerts with metadata

---

### 2. Smart Layer Processor
**File:** `smart_layer.py`

**Purpose:** Refines raw alerts by applying exclusion rules and context checks.

**Key Features:**
- **Vectorized processing** (85% faster than iterrows)
- Trigger window filtering (only relevant transactions)
- Event context detection (education, crypto, loans)
- Beneficiary verification against whitelist
- Audit logging (AlertExclusionLog)

**Main Class:** `SmartLayerProcessor`

**Key Methods:**
```python
apply_refinements(
    alerts: pd.DataFrame,
    transactions: pd.DataFrame,
    refinement_rules: List[Dict],
    lookback_days: int = 30
) -> pd.DataFrame
```

**Optimization:**
- Uses `str.contains()` with regex for bulk filtering
- Processes only matched customer subset
- Avoids nested loops where possible

---

### 3. Risk Engine
**File:** `risk_engine.py`

**Purpose:** Analyzes excluded alerts to quantify security risk ("Red Teaming").

**Key Features:**
- Scores excluded alerts (0-100 scale)
- Uses CustomerRiskProfile (PEP, adverse media, SAR count)
- Validates against VerifiedEntity whitelist
- Generates exploit scenarios for UI

**Main Class:** `RiskEngine`

**Key Methods:**
```python
analyze_excluded_alerts(
    excluded_alerts: List[Dict]
) -> Dict[str, Any]
```

**Returns:**
```python
{
    "risk_score": 45.2,
    "risk_level": "CAUTION",  # SAFE | CAUTION | DANGEROUS | CRITICAL
    "excluded_count": 12,
    "sample_exploits": [...]
}
```

**Scoring Factors:**
- Amount reasonability (education > $50k = suspicious)
- Beneficiary verification status
- Customer risk profile (PEP, adverse media)
- Previous SAR filings

---

### 4. Event Detector
**File:** `event_detector.py`

**Purpose:** Detects transaction context (education, crypto, loans) and validates legitimacy.

**Key Features:**
- Keyword-based event classification
- Beneficiary verification lookup
- Amount reasonability checks
- Returns structured context object

**Main Class:** `EventDetector`

**Key Methods:**
```python
detect_event_context(
    narrative: str,
    amount: float,
    beneficiary: str
) -> Dict | None
```

**Returns:**
```python
{
    "type": "education",
    "is_verified": True,
    "amount_reasonable": True,
    "confidence": 0.95
}
```

---

### 5. TTL Manager
**File:** `ttl_manager.py`

**Purpose:** Manages time-to-live for uploaded data (auto-deletion after 48h).

**Key Features:**
- Creates upload metadata records
- Sets expiry timestamps
- Extends TTL (max 168h)
- Cleanup expired data (cron job)

**Main Class:** `TTLManager`

**Key Methods:**
```python
create_upload_record(db, user_id, filename, txn_count, ...) -> str
extend_ttl(db, upload_id, additional_hours=24) -> bool
cleanup_expired(db) -> Dict[str, int]
```

**TTL Lifecycle:**
1. Upload → `expires_at = now + 48h`
2. User extends → `expires_at += 24h` (max 168h)
3. Expiry reached → Cleanup job deletes records

---

## 🔄 Data Flow

```
Transactions (CSV) 
    ↓
UniversalEngine.execute_scenarios()
    ↓
Raw Alerts DataFrame
    ↓
SmartLayerProcessor.apply_refinements()
    ↓
EventDetector.detect_event_context()
    ↓
Refined Alerts (excluded flagged)
    ↓
RiskEngine.analyze_excluded_alerts()
    ↓
Risk Analysis + Exploit Scenarios
```

## 🎯 Performance Optimizations

### Vectorization (SmartLayer)
- **Before:** O(n) iterrows loop → 18s for 10k alerts
- **After:** Vectorized filtering → 2.7s (**85% faster**)

### Key Techniques:
1. Bulk keyword matching with `str.contains()`
2. Filter to matched customers first
3. Process only relevant subset
4. Avoid nested transaction loops

## 🧪 Testing

Each module has corresponding tests in `backend/tests/`:
- `test_simulation.py` - UniversalEngine tests
- `test_smart_layer.py` - SmartLayer tests (TODO)
- `test_risk_engine.py` - RiskEngine tests (TODO)

## 📊 Monitoring

All modules log to structured JSON via `structlog`:
```python
logger.info("scenario_executed", 
    scenario_id=scenario_id, 
    alerts_generated=len(alerts),
    duration_ms=duration
)
```

## 🔗 Dependencies

- `pandas` - DataFrame operations
- `sqlalchemy` - Database ORM
- `pydantic` - Configuration validation
- `simpleeval` - Safe expression evaluation

## 📝 Configuration

Scenarios are defined in `ScenarioConfig` table with JSON structure:
```json
{
    "filters": [...],
    "aggregation": {...},
    "threshold": {...},
    "alert_condition": {...}
}
```

See `config_models.py` for Pydantic schemas.


================================================================================
FILE: backend/core/config_models.py
================================================================================

from pydantic import BaseModel, Field
from typing import List, Optional, Any, Dict, Union, Literal

# --- Filter Models ---
class AmountRange(BaseModel):
    min: Optional[float] = None
    max: Optional[float] = None

class CustomFilter(BaseModel):
    field: str
    operator: Literal['equals', 'contains', 'greater_than', 'less_than', 'in']
    value: Union[str, int, float, List[Union[str, int, float]]]

class ScenarioFilters(BaseModel):
    transaction_type: Optional[List[str]] = None
    channel: Optional[List[str]] = None
    direction: Optional[Literal['debit', 'credit', 'both']] = 'both'
    customer_type: Optional[List[str]] = None
    amount_range: Optional[AmountRange] = None
    custom_field_filters: Optional[List[CustomFilter]] = None

# --- Aggregation Models ---
class TimeWindow(BaseModel):
    value: int
    unit: Literal['days', 'hours', 'months']
    type: Literal['rolling', 'calendar'] = 'calendar'

class ScenarioAggregation(BaseModel):
    method: Literal['sum', 'count', 'avg', 'max', 'min']
    field: str
    group_by: List[str]
    time_window: Optional[TimeWindow] = None

# --- Threshold Models ---
class FieldBasedThreshold(BaseModel):
    reference_field: str
    calculation: str # e.g. "reference_field * 3 / 12"

class SegmentBasedThreshold(BaseModel):
    segment_field: str
    values: Dict[str, float]
    default: float = 0.0

class ScenarioThreshold(BaseModel):
    type: Literal['fixed', 'customer_based', 'segment_based', 'field_based', 'dynamic'] # Added dynamic mapping to field_based in logic likely
    fixed_value: Optional[float] = None
    field_based: Optional[FieldBasedThreshold] = None
    segment_based: Optional[SegmentBasedThreshold] = None

# --- Alert Condition Models ---
class AdditionalCondition(BaseModel):
    field: str
    operator: Literal['equals', 'greater_than', 'less_than']
    value: Union[str, int, float]

class AlertCondition(BaseModel):
    expression: str # e.g. "aggregated_value > threshold"
    additional_conditions: Optional[List[AdditionalCondition]] = None

# --- Refinement Config (Placeholder per user Layers) ---
class RefinementConfig(BaseModel):
    type: Literal['event_based', 'behavioral', 'threshold_adjustment']
    enabled: bool = True
    config: Dict[str, Any]

# --- Top Level Configuration ---
class ScenarioConfigModel(BaseModel):
    scenario_id: str
    scenario_name: str
    description: Optional[str] = None
    frequency: Literal['daily', 'monthly', 'realtime', 'end_of_month'] = 'daily'
    
    # Accept BOTH formats: ScenarioFilters object OR simple array
    filters: Optional[Union[ScenarioFilters, List[Dict[str, Any]]]] = None
    aggregation: Optional[ScenarioAggregation] = None
    threshold: Optional[ScenarioThreshold] = None
    alert_condition: Optional[AlertCondition] = None
    
    refinements: Optional[List[RefinementConfig]] = None
    field_mappings: Optional[Dict[str, str]] = None  # Maps scenario fields to actual DB columns



================================================================================
FILE: backend/core/redis_client.py
================================================================================

import os
import redis
import time
from contextlib import contextmanager
from typing import Generator

def get_redis_client() -> redis.Redis:
    """
    Get a Redis client instance.
    Uses generic REDIS_URL environment variable.
    """
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    # decode_responses=True ensures we get strings back, not bytes
    return redis.from_url(redis_url, socket_connect_timeout=5, decode_responses=True)

@contextmanager
def acquire_lock(lock_name: str, acquire_timeout: int = 10, lock_timeout: int = 60):
    """
    Distributed lock context manager using Redis.
    
    Args:
        lock_name: Unique identifier for the lock
        acquire_timeout: Max time to wait to acquire lock (seconds)
        lock_timeout: Max time to hold lock (seconds)
    """
    r = get_redis_client()
    lock_key = f"lock:{lock_name}"
    identifier = str(time.time())
    
    end_time = time.time() + acquire_timeout
    
    try:
        # Spin lock
        while time.time() < end_time:
            if r.set(lock_key, identifier, ex=lock_timeout, nx=True):
                yield True
                return
            time.sleep(0.1)
        
        # Failed to acquire
        yield False
        
    finally:
        # Release lock only if we own it
        try:
            current_value = r.get(lock_key)
            if current_value == identifier:
                r.delete(lock_key)
        except Exception as e:
            # Log error instead of silent failure
            print(f"Error releasing lock {lock_key}: {e}")
            pass


================================================================================
FILE: backend/core/comparison_engine.py
================================================================================

from sqlalchemy.orm import Session
from models import Alert, SimulationRun
import pandas as pd
from typing import Dict, Any

class ComparisonEngine:
    """
    Compares two simulation runs (Baseline vs Refined) to quantify improvement.
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    def compare(self, baseline_run_id: str, refined_run_id: str) -> Dict[str, Any]:
        """
        Compare two runs and return diff stats.
        """
        # Load alerts from both runs
        baseline_alerts = self.db.query(Alert).filter(
            Alert.run_id == baseline_run_id
        ).all()
        
        refined_alerts = self.db.query(Alert).filter(
            Alert.run_id == refined_run_id
        ).all()
        
        # Simple diff
        baseline_count = len(baseline_alerts)
        refined_count = len(refined_alerts)
        
        reduction = baseline_count - refined_count
        pct = (reduction / baseline_count * 100) if baseline_count > 0 else 0
        
        # Granular Diff Logic
        # Identify which alerts were removed (True Positives vs False Positives logic requires ground truth, 
        # so here we just track net reduction)
        
        removed_ids = set([a.customer_id for a in baseline_alerts]) - set([a.customer_id for a in refined_alerts])
        
        return {
            "summary": {
                "baseline_alerts": baseline_count,
                "refined_alerts": refined_count,
                "net_change": reduction,
                "percent_reduction": round(pct, 2)
            },
            "granular_diff": [
                {"customer_id": cid, "status": "removed"} for cid in list(removed_ids)[:50] # Limit sample
            ],
            "risk_analysis": {
                "risk_score": 0, # Placeholder for real risk scoring model
                "risk_level": "LOW",
                "sample_exploits": []
            }
        }


================================================================================
FILE: backend/core/universal_engine.py
================================================================================

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import timedelta
import logging
from simpleeval import simple_eval
import uuid

from .config_models import ScenarioConfigModel, TimeWindow
# Assuming SmartLayerProcessor is available
from .smart_layer import SmartLayerProcessor

logger = logging.getLogger(__name__)

class FilterProcessor:
    """
    Handles the filtering of transaction data based on scenario configurations.
    """
    
    def apply_filters(self, transactions: pd.DataFrame, filter_config) -> pd.DataFrame:
        if not filter_config:
            return transactions
            
        df = transactions.copy()
        
        # Handle simple array format from frontend
        if isinstance(filter_config, list):
            for filter_item in filter_config:
                df = self._apply_single_filter(df, filter_item)
            return df
        
        # Handle complex ScenarioFilters object format
        if hasattr(filter_config, 'transaction_type') and filter_config.transaction_type:
            df = df[df['transaction_type'].isin(filter_config.transaction_type)]
            
        if hasattr(filter_config, 'channel') and filter_config.channel:
            df = df[df['channel'].isin(filter_config.channel)]
            
        if hasattr(filter_config, 'direction') and filter_config.direction:
            if filter_config.direction == 'debit':
                df = df[df['debit_credit_indicator'] == 'D']
            elif filter_config.direction == 'credit':
                df = df[df['debit_credit_indicator'] == 'C']
                
        # 2. Amount Range
        if hasattr(filter_config, 'amount_range') and filter_config.amount_range:
            ar = filter_config.amount_range
            if ar.min is not None:
                df = df[df['transaction_amount'] >= ar.min]
            if ar.max is not None:
                df = df[df['transaction_amount'] <= ar.max]
                
        # 3. Custom Field Filters
        if hasattr(filter_config, 'custom_field_filters') and filter_config.custom_field_filters:
            for custom in filter_config.custom_field_filters:
                df = self._apply_single_filter(df, custom)
                
        return df
    
    def _apply_single_filter(self, df: pd.DataFrame, filter_config: Any) -> pd.DataFrame:
        """
        Apply a single filter. Handles both dict (simple) and object (custom) formats.
        Unified logic for cleaner maintenance.
        """
        # Extract fields based on type
        if isinstance(filter_config, dict):
            field = filter_config.get('field')
            operator = filter_config.get('operator')
            value = filter_config.get('value')
        else:
            field = getattr(filter_config, 'field', None)
            operator = getattr(filter_config, 'operator', None)
            value = getattr(filter_config, 'value', None)
            
        if not field:
            return df
            
        if field not in df.columns:
            print(f"[ERROR] Filter field '{field}' not found! Available: {list(df.columns)}")
            return df
        
        # Normalize operator
        op = str(operator).lower() if operator else ''
        
        # SAFE TYPE CASTING
        # Ensure 'value' matches the column's dtype
        target_dtype = df[field].dtype
        
        try:
            if np.issubdtype(target_dtype, np.number):
                if isinstance(value, list):
                    value = [float(v) for v in value]
                else:
                    value = float(value)
            elif np.issubdtype(target_dtype, np.datetime64):
                if isinstance(value, list):
                    value = [pd.to_datetime(v) for v in value]
                else:
                    value = pd.to_datetime(value)
            # Else keep as is (string/object)
        except Exception:
            # If casting fails, we might just proceed (pandas might handle it or error later)
            # But usually it's better to just log and try
            pass

        # Apply Logic
        if op in ['==', 'equals']:
            return df[df[field] == value]
        elif op in ['!=', 'not_equals']:
            return df[df[field] != value]
        elif op in ['>', 'greater_than', 'greaterthan']:
            return df[df[field] > value]
        elif op in ['<', 'less_than', 'lessthan']:
            return df[df[field] < value]
        elif op in ['>=', 'greater_than_or_equal']:
             return df[df[field] >= value]
        elif op in ['<=', 'less_than_or_equal']:
             return df[df[field] <= value]
        elif op == 'in':
            val_list = value if isinstance(value, list) else [value]
            return df[df[field].isin(val_list)]
        elif op == 'contains':
            return df[df[field].astype(str).str.contains(str(value), case=False, na=False)]
        
        return df

class AggregationProcessor:
    """
    Handles data aggregation.
    """
    def aggregate_data(self, transactions: pd.DataFrame, agg_config) -> pd.DataFrame:
        # Handle empty input
        if transactions.empty:
            return pd.DataFrame()
        
        # ✅ FIX: Proper None check for aggregation config
        if agg_config is None:
            print("[WARN] No aggregation config provided")
            return transactions.copy()
        
        # ✅ FIX: Check if required fields exist
        if not hasattr(agg_config, 'method') or not hasattr(agg_config, 'field'):
            print("[ERROR] Aggregation config missing required fields (method/field)")
            return transactions.copy()
        
        if not hasattr(agg_config, 'group_by') or not agg_config.group_by:
            print("[ERROR] Aggregation config missing group_by")
            return transactions.copy()
            
        df = transactions.copy()
        method = agg_config.method
        field = agg_config.field
        group_fields = agg_config.group_by.copy()
        

        
        # Validate field exists
        if field not in df.columns:
            print(f"[ERROR] Aggregation field '{field}' not in columns: {list(df.columns)}")
            return pd.DataFrame()
        
        # Time Window Handling
        if agg_config.time_window:
            tw = agg_config.time_window
            
            # 1. Global Datetime Conversion Safety Check
            if 'transaction_date' in df.columns:
                if not pd.api.types.is_datetime64_any_dtype(df['transaction_date']):
                    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce', utc=True)
                    if df['transaction_date'].dt.tz is not None:
                        df['transaction_date'] = df['transaction_date'].dt.tz_localize(None)

            # 2. Calendar-based Aggregation
            if hasattr(tw, 'type') and tw.type == 'calendar':
                if tw.unit == 'days':
                    df['time_group'] = df['transaction_date'].dt.floor(f'{tw.value}D')
                    group_fields.append('time_group')
                elif tw.unit == 'months':
                    df['time_group'] = df['transaction_date'].dt.to_period('M')
                    group_fields.append('time_group')
            
            # 3. Rolling Windows
            if hasattr(tw, 'type') and tw.type == 'rolling':
                return self._apply_rolling_window(df, agg_config, group_fields)

        # Standard GroupBy Aggregation
        try:
            # Ensure all group fields exist
            missing_group = [f for f in group_fields if f not in df.columns]
            if missing_group:
                print(f"[ERROR] Missing group fields: {missing_group}")
                return pd.DataFrame()

            # Define aggregation dictionary
            agg_dict = {field: method}
            
            # Traceability: Collect transaction IDs
            if 'transaction_id' in df.columns:
                agg_dict['transaction_id'] = lambda x: list(x)
            

            
            # Perform GroupBy
            result = df.groupby(group_fields, as_index=False).agg(agg_dict)
            

            
            # ✅ ADD TRANSACTION COUNT AFTER AGGREGATION
            if 'transaction_id' in result.columns:
                result['txn_count'] = result['transaction_id'].apply(len)
            else:
                result['txn_count'] = 0
            
            # Rename columns
            rename_dict = {
                field: 'aggregated_value'
            }
            
            if 'transaction_id' in agg_dict:
                rename_dict['transaction_id'] = 'involved_transactions'
            
            result.rename(columns=rename_dict, inplace=True)
            
            # Add alert_date as max transaction date per group
            if 'transaction_date' in df.columns:
                max_dates = df.groupby(group_fields)['transaction_date'].max().reset_index()
                result = result.merge(max_dates, on=group_fields, how='left')
                result.rename(columns={'transaction_date': 'alert_date'}, inplace=True)
            else:
                # Use current time if no transaction date available
                from datetime import datetime
                result['alert_date'] = datetime.utcnow()
            

            
        except Exception as e:
            print(f"[ERROR] Aggregation Failed: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

        return result

    def _apply_rolling_window(self, df: pd.DataFrame, agg_config, group_fields) -> pd.DataFrame:
        """Apply rolling time window aggregation"""
        tw = agg_config.time_window
        field = agg_config.field
        method = agg_config.method
        
        if 'transaction_date' not in df.columns:
            print("[ERROR] transaction_date required for rolling window")
            return pd.DataFrame()
        
        # Sort by date
        df = df.sort_values(['customer_id', 'transaction_date'])
        
        window_value = tw.value
        window_unit = tw.unit
        
        # Convert to days
        if window_unit == 'days':
            window_days = window_value
        elif window_unit == 'months':
            window_days = window_value * 30
        elif window_unit == 'hours':
            window_days = window_value / 24
        else:
            window_days = window_value
        
        print(f"[ROLLING] Window: {window_days} days")
        
        # Calculate rolling aggregation per customer
        results = []
        entity_key = 'customer_id' if 'customer_id' in group_fields else group_fields[0]
        
        for customer_id in df[entity_key].unique():
            cust_df = df[df[entity_key] == customer_id].copy()
            
            for idx, row in cust_df.iterrows():
                txn_date = row['transaction_date']
                start_date = txn_date - pd.Timedelta(days=window_days)
                
                # Get transactions in window
                window_df = cust_df[
                    (cust_df['transaction_date'] >= start_date) &
                    (cust_df['transaction_date'] <= txn_date)
                ]
                
                # Calculate aggregation
                if method == 'sum':
                    agg_value = window_df[field].sum()
                elif method == 'count':
                    agg_value = len(window_df)
                elif method == 'mean':
                    agg_value = window_df[field].mean()
                else:
                    agg_value = window_df[field].sum()
                
                results.append({
                    entity_key: customer_id,
                    'alert_date': txn_date,
                    'aggregated_value': agg_value,
                    'transaction_count': len(window_df),
                    'involved_transactions': window_df['transaction_id'].tolist() if 'transaction_id' in window_df.columns else []
                })
        
        result_df = pd.DataFrame(results)
        print(f"[ROLLING] Generated {len(result_df)} windows")
        
        return result_df

class ThresholdProcessor:
    def apply_thresholds(self, aggregated_data: pd.DataFrame, customers: pd.DataFrame, threshold_config) -> pd.DataFrame:
        if threshold_config is None or aggregated_data.empty:
            print("[THRESHOLD] No threshold config or empty data")
            return aggregated_data
        
        # Re-merge customer fields if needed for threshold calculation
        if 'customer_id' in aggregated_data.columns:
             t_type = getattr(threshold_config, 'type', None)
             
             if t_type == 'segment_based' and hasattr(threshold_config, 'segment_based'):
                 seg = threshold_config.segment_based
                 needed_field = getattr(seg, 'segment_field', None)
                 if needed_field and needed_field not in aggregated_data.columns and needed_field in customers.columns:
                     aggregated_data = aggregated_data.merge(
                         customers[['customer_id', needed_field]], 
                         on='customer_id', 
                         how='left'
                     )

        t_type = getattr(threshold_config, 'type', 'fixed')
        
        # Calculate threshold values
        if t_type == 'fixed':
            threshold_value = getattr(threshold_config, 'fixed_value', 0)
            aggregated_data['threshold'] = threshold_value

            
        elif t_type == 'segment_based':
            seg = threshold_config.segment_based
            if seg and hasattr(seg, 'segment_field') and seg.segment_field in aggregated_data.columns:
                aggregated_data['threshold'] = aggregated_data[seg.segment_field].map(
                    seg.values
                ).fillna(seg.default)
            else:
                aggregated_data['threshold'] = getattr(seg, 'default', 0)
                
        elif t_type == 'field_based' and hasattr(threshold_config, 'field_based'):
            fb = threshold_config.field_based
            ref = getattr(fb, 'reference_field', None)
            calc = getattr(fb, 'calculation', 'reference_field')
            
            def eval_calc(row):
                try:
                    val = row.get(ref, 0)
                    return simple_eval(calc, names={'reference_field': val})
                except:
                    return 0
            
            aggregated_data['threshold'] = aggregated_data.apply(eval_calc, axis=1)
        
        # ✅ FIX: Actually FILTER by threshold!
        if 'threshold' in aggregated_data.columns and 'aggregated_value' in aggregated_data.columns:
            before_count = len(aggregated_data)
            aggregated_data = aggregated_data[aggregated_data['aggregated_value'] >= aggregated_data['threshold']]
            after_count = len(aggregated_data)

        
        return aggregated_data

class AlertConditionEvaluator:
    def evaluate_condition(self, data: pd.DataFrame, condition_config) -> pd.DataFrame:
        if data.empty: return pd.DataFrame()
        
        if 'threshold' in data.columns and 'aggregated_value' in data.columns:
             alerts = data[data['aggregated_value'] > data['threshold']].copy()
             if not condition_config: return alerts
             if not getattr(condition_config, 'expression', None): return alerts
        
        if not condition_config: return pd.DataFrame()
        expr = condition_config.expression
        
        def safe_eval(row):
            try:
                names = row.to_dict()
                return simple_eval(expr, names=names)
            except: return False
        
        alerts = data[data.apply(safe_eval, axis=1)].copy()
        return alerts

class UniversalScenarioEngine:
    """
    Universal, schema-agnostic AML scenario execution engine.
    
    This engine dynamically determines data requirements from scenario configuration
    and executes a multi-stage pipeline to generate alerts:
    
    Pipeline Stages:
        1. Customer Field Detection - Identifies required customer fields
        2. Smart Merge - Joins customer data only if needed
        3. Filter Application - Applies transaction filters
        4. Aggregation - Groups and aggregates data
        5. Threshold Evaluation - Checks alert thresholds
        6. Condition Evaluation - Evaluates alert conditions
        7. Refinements - Applies smart exclusions
    
    The engine is designed to work with any schema by using flexible field mapping
    and dynamic column detection.
    
    Attributes:
        db_session: Database session for smart layer queries (optional)
        filter_processor: Handles transaction filtering
        aggregation_processor: Handles data aggregation
        threshold_processor: Evaluates alert thresholds
        condition_evaluator: Evaluates alert conditions
        smart_layer: Applies intelligent exclusions (optional)
    
    Example:
        >>> engine = UniversalScenarioEngine(db_session=db)
        >>> alerts = engine.execute(
        ...     scenario_config=scenario,
        ...     transactions=txn_df,
        ...     customers=cust_df,
        ...     run_id="run-123"
        ... )
        >>> print(f"Generated {len(alerts)} alerts")
    """
    
    def __init__(self, db_session=None):
        """
        Initialize the scenario execution engine.
        
        Args:
            db_session: SQLAlchemy database session for smart layer queries.
                       If None, smart layer refinements will be skipped.
        """
        self.db_session = db_session
        self.filter_processor = FilterProcessor()
        self.aggregation_processor = AggregationProcessor()
        self.threshold_processor = ThresholdProcessor()
        self.condition_evaluator = AlertConditionEvaluator()
        self.smart_layer = SmartLayerProcessor(db_session) if db_session else None
    
    def _get_required_customer_fields(self, scenario_config: ScenarioConfigModel) -> set:
        """
        Intelligently determines which customer fields are needed for the scenario.
        
        Scans the scenario configuration to identify all customer-related fields
        referenced in filters, aggregations, thresholds, and conditions. This
        ensures smart merging - only joining customer data if actually needed.
        
        Args:
            scenario_config: Validated scenario configuration
            
        Returns:
            Set of customer field names required for execution
            
        Example:
            >>> fields = engine._get_required_customer_fields(scenario)
            >>> print(fields)
            {'occupation', 'annual_income', 'risk_score'}
        """
        required = set()
        
        # Check filters
        if scenario_config.filters:
            for f in scenario_config.filters:
                if hasattr(f, 'field') and f.field:
                    if f.field.startswith('customer_'):
                        required.add(f.field)
        
        # Check aggregation group_by
        if scenario_config.aggregation and hasattr(scenario_config.aggregation, 'group_by'):
            for field in scenario_config.aggregation.group_by or []:
                if field.startswith('customer_'):
                    required.add(field)
        
        # Check threshold calculations
        if scenario_config.threshold and hasattr(scenario_config.threshold, 'calculation'):
            calc = scenario_config.threshold.calculation or ""
            for field in ['customer_type', 'occupation', 'annual_income', 'risk_score']:
                if field in calc:
                    required.add(field)
        
        # Check alert conditions
        if scenario_config.alert_condition and hasattr(scenario_config.alert_condition, 'expression'):
            expr = scenario_config.alert_condition.expression or ""
            for field in ['customer_type', 'occupation', 'annual_income', 'risk_score']:
                if field in expr:
                    required.add(field)
        
        return required

    def _smart_merge_customers(self, transactions: pd.DataFrame, customers: pd.DataFrame, required_fields: set) -> pd.DataFrame:
        """
        Intelligently merges customer data with transactions ONLY if required.
        
        Performs a left join only when customer fields are actually needed by the
        scenario. This optimization significantly improves performance for scenarios
        that only analyze transaction data.
        
        Args:
            transactions: Transaction DataFrame
            customers: Customer DataFrame
            required_fields: Set of customer fields needed (from _get_required_customer_fields)
            
        Returns:
            Merged DataFrame with customer fields if needed, otherwise original transactions
            
        Example:
            >>> enriched = engine._smart_merge_customers(txns, custs, {'occupation'})
            >>> assert 'occupation' in enriched.columns
        """
        if not required_fields:
            print("[OPTIMIZATION] No customer fields needed - skipping merge")
            return transactions.copy()
        
        if customers.empty:
            print("[WARN] Customer data is empty, cannot merge")
            return transactions.copy()
        
        # Only select required customer columns + customer_id for join
        customer_cols = ['customer_id'] + [f for f in required_fields if f in customers.columns]
        
        # Deduplicate customers by customer_id to prevent merge explosion
        customers_subset = customers[customer_cols].copy()
        customers_subset = customers_subset.drop_duplicates(subset=['customer_id'])
        

        
        # Check if transactions has duplicate customer_id columns
        txn_customer_id_count = list(transactions.columns).count('customer_id')
        cust_customer_id_count = list(customers_subset.columns).count('customer_id')
        
        if txn_customer_id_count > 1:
            print(f"[ERROR] Transactions DataFrame has {txn_customer_id_count} 'customer_id' columns!")
            # Drop duplicates, keeping first
            transactions = transactions.loc[:, ~transactions.columns.duplicated()]
            print(f"[FIX] Dropped duplicate columns. New columns: {list(transactions.columns)}")
        
        if cust_customer_id_count > 1:
            print(f"[ERROR] Customers DataFrame has {cust_customer_id_count} 'customer_id' columns!")
            customers_subset = customers_subset.loc[:, ~customers_subset.columns.duplicated()]
            print(f"[FIX] Dropped duplicate columns. New columns: {list(customers_subset.columns)}")
        
        # Perform left join
        merged = transactions.merge(
            customers_subset,
            on='customer_id',
            how='left',
            suffixes=('', '_cust')
        )
        

        return merged

    def execute(self, scenario_config: ScenarioConfigModel, transactions: pd.DataFrame, customers: pd.DataFrame, run_id: str) -> List[Dict]:
        """
        Execute an AML scenario against transaction data.
        
        This is the main entry point for scenario execution. It orchestrates the
        entire pipeline from filtering to alert generation.
        
        Args:
            scenario_config: Validated scenario configuration (Pydantic model)
            transactions: Transaction DataFrame with columns:
                - transaction_id (str, required)
                - customer_id (str, required)
                - transaction_date (datetime, required)
                - transaction_amount (float, required)
                - transaction_type, channel, etc. (optional)
            customers: Customer DataFrame with columns:
                - customer_id (str, required)
                - customer_name, occupation, annual_income, etc. (optional)
            run_id: Unique identifier for this simulation run
            
        Returns:
            List of alert dictionaries, each containing:
                - alert_id: Unique alert identifier
                - customer_id: Customer who triggered alert
                - customer_name: Customer name
                - scenario_id: Scenario that generated alert
                - scenario_name: Human-readable scenario name
                - alert_date: Date alert was triggered
                - risk_score: Calculated risk score
                - trigger_details: JSON with alert details
                - run_id: Simulation run identifier
                
        Raises:
            ValueError: If required columns are missing from input DataFrames
            
        Example:
            >>> alerts = engine.execute(
            ...     scenario_config=rapid_movement_scenario,
            ...     transactions=txn_df,
            ...     customers=cust_df,
            ...     run_id="run-abc-123"
            ... )
            >>> for alert in alerts:
            ...     print(f"Alert for {alert['customer_name']}: {alert['risk_score']}")
        """
        # Step 0: Intelligent Customer Field Detection
        required_customer_fields = self._get_required_customer_fields(scenario_config)
        
        # Step 1: Smart Merge
        enriched_data = self._smart_merge_customers(transactions, customers, required_customer_fields)
        
        # Step 2: Apply Filters
        filtered = self.filter_processor.apply_filters(enriched_data, scenario_config.filters)
        if filtered.empty:
            print("[WARN] All transactions filtered out!")
            return []

        
        # Step 3: Aggregations
        aggregated = self.aggregation_processor.aggregate_data(filtered, scenario_config.aggregation)
        if aggregated.empty:
            print("[WARN] No data after aggregation")
            return []

        
        # Step 4: Thresholds
        with_thresh = self.threshold_processor.apply_thresholds(aggregated, customers, scenario_config.threshold)

        
        # Step 5: Conditions
        alerts_df = self.condition_evaluator.evaluate_condition(with_thresh, scenario_config.alert_condition)
        if alerts_df.empty:
            print("[INFO] No alerts triggered")
            return []

        
        # Step 6: Refinements
        if scenario_config.refinements and self.smart_layer:
            refinements_list = [r.dict() for r in scenario_config.refinements]
            alerts_df = self.smart_layer.apply_refinements(alerts_df, enriched_data, refinements_list)

            
        # Step 7: Metadata
        alerts_df['scenario_id'] = scenario_config.scenario_id
        alerts_df['scenario_name'] = scenario_config.scenario_name
        alerts_df['run_id'] = run_id
        
        return self._generate_alert_objects(alerts_df)

    def _generate_alert_objects(self, df: pd.DataFrame) -> List[Dict]:
        alerts = []
        for _, row in df.iterrows():
            # Calculate risk score based on aggregated amount
            # Higher amounts = higher risk
            agg_amount = row.get('aggregated_amount', 0)
            if agg_amount >= 100000:  # Very high amount
                risk_score = 85
            elif agg_amount >= 50000:  # High amount
                risk_score = 70
            elif agg_amount >= 20000:  # Medium-high amount
                risk_score = 55
            elif agg_amount >= 10000:  # Medium amount
                risk_score = 40
            else:  # Lower amounts
                risk_score = 25
            
            base = {
                "alert_id": str(uuid.uuid4()),
                "scenario_id": row.get('scenario_id'),
                "scenario_name": row.get('scenario_name'),
                "customer_id": row.get('customer_id'),
                "customer_name": row.get('customer_name', 'Unknown'),
                "run_id": row.get('run_id'),
                "alert_date": row.get('transaction_date', pd.Timestamp.utcnow()),
                "risk_score": risk_score,
                "excluded": row.get('excluded', False),
                "exclusion_reason": row.get('exclusion_reason'),
                "is_excluded": row.get('excluded', False),
                "involved_transactions": row.get('involved_transactions', []) # Traceability
            }
            details = row.to_dict()
            # remove large list from trigger details to save space
            if 'involved_transactions' in details:
                del details['involved_transactions']
                
            base['trigger_details'] = {str(k): str(v) for k,v in details.items()}
            alerts.append(base)
        return alerts


================================================================================
FILE: backend/core/data_quality.py
================================================================================

"""
Data Quality Validation

Validates uploaded data for quality issues before processing:
- Negative amounts
- Future dates
- Missing critical fields
- Duplicates
- Data type mismatches
"""

import pandas as pd
from typing import Dict, List, Any
from datetime import datetime, timezone
import structlog

logger = structlog.get_logger("data_quality")


class DataQualityValidator:
    """
    Validates data quality for transactions and customers.
    
    Returns a quality report with:
    - valid: bool (whether data passes validation)
    - issues: list of quality issues found
    - quality_score: 0-100 score
    - total_rows: number of rows validated
    """
    
    @staticmethod
    def validate_transactions(df: pd.DataFrame) -> Dict[str, Any]:
        """
        Validate transaction data quality.
        
        Checks:
        - No negative amounts
        - No future dates
        - No duplicate transaction IDs
        - No missing customer IDs
        - Valid transaction types
        - Reasonable amount ranges
        """
        issues = []
        warnings = []
        
        # Check for negative amounts
        negative_amounts = (df['transaction_amount'] < 0).sum()
        if negative_amounts > 0:
            issues.append({
                "severity": "error",
                "field": "transaction_amount",
                "message": f"{negative_amounts} transactions have negative amounts",
                "count": negative_amounts
            })
        
        # Check for future dates
        now = pd.Timestamp.now(tz=timezone.utc)
        df['transaction_date'] = pd.to_datetime(df['transaction_date'], utc=True)
        future_dates = (df['transaction_date'] > now).sum()
        if future_dates > 0:
            warnings.append({
                "severity": "warning",
                "field": "transaction_date",
                "message": f"{future_dates} transactions are future-dated",
                "count": future_dates
            })
        
        # Check for duplicates
        if 'transaction_id' in df.columns:
            dupes = df.duplicated(subset=['transaction_id']).sum()
            if dupes > 0:
                issues.append({
                    "severity": "error",
                    "field": "transaction_id",
                    "message": f"{dupes} duplicate transaction IDs found",
                    "count": dupes
                })
        
        # Check for missing customer IDs
        missing_customers = df['customer_id'].isna().sum()
        if missing_customers > 0:
            issues.append({
                "severity": "error",
                "field": "customer_id",
                "message": f"{missing_customers} transactions missing customer_id",
                "count": missing_customers
            })
        
        # Check for missing amounts
        missing_amounts = df['transaction_amount'].isna().sum()
        if missing_amounts > 0:
            issues.append({
                "severity": "error",
                "field": "transaction_amount",
                "message": f"{missing_amounts} transactions missing amount",
                "count": missing_amounts
            })
        
        # Check for unreasonably large amounts (potential data entry errors)
        if 'transaction_amount' in df.columns:
            very_large = (df['transaction_amount'] > 10_000_000).sum()  # > 10M
            if very_large > 0:
                warnings.append({
                    "severity": "warning",
                    "field": "transaction_amount",
                    "message": f"{very_large} transactions exceed 10M (potential data entry errors)",
                    "count": very_large
                })
        
        # Check for zero amounts
        zero_amounts = (df['transaction_amount'] == 0).sum()
        if zero_amounts > 0:
            warnings.append({
                "severity": "warning",
                "field": "transaction_amount",
                "message": f"{zero_amounts} transactions have zero amount",
                "count": zero_amounts
            })
        
        # Calculate quality score
        total_issues = len(issues)
        total_warnings = len(warnings)
        quality_score = max(0, 100 - (total_issues * 15) - (total_warnings * 5))
        
        # Determine if valid (no errors, warnings are okay)
        is_valid = total_issues == 0
        
        result = {
            "valid": is_valid,
            "issues": issues,
            "warnings": warnings,
            "total_rows": len(df),
            "quality_score": quality_score,
            "summary": {
                "errors": total_issues,
                "warnings": total_warnings,
                "clean_rows": len(df) - sum(i['count'] for i in issues + warnings)
            }
        }
        
        logger.info(
            "transaction_validation_complete",
            valid=is_valid,
            total_rows=len(df),
            quality_score=quality_score,
            errors=total_issues,
            warnings=total_warnings
        )
        
        return result
    
    @staticmethod
    def validate_customers(df: pd.DataFrame) -> Dict[str, Any]:
        """
        Validate customer data quality.
        
        Checks:
        - No duplicate customer IDs
        - No missing customer names
        - Valid data types
        """
        issues = []
        warnings = []
        
        # Check for duplicate customer IDs
        if 'customer_id' in df.columns:
            dupes = df.duplicated(subset=['customer_id']).sum()
            if dupes > 0:
                issues.append({
                    "severity": "error",
                    "field": "customer_id",
                    "message": f"{dupes} duplicate customer IDs found",
                    "count": dupes
                })
        
        # Check for missing customer IDs
        missing_ids = df['customer_id'].isna().sum()
        if missing_ids > 0:
            issues.append({
                "severity": "error",
                "field": "customer_id",
                "message": f"{missing_ids} customers missing customer_id",
                "count": missing_ids
            })
        
        # Check for missing customer names
        if 'customer_name' in df.columns:
            missing_names = df['customer_name'].isna().sum()
            if missing_names > 0:
                warnings.append({
                    "severity": "warning",
                    "field": "customer_name",
                    "message": f"{missing_names} customers missing name",
                    "count": missing_names
                })
        
        # Calculate quality score
        total_issues = len(issues)
        total_warnings = len(warnings)
        quality_score = max(0, 100 - (total_issues * 15) - (total_warnings * 5))
        
        is_valid = total_issues == 0
        
        result = {
            "valid": is_valid,
            "issues": issues,
            "warnings": warnings,
            "total_rows": len(df),
            "quality_score": quality_score,
            "summary": {
                "errors": total_issues,
                "warnings": total_warnings,
                "clean_rows": len(df) - sum(i['count'] for i in issues + warnings)
            }
        }
        
        logger.info(
            "customer_validation_complete",
            valid=is_valid,
            total_rows=len(df),
            quality_score=quality_score,
            errors=total_issues,
            warnings=total_warnings
        )
        
        return result


================================================================================
FILE: backend/tests/conftest.py
================================================================================

"""
Pytest configuration and fixtures for SAS Simulator tests
"""
import pytest
from httpx import AsyncClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

# Import app and models
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from main import app
from database import Base, get_db
from models import Transaction, Customer, ScenarioConfig, SimulationRun

# Test database URL (in-memory SQLite)
TEST_DATABASE_URL = "sqlite:///:memory:"

@pytest.fixture(scope="function")
def test_db():
    """Create a test database for each test"""
    engine = create_engine(
        TEST_DATABASE_URL,
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    # Create tables
    Base.metadata.create_all(bind=engine)
    
    db = TestingSessionLocal()
    try:
        yield db
    finally:
        db.close()
        Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="function")
def override_get_db(test_db):
    """Override the get_db dependency"""
    def _override_get_db():
        try:
            yield test_db
        finally:
            pass
    
    app.dependency_overrides[get_db] = _override_get_db
    yield
    app.dependency_overrides.clear()

@pytest.fixture
async def client(override_get_db):
    """Create an async test client"""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
def sample_transactions(test_db):
    """Create sample transactions for testing"""
    transactions = [
        Transaction(
            transaction_id="TXN001",
            customer_id="CUST001",
            transaction_date="2024-01-15",
            transaction_amount=50000,
            transaction_narrative="University tuition payment",
            beneficiary_name="MIT",
            upload_id="test_upload"
        ),
        Transaction(
            transaction_id="TXN002",
            customer_id="CUST001",
            transaction_date="2024-01-16",
            transaction_amount=25000,
            transaction_narrative="Bitcoin purchase",
            beneficiary_name="Binance",
            upload_id="test_upload"
        )
    ]
    
    for txn in transactions:
        test_db.add(txn)
    test_db.commit()
    
    return transactions

@pytest.fixture
def sample_customers(test_db):
    """Create sample customers for testing"""
    customers = [
        Customer(
            customer_id="CUST001",
            customer_name="John Doe",
            customer_type="Individual",
            upload_id="test_upload"
        )
    ]
    
    for cust in customers:
        test_db.add(cust)
    test_db.commit()
    
    return customers


================================================================================
FILE: backend/tests/test_auth.py
================================================================================

"""
Tests for authentication and authorization
"""
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_protected_endpoint_without_auth(client: AsyncClient):
    """Test that protected endpoints require authentication"""
    response = await client.get("/api/rules/scenarios")
    # Should return 401 or 403 without auth
    assert response.status_code in [401, 403]

@pytest.mark.asyncio
async def test_database_connection_validation(client: AsyncClient):
    """Test database connection validation"""
    # Test with invalid URL
    response = await client.post(
        "/api/connect",
        json={"db_url": "mysql://invalid"}
    )
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "failed"
    
    # Test with non-PostgreSQL URL
    response = await client.post(
        "/api/connect",
        json={"db_url": "mongodb://localhost"}
    )
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "failed"
    assert "PostgreSQL" in data["message"]


================================================================================
FILE: backend/tests/test_data.py
================================================================================

"""
Tests for data upload endpoints
"""
import pytest
from httpx import AsyncClient
from io import BytesIO

@pytest.mark.asyncio
async def test_upload_transactions_csv(client: AsyncClient):
    """Test transaction CSV upload"""
    csv_content = b"""transaction_id,customer_id,transaction_date,transaction_amount,transaction_narrative
TXN001,CUST001,2024-01-15,50000,Payment for services
TXN002,CUST001,2024-01-16,25000,Transfer to account"""
    
    files = {"file": ("transactions.csv", BytesIO(csv_content), "text/csv")}
    
    response = await client.post("/api/data/upload/transactions", files=files)
    
    # Should succeed or return validation error
    assert response.status_code in [200, 400, 413, 422]
    
    if response.status_code == 200:
        data = response.json()
        assert "status" in data
        assert "records_uploaded" in data

@pytest.mark.asyncio
async def test_upload_invalid_file_type(client: AsyncClient):
    """Test upload with invalid file type"""
    files = {"file": ("test.txt", BytesIO(b"invalid content"), "text/plain")}
    
    response = await client.post("/api/data/upload/transactions", files=files)
    assert response.status_code == 400

@pytest.mark.asyncio
async def test_ttl_extension(client: AsyncClient):
    """Test TTL extension endpoint"""
    response = await client.post(
        "/api/data/ttl/extend",
        params={"upload_id": "nonexistent", "additional_hours": 24}
    )
    # Should return 404 for nonexistent upload
    assert response.status_code == 404


================================================================================
FILE: backend/tests/test_simulation.py
================================================================================

"""
Tests for simulation endpoints
"""
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_health_check(client: AsyncClient):
    """Test health check endpoint"""
    response = await client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "healthy"
    assert "version" in data

@pytest.mark.asyncio
async def test_metrics_endpoint(client: AsyncClient):
    """Test Prometheus metrics endpoint"""
    response = await client.get("/metrics")
    assert response.status_code == 200
    assert "text/plain" in response.headers["content-type"]

@pytest.mark.asyncio
async def test_simulation_run_validation(client: AsyncClient):
    """Test simulation run with invalid data"""
    response = await client.post(
        "/api/simulation/run",
        json={
            "scenarios": [],  # Empty scenarios should fail
            "run_type": "baseline"
        }
    )
    # Should return validation error or 422
    assert response.status_code in [400, 422]

@pytest.mark.asyncio
async def test_check_schema_endpoint(client: AsyncClient):
    """Test schema validation endpoint"""
    response = await client.post(
        "/api/simulation/check-schema",
        json={
            "scenarios": ["ICICI_01"],
            "run_type": "baseline"
        }
    )
    assert response.status_code == 200
    data = response.json()
    assert "status" in data
    assert "missing_fields" in data


================================================================================
FILE: backend/scripts/apply_policies.py
================================================================================

from sqlalchemy import create_engine, text
import os
from dotenv import load_dotenv

load_dotenv()

# Use the full connection string from .env directly or hardcode if needed for script
DB_URL = os.getenv("DATABASE_URL")

def apply_rls():
    if not DB_URL or "sqlite" in DB_URL:
        print("Skipping RLS: Not connected to Supabase PostgreSQL.")
        return

    print(f"Applying RLS policies to {DB_URL.split('@')[1]}...")
    
    engine = create_engine(DB_URL)
    
    statements = [
        "ALTER TABLE transactions ENABLE ROW LEVEL SECURITY;",
        "ALTER TABLE customers ENABLE ROW LEVEL SECURITY;",
        "ALTER TABLE scenarios_config ENABLE ROW LEVEL SECURITY;",
        "ALTER TABLE simulation_runs ENABLE ROW LEVEL SECURITY;",
        "ALTER TABLE alerts ENABLE ROW LEVEL SECURITY;",
        
        # Scenarios Policies
        """
        DO $$ 
        BEGIN
            IF NOT EXISTS (SELECT FROM pg_policies WHERE tablename = 'scenarios_config' AND policyname = 'Users can view own scenarios') THEN
                CREATE POLICY "Users can view own scenarios" ON scenarios_config FOR SELECT USING (auth.uid()::text = user_id::text);
            END IF;
        END $$;
        """,
        """
        DO $$ 
        BEGIN
            IF NOT EXISTS (SELECT FROM pg_policies WHERE tablename = 'scenarios_config' AND policyname = 'Users can create own scenarios') THEN
                CREATE POLICY "Users can create own scenarios" ON scenarios_config FOR INSERT WITH CHECK (auth.uid()::text = user_id::text);
            END IF;
        END $$;
        """,
        
        # Simulation Runs Policies
        """
        DO $$ 
        BEGIN
            IF NOT EXISTS (SELECT FROM pg_policies WHERE tablename = 'simulation_runs' AND policyname = 'Users can view own runs') THEN
                CREATE POLICY "Users can view own runs" ON simulation_runs FOR SELECT USING (auth.uid()::text = user_id::text);
            END IF;
        END $$;
        """,
        """
        DO $$ 
        BEGIN
            IF NOT EXISTS (SELECT FROM pg_policies WHERE tablename = 'simulation_runs' AND policyname = 'Users can create own runs') THEN
                CREATE POLICY "Users can create own runs" ON simulation_runs FOR INSERT WITH CHECK (auth.uid()::text = user_id::text);
            END IF;
        END $$;
        """
    ]
    
    with engine.connect() as conn:
        for stmt in statements:
            try:
                # Wrap each in a sub-transaction savepoint logic or just raw execute if autocommit
                # Simplest for script: Try/Except with rollback on failure
                conn.execute(text(stmt))
                conn.commit()
                print(f"Executed: {stmt[:50].strip()}...")
            except Exception as e:
                print(f"Error executing stmt: {e}")
                conn.rollback() # Important: Reset transaction state so next stmt works

if __name__ == "__main__":
    apply_rls()


================================================================================
FILE: backend/api/simulation.py
================================================================================

from fastapi import APIRouter, Depends, BackgroundTasks, HTTPException, Header
from fastapi.responses import StreamingResponse
import io
import pandas as pd
from sqlalchemy.orm import Session
from datetime import datetime
from typing import List, Optional, Dict
from pydantic import BaseModel

from database import get_db, _get_engine, DEFAULT_DB_URL, resolve_db_url, get_service_engine
from services.simulation_service import SimulationService
from auth import get_current_user
from models import DataUpload, Customer
from datetime import datetime, timezone

def run_simulation_background(run_id: str, db_url: str):
    # Use service role for background system operations to bypass RLS
    SessionLocal = get_service_engine()
    db = SessionLocal()
    try:
        service = SimulationService(db)
        service.execute_run(run_id)
    finally:
        db.close()

router = APIRouter(prefix="/api/simulation", tags=["Simulation"])

class RunRequest(BaseModel):
    scenarios: List[str]
    run_type: str = "baseline"
    field_mappings: Optional[Dict[str, str]] = None
    date_range_start: Optional[datetime] = None
    date_range_end: Optional[datetime] = None

@router.post("/check-schema")
async def check_schema(
    request: RunRequest,
    db: Session = Depends(get_db),
    user_data: dict = Depends(get_current_user)
):
    """
    Validates if the current DB has all columns required by the selected scenarios.
    Returns list of missing fields.
    """
    from models import ScenarioConfig, Transaction, Customer, DataUpload
    from sqlalchemy import inspect
    
    user_id = user_data.get("sub")
    
    # 1. Collect all required fields from scenarios
    required_fields = set()
    
    scenarios = db.query(ScenarioConfig).filter(
        ScenarioConfig.scenario_id.in_(request.scenarios)
    ).all()
    
    for sc in scenarios:
        config = sc.config_json
        if not config:
            continue
        
        # Filters
        if 'filters' in config:
            for f in config['filters']:
                if 'field' in f:
                    required_fields.add(f['field'])
                
        # Aggregation
        if 'aggregation' in config and 'field' in config['aggregation']:
            required_fields.add(config['aggregation']['field'])
        
        # Aggregation group_by
        if 'aggregation' in config and 'group_by' in config['aggregation']:
            group_by = config['aggregation']['group_by']
            if isinstance(group_by, list):
                required_fields.update(group_by)
            elif isinstance(group_by, str):
                required_fields.add(group_by)
            
        # Threshold (Field based)
        if 'threshold' in config:
            if 'field_based' in config['threshold']:
                ref_field = config['threshold']['field_based'].get('reference_field')
                if ref_field:
                    required_fields.add(ref_field)
            
            # Segment-based threshold
            if 'segment_based' in config['threshold']:
                segment_field = config['threshold']['segment_based'].get('segment_field')
                if segment_field:
                    required_fields.add(segment_field)

    # 2. Get available columns from BOTH physical columns AND raw_data
    available_columns = set()
    
    # Add physical columns from inspector
    inspector = inspect(db.bind)
    for table in [Transaction, Customer]:
        for col in inspector.get_columns(table.__tablename__):
            available_columns.add(col['name'])
    
    # Extract fields from raw_data JSONB (same logic as /schema endpoint)
    latest_upload = db.query(DataUpload).filter(
        DataUpload.user_id == user_id
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if latest_upload:
        # Get sample transaction to inspect raw_data keys
        sample_txn = db.query(Transaction).filter(
            Transaction.upload_id == latest_upload.upload_id
        ).first()
        
        if sample_txn and sample_txn.raw_data:
            # Add all keys from raw_data JSONB
            for field_name in sample_txn.raw_data.keys():
                available_columns.add(field_name)
        
        # Get sample customer to inspect raw_data keys
        sample_cust = db.query(Customer).filter(
            Customer.upload_id == latest_upload.upload_id
        ).first()
        
        if sample_cust and sample_cust.raw_data:
            # Add all keys from customer raw_data
            for field_name in sample_cust.raw_data.keys():
                available_columns.add(field_name)
    
    # 3. Determine missing fields
    missing = [f for f in required_fields if f and f not in available_columns]
    
    return {
        "status": "ok" if not missing else "missing_fields",
        "missing_fields": missing,
        "available_columns": sorted(list(available_columns))
    }

@router.post("/run")
async def start_simulation(
    request: RunRequest,
    background_tasks: BackgroundTasks,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db),
    x_db_url: Optional[str] = Header(None)
):
    user_id = user_data.get("sub")
    service = SimulationService(db)
    
    # Pass mappings and dates to create_run (Need to update Service signature)
    # Storing mappings in metadata or just passing to execution?
    # Ideally passing to execution.
    # Service logic updates needed.
    
    run = service.create_run(request.run_type, request.scenarios, user_id)
    
    # Pass the current DB URL to the background task
    target_url = resolve_db_url(x_db_url) or DEFAULT_DB_URL
    
    # Update Run Metadata with Mappings/Dates for context
    run.metadata_info = {
        "field_mappings": request.field_mappings,
        "date_range": {
            "start": request.date_range_start.isoformat() if request.date_range_start else None,
            "end": request.date_range_end.isoformat() if request.date_range_end else None
        }
    }
    
    # ✅ Explicitly save Date Ranges to columns
    if request.date_range_start:
        run.date_range_start = request.date_range_start
    if request.date_range_end:
        run.date_range_end = request.date_range_end
        
    db.commit()
    
    background_tasks.add_task(run_simulation_background, run.run_id, target_url)
    
    return {"run_id": run.run_id, "status": "pending"}

@router.get("/{run_id}/status")
async def get_status(
    run_id: str, 
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    from models import SimulationRun
    user_id = user_data.get("sub")
    
    run = db.query(SimulationRun).filter(
        SimulationRun.run_id == run_id,
        SimulationRun.user_id == user_id
    ).first()
    
    if not run:
        raise HTTPException(404, "Run not found or access denied")
    return {
        "run_id": run.run_id,
        "status": run.status,
        "total_alerts": run.total_alerts,
        "error": run.metadata_info
    }

@router.get("/runs")
async def list_simulation_runs(
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """List all completed simulation runs for the current user."""
    from models import SimulationRun, ScenarioConfig
    user_id = user_data.get("sub")
    
    runs = db.query(SimulationRun).filter(
        SimulationRun.user_id == user_id,
        SimulationRun.status == 'completed'
    ).order_by(SimulationRun.created_at.desc()).all()

    # Create mapping of Scenario ID -> Name
    all_scenarios = db.query(ScenarioConfig).all()
    scenario_map = {s.scenario_id: s.scenario_name for s in all_scenarios}
    
    results = []
    for r in runs:
        scenario_names = []
        if r.scenarios_run:
            for sid in r.scenarios_run:
                # Resolve ID to name, fallback to ID if not found
                scenario_names.append(scenario_map.get(sid, sid))

        results.append({
            "run_id": r.run_id,
            "run_type": r.run_type,
            "total_alerts": r.total_alerts,
            "created_at": r.created_at.isoformat(),
            "scenarios_run": r.scenarios_run, # Keep IDs for logic
            "scenario_names": scenario_names, # Add Names for display
            "status": r.status,
            "metadata_info": r.metadata_info
        })
    
    return results

@router.get("/{run_id}/alerts")
async def get_run_alerts(
    run_id: str, 
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    from models import Alert, SimulationRun
    user_id = user_data.get("sub")
    
    # Verify ownership
    run_exists = db.query(SimulationRun).filter(
        SimulationRun.run_id == run_id,
        SimulationRun.user_id == user_id
    ).first()
    
    if not run_exists:
        raise HTTPException(404, "Run not found or access denied")
        
    alerts = db.query(Alert).filter(Alert.run_id == run_id).all()
    return alerts

@router.get("/{run_id}/export/excel")
async def export_run_results(
    run_id: str, 
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    from models import Alert, SimulationRun
    user_id = user_data.get("sub")
    
    # Verify ownership
    run_exists = db.query(SimulationRun).filter(
        SimulationRun.run_id == run_id,
        SimulationRun.user_id == user_id
    ).first()
    
    if not run_exists:
        raise HTTPException(404, "Run not found or access denied")
    
    # Query alerts using pandas
    alerts_query = db.query(Alert).filter(Alert.run_id == run_id)
    df = pd.read_sql(alerts_query.statement, db.bind)
    
    if df.empty:
        # Create empty DF with headers if no data to avoid crash
        df = pd.DataFrame(columns=['alert_id', 'customer_id', 'scenario_id', 'risk_score'])

    # Clean up JSON columns for Excel
    if 'trigger_details' in df.columns:
        df['trigger_details'] = df['trigger_details'].astype(str)
        
    # Remove timezones from all datetime columns
    for col in df.select_dtypes(include=['datetime64[ns, UTC]', 'datetime64[ns]', 'datetime']).columns:
        df[col] = df[col].dt.tz_localize(None)
        
    # Generate Excel in memory
    output = io.BytesIO()
    # Ensure openpyxl is installed. If not, fallback to CSV could be considered, but user asked for Excel.
    # Assuming openpyxl is present (standard with pandas in many envs, or implied).
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Alerts')
    
    output.seek(0)
    
    return StreamingResponse(
        output, 
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        headers={"Content-Disposition": f"attachment; filename=simulation_results_{run_id}.xlsx"}
    )

@router.post("/preview")
async def preview_scenario(
    payload: dict,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    Test scenario logic - returns sample alerts without saving to DB
    """
    try:
        user_id = current_user.get('sub')
        limit = payload.get('limit', 5)
        
        # Get active upload
        active_upload = db.query(DataUpload).filter(
            DataUpload.user_id == user_id,
            DataUpload.status == 'active',
            DataUpload.expires_at > datetime.now(timezone.utc)
        ).order_by(DataUpload.upload_timestamp.desc()).first()
        
        if not active_upload:
            return {
                "status": "no_data",
                "message": "No active data upload found"
            }
        
        # Initialize Service
        from services.simulation_service import SimulationService
        service = SimulationService(db)
        
        # Build theoretical scenario config
        scenario_config = {
            "scenario_id": payload.get('scenario_id', 'PREVIEW_TEST'),
            "scenario_name": payload.get('scenario_name', 'Preview Test'),
            "config_json": payload.get('config_json', {}),
            "priority": "MEDIUM",
            "is_active": True,
            "field_mappings": payload.get('field_mappings') # Pass mappings if any
        }
        
        # Get customer list for this user
        # 1. Try active upload first
        customers = db.query(Customer.customer_id).filter(
            Customer.upload_id == active_upload.upload_id
        ).distinct().all()
        
        # 2. Fallback: If no customers in active upload (e.g. it was transactions only),
        # Find the most recent upload that has customers
        if not customers:
            recent_cust_upload = db.query(DataUpload).filter(
                DataUpload.user_id == user_id,
                DataUpload.record_count_customers > 0,
                DataUpload.status == 'active'
            ).order_by(DataUpload.upload_timestamp.desc()).first()
            
            if recent_cust_upload:
                print(f"[PREVIEW] Fallback: Using customers from upload {recent_cust_upload.upload_id}")
                customers = db.query(Customer.customer_id).filter(
                    Customer.upload_id == recent_cust_upload.upload_id
                ).distinct().all()
        
        customer_ids = [c[0] for c in customers]
        
        if not customer_ids:
            return {
                "status": "no_data",
                "message": "No customers found in active upload or recent history"
            }
        
        # Run simulation in preview mode (dry run, no DB writes)
        # Limit to 20 customers for preview speed
        print(f"[PREVIEW] Running scenario for sample of {min(20, len(customer_ids))} customers")
        
        alerts_df = service._execute_single_scenario(
            scenario_config=scenario_config,
            customer_ids=customer_ids[:20],
            upload_id=active_upload.upload_id,
            run_id='preview_run',
            user_id=user_id
        )
        
        if alerts_df is None or alerts_df.empty:
            return {
                "status": "success",
                "alert_count": 0,
                "sample_alerts": [],
                "sample_size": len(customer_ids),
                "estimated_monthly_volume": 0,
                "message": "No alerts generated with current configuration"
            }
        
        # Convert to sample alerts
        sample_alerts = []
        for _, row in alerts_df.head(limit).iterrows():
            # Check if alert_date is a Timestamp or datetime object
            a_date = row.get('alert_date', datetime.now())
            if hasattr(a_date, 'isoformat'):
                a_date = a_date.isoformat()
            else:
                a_date = str(a_date)
                
            sample_alerts.append({
                "customer_id": str(row.get('customer_id')),
                "alert_date": a_date,
                "trigger_details": {
                    "aggregated_value": float(row.get('aggregated_value', 0)) if row.get('aggregated_value') else 0,
                    "transaction_count": int(row.get('transaction_count', 0)) if row.get('transaction_count') else 0
                }
            })
        
        # Estimate monthly volume
        alert_count = len(alerts_df)
        total_customers = len(customer_ids)
        
        # Simple extrapolation: (alerts_in_sample / customers_in_sample) * total_customers
        # Multiplied by 1.5 as a loose "monthly scaling" factor
        estimated_monthly = int((alert_count / min(20, total_customers)) * total_customers * 1.5)
        
        return {
            "status": "success",
            "alert_count": alert_count,
            "sample_alerts": sample_alerts,
            "sample_size": total_customers,
            "estimated_monthly_volume": estimated_monthly
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"Preview failed: {str(e)}"
        }

def _apply_field_mappings(config: dict, mappings: dict) -> dict:
    """
    Recursively replaces field names in config JSON.
    Example: {"transaction_amount": "txn_amt"} replaces all occurrences.
    """
    import copy
    config = copy.deepcopy(config)
    
    def replace_in_obj(obj):
        if isinstance(obj, dict):
            for key, value in list(obj.items()):
                # Replace field references
                if key == 'field' and value in mappings:
                    obj[key] = mappings[value]
                elif key == 'group_by' and isinstance(value, list):
                    obj[key] = [mappings.get(v, v) for v in value]
                elif key == 'segment_field' and value in mappings:
                     obj[key] = mappings[value]
                else:
                    replace_in_obj(value)
        elif isinstance(obj, list):
            for item in obj:
                replace_in_obj(item)
    
    replace_in_obj(config)
    return config


================================================================================
FILE: backend/api/fields.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from sqlalchemy import text
from typing import List, Dict, Optional, Any
import json

from database import get_db
from auth import get_current_user
from models import FieldMetadata, FieldValueIndex, DataUpload
from core.redis_client import get_redis_client

router = APIRouter(prefix="/api/fields", tags=["Fields & Intelligence"])

@router.get("/discover")
async def discover_fields(
    table: str = Query(..., regex="^(transactions|customers)$"),
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get all discoverable fields and their metadata for the user's active data.
    """
    user_id = user_payload.get("sub")
    
    # 1. Get latest active upload
    upload = db.query(DataUpload).filter(
        DataUpload.user_id == user_id,
        DataUpload.status == 'active'
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if not upload:
        return {"fields": []}
        
    # 2. Get Metadata
    metadata_records = db.query(FieldMetadata).filter(
        FieldMetadata.upload_id == upload.upload_id,
        FieldMetadata.table_name == table
    ).all()
    
    results = []
    for m in metadata_records:
        results.append({
            "name": m.field_name,
            "type": m.field_type,
            "label": m.field_name.replace('_', ' ').title(),
            "stats": {
                "total": m.total_records,
                "distinct": m.distinct_count,
                "nulls": m.null_count
            },
            "operators": m.recommended_operators,
            "sample_values": m.sample_values
        })
        
    return {"fields": results}

@router.get("/{field_name}/values")
async def get_field_values(
    field_name: str,
    table: str = Query(..., regex="^(transactions|customers)$"),
    search: str = "",
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get autocomplete values for a field.
    Uses Redis caching (1 hour TTL).
    """
    user_id = user_payload.get("sub")
    redis_client = get_redis_client()
    
    # 1. Get Upload ID
    upload = db.query(DataUpload).filter(
        DataUpload.user_id == user_id,
        DataUpload.status == 'active'
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if not upload:
        return {"values": []}
    
    # 2. Redis Cache Key
    # Key format: fields:{upload_id}:{table}:{field_name}:values
    cache_key = f"fields:{upload.upload_id}:{table}:{field_name}:values"
    
    # 3. Check Cache (if no search term, or handle search filtering in memory if list small?)
    # For now, we cache the FULL list of values for the field, then filter in application
    # This avoids caching every search permutation.
    
    cached_data = None
    try:
        cached_data = redis_client.get(cache_key)
    except Exception as e:
        print(f"Redis error: {e}")
        
    if cached_data:
        all_values = json.loads(cached_data)
    else:
        # 4. Cache Miss - Query DB
        # Query FieldValueIndex table
        index_records = db.query(FieldValueIndex).filter(
            FieldValueIndex.upload_id == upload.upload_id,
            FieldValueIndex.table_name == table,
            FieldValueIndex.field_name == field_name
        ).order_by(FieldValueIndex.value_count.desc()).limit(100).all() # Cap at 100 for autocomplete
        
        all_values = [
            {
                "value": r.field_value, 
                "count": r.value_count, 
                "percentage": float(r.value_percentage) if r.value_percentage else 0
            } 
            for r in index_records
        ]
        
        # 5. Set Cache (TTL 1 hour = 3600 seconds)
        try:
            if all_values:
                redis_client.setex(cache_key, 3600, json.dumps(all_values))
        except Exception as e:
            print(f"Redis set error: {e}")

    # 6. Apply Search Filter
    if search:
        search_lower = search.lower()
        filtered = [v for v in all_values if search_lower in str(v['value']).lower()]
        return {"values": filtered[:20]} # Return top 20 matches
        
    return {"values": all_values[:20]} # Return top 20 by default

@router.get("/{field_name}/operators")
async def get_field_operators(
    field_name: str,
    table: str = Query(..., regex="^(transactions|customers)$"),
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get compatible operators for a field based on its type.
    """
    user_id = user_payload.get("sub")
    
    upload = db.query(DataUpload).filter(
        DataUpload.user_id == user_id,
        DataUpload.status == 'active'
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if not upload:
        return {"operators": ["equals"]} # Fallback
        
    metadata = db.query(FieldMetadata).filter(
        FieldMetadata.upload_id == upload.upload_id,
        FieldMetadata.table_name == table,
        FieldMetadata.field_name == field_name
    ).first()
    
    if metadata and metadata.recommended_operators:
        return {"operators": metadata.recommended_operators, "type": metadata.field_type}
        
    return {"operators": ["equals", "not_equals", "in"]} # Default fallback


================================================================================
FILE: backend/api/__init__.py
================================================================================



================================================================================
FILE: backend/api/rules.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from database import get_db
from models import ScenarioConfig
from auth import get_current_user
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import datetime
import uuid


router = APIRouter(prefix="/api/rules", tags=["Rules"])


class ScenarioUpdate(BaseModel):
    scenario_name: Optional[str] = None
    enabled: Optional[bool] = None
    config_json: Optional[Dict[str, Any]] = None
    thresholds: Optional[Dict[str, float]] = None
    refinements: Optional[List[Dict[str, Any]]] = None


class CreateScenarioRequest(BaseModel):
    scenario_id: Optional[str] = None
    scenario_name: str
    priority: str
    is_active: bool
    config_json: Dict[str, Any]  # ✅ This should contain filters, aggregation, threshold
    description: Optional[str] = None
    field_mappings: Optional[Dict[str, str]] = None


@router.post("/scenarios")
async def create_scenario(
    request: CreateScenarioRequest,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Creates a new scenario configuration."""
    try:
        user_id = user_data.get("sub")
        scenario_id = request.scenario_id or str(uuid.uuid4())[:8]
        
        # Check if scenario already exists
        existing = db.query(ScenarioConfig).filter(ScenarioConfig.scenario_id == scenario_id).first()
        if existing:
            raise HTTPException(400, f"Scenario ID '{scenario_id}' already exists")
        
        # Build config_json
        config_json = request.config_json.copy()
        config_json['scenario_id'] = scenario_id
        config_json['scenario_name'] = request.scenario_name
        config_json['description'] = request.description
        
        new_scenario = ScenarioConfig(
            scenario_id=scenario_id,
            user_id=user_id,
            scenario_name=request.scenario_name,
            description=request.description,
            priority=request.priority,
            enabled=request.is_active,
            config_json=config_json,
            field_mappings=request.field_mappings,
            frequency="daily",
            updated_at=datetime.datetime.utcnow()
        )
        
        db.add(new_scenario)
        db.commit()
        db.refresh(new_scenario)
        
        return {
            "status": "success",
            "scenario_id": scenario_id,
            "message": f"Scenario '{request.scenario_name}' created successfully"
        }
        
    except HTTPException as he:
        raise he
    except Exception as e:
        db.rollback()
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"Failed to create scenario: {str(e)}")


@router.get("/scenarios")
async def list_scenarios(
    user_data: dict = Depends(get_current_user), 
    db: Session = Depends(get_db)
):
    """List all scenarios for the current user."""
    user_id = user_data.get("sub")
    scenarios = db.query(ScenarioConfig).filter(ScenarioConfig.user_id == user_id).all()
    return scenarios


@router.get("/scenarios/{scenario_id}")
async def get_scenario(
    scenario_id: str, 
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Get a specific scenario by ID."""
    user_id = user_data.get("sub")
    scenario = db.query(ScenarioConfig).filter(
        ScenarioConfig.scenario_id == scenario_id,
        ScenarioConfig.user_id == user_id
    ).first()
    if not scenario:
        raise HTTPException(404, "Scenario not found")
    return scenario


@router.put("/scenarios/{scenario_id}")
async def update_scenario(
    scenario_id: str, 
    update: ScenarioUpdate, 
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Update an existing scenario."""
    user_id = user_data.get("sub")
    scenario = db.query(ScenarioConfig).filter(
        ScenarioConfig.scenario_id == scenario_id,
        ScenarioConfig.user_id == user_id
    ).first()
    
    if not scenario:
        raise HTTPException(404, "Scenario not found")
    
    if update.scenario_name is not None:
        scenario.scenario_name = update.scenario_name
    if update.enabled is not None:
        scenario.enabled = update.enabled
    if update.config_json is not None:
        # ✅ Merge with existing config_json
        existing_config = scenario.config_json or {}
        existing_config.update(update.config_json)
        scenario.config_json = existing_config
    if update.thresholds is not None:
        scenario.thresholds = update.thresholds
    if update.refinements is not None:
        scenario.refinements = update.refinements
        
    scenario.updated_at = datetime.datetime.utcnow()
        
    db.commit()
    db.refresh(scenario)
    return scenario


@router.delete("/scenarios/{scenario_id}")
async def delete_scenario(
    scenario_id: str,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Delete a scenario."""
    user_id = user_data.get("sub")
    scenario = db.query(ScenarioConfig).filter(
        ScenarioConfig.scenario_id == scenario_id,
        ScenarioConfig.user_id == user_id
    ).first()
    
    if not scenario:
        raise HTTPException(404, "Scenario not found")
    
    db.delete(scenario)
    db.commit()
    
    return {"status": "success", "message": f"Scenario '{scenario.scenario_name}' deleted"}


@router.patch("/scenarios/{scenario_id}/toggle")
async def toggle_scenario(
    scenario_id: str,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Toggle scenario active/inactive status."""
    user_id = user_data.get("sub")
    scenario = db.query(ScenarioConfig).filter(
        ScenarioConfig.scenario_id == scenario_id,
        ScenarioConfig.user_id == user_id
    ).first()
    
    if not scenario:
        raise HTTPException(404, "Scenario not found")
    
    scenario.enabled = not scenario.enabled
    db.commit()
    db.refresh(scenario)
    
    return {
        "status": "success",
        "enabled": scenario.enabled,
        "message": f"Scenario '{scenario.scenario_name}' {'enabled' if scenario.enabled else 'disabled'}"
    }


================================================================================
FILE: backend/api/README.md
================================================================================

# Backend API Endpoints

This directory contains all FastAPI route handlers organized by domain.

## 📁 API Structure

```
api/
├── data.py              # Data upload & TTL management
├── simulation.py        # Simulation execution & status
├── rules.py             # Scenario configuration
├── scenario_config.py   # Advanced scenario management
├── dashboard.py         # Dashboard statistics
├── comparison.py        # Simulation comparison
└── risk.py              # Risk analysis endpoints
```

## 🌐 API Endpoints Overview

### 1. Data API (`/api/data`)
**File:** `data.py`

**Endpoints:**
- `POST /upload/transactions` - Upload transaction CSV/Excel
- `POST /upload/customers` - Upload customer CSV/Excel
- `POST /ttl/extend` - Extend data TTL by 24 hours
- `GET /field-values` - Get unique field values for filtering

**Features:**
- File validation (CSV/Excel only)
- Size limits (10k transactions, 5k customers)
- TTL management (48h default)
- Bulk insert optimization
- Error handling with rollback

**Example:**
```bash
curl -X POST http://localhost:8000/api/data/upload/transactions \
  -F "file=@transactions.csv"
```

**Response:**
```json
{
    "status": "success",
    "records_uploaded": 1523,
    "upload_id": "uuid-here",
    "expires_at": "2024-01-20T15:30:00Z"
}
```

---

### 2. Simulation API (`/api/simulation`)
**File:** `simulation.py`

**Endpoints:**
- `POST /check-schema` - Validate scenario requirements
- `POST /run` - Start simulation (background task)
- `GET /{run_id}/status` - Get simulation status
- `GET /{run_id}/results` - Get simulation results
- `GET /{run_id}/download` - Download Excel report

**Features:**
- Schema validation before execution
- Background task execution
- Field mapping support
- Date range filtering
- Real-time status updates

**Example:**
```bash
curl -X POST http://localhost:8000/api/simulation/run \
  -H "Content-Type: application/json" \
  -d '{
    "scenarios": ["ICICI_01", "ICICI_44"],
    "run_type": "baseline",
    "field_mappings": {"amount": "transaction_amount"}
  }'
```

**Response:**
```json
{
    "run_id": "uuid-here",
    "status": "pending"
}
```

---

### 3. Rules API (`/api/rules`)
**File:** `rules.py`

**Endpoints:**
- `GET /scenarios` - List all scenarios
- `GET /scenarios/{id}` - Get scenario details
- `PUT /scenarios/{id}` - Update scenario
- `POST /refinements` - Apply refinements to run

**Features:**
- User-scoped scenarios
- Enable/disable scenarios
- Refinement rule management
- Default scenario fallback

---

### 4. Dashboard API (`/api/dashboard`)
**File:** `dashboard.py`

**Endpoints:**
- `GET /stats` - Get dashboard statistics

**Returns:**
```json
{
    "risk_score": "72.5",
    "active_high_risk_alerts": 45,
    "transactions_scanned": 15234,
    "system_coverage": "95%",
    "total_simulations": 127,
    "recent_simulations": [...]
}
```

---

## 🔐 Security

### Authentication
All endpoints (except `/health`, `/metrics`) require JWT authentication:
```python
user_data: dict = Depends(get_current_user)
```

### Rate Limiting
Default: 200 requests/minute per IP
```python
@limiter.limit("5/minute")  # Override for specific endpoint
async def sensitive_endpoint():
    ...
```

### CORS
Production origins configured via environment:
```bash
ALLOWED_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
```

---

## 📊 Request/Response Flow

```
Client Request
    ↓
Rate Limiter (200/min)
    ↓
CORS Validation
    ↓
JWT Authentication
    ↓
Request Logging (structlog)
    ↓
Endpoint Handler
    ↓
Database Operations
    ↓
Response Logging
    ↓
Client Response
```

---

## 🧪 Testing

Each API module has tests in `backend/tests/`:
```bash
pytest tests/test_simulation.py -v
pytest tests/test_data.py -v
pytest tests/test_auth.py -v
```

---

## 📝 API Documentation

Interactive docs available at:
- **Swagger UI:** `http://localhost:8000/docs`
- **ReDoc:** `http://localhost:8000/redoc`

---

## 🔧 Error Handling

All endpoints return consistent error format:
```json
{
    "detail": "Error message",
    "code": "ERROR_CODE"
}
```

**Common Status Codes:**
- `200` - Success
- `400` - Bad Request (validation error)
- `401` - Unauthorized (missing/invalid token)
- `403` - Forbidden (insufficient permissions)
- `413` - Payload Too Large (dataset limits)
- `422` - Unprocessable Entity (Pydantic validation)
- `429` - Too Many Requests (rate limit)
- `500` - Internal Server Error (logged to Sentry)

---

## 📈 Monitoring

### Metrics Endpoint
```bash
curl http://localhost:8000/metrics
```

Returns Prometheus metrics:
- Request count by endpoint
- Request duration histogram
- Error rates
- Active connections

### Health Check
```bash
curl http://localhost:8000/health
```

Returns:
```json
{
    "status": "healthy",
    "service": "sas-sandbox-simulator",
    "version": "1.0.0",
    "environment": "production"
}
```

---

## 🚀 Performance

### Optimization Techniques:
1. **Bulk Operations:** Use `bulk_insert_mappings` for data uploads
2. **Background Tasks:** Long-running simulations run async
3. **Database Indexing:** Indexed on customer_id, transaction_date
4. **Connection Pooling:** SQLAlchemy pool (size=20)

### Response Times (P95):
- Data upload (1k rows): < 500ms
- Simulation start: < 200ms
- Status check: < 50ms
- Dashboard stats: < 300ms


================================================================================
FILE: backend/api/comparison.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import Dict, Any, List
from database import get_db
from models import SimulationRun
from services.comparison_service import ComparisonEngine
from auth import get_current_user
import structlog

logger = structlog.get_logger("comparison_api")

router = APIRouter(prefix="/api/comparison", tags=["Comparison"])


class ComparisonRequest(BaseModel):
    """Request model for run comparison (accepts Run IDs or Scenario IDs)"""
    baseline_run_id: str
    refined_run_id: str


def resolve_to_run_id(db: Session, user_id: str, identifier: str) -> str:
    """
    Resolves an identifier to an actual SimulationRun ID.
    Supports: 
    1. Direct Run ID (UUID)
    2. Scenario ID (Resolves to latest completed run containing this scenario)
    """
    # 1. Try as direct Run ID
    run = db.query(SimulationRun).filter(
        SimulationRun.run_id == identifier,
        SimulationRun.user_id == user_id
    ).first()
    if run:
        return run.run_id
    
    # 2. Try as Scenario ID (latest completed run for this user)
    latest_run = db.query(SimulationRun).filter(
        SimulationRun.user_id == user_id,
        SimulationRun.status == 'completed'
    ).order_by(SimulationRun.created_at.desc()).all()
    
    for r in latest_run:
        if r.scenarios_run and identifier in r.scenarios_run:
            return r.run_id
            
    raise HTTPException(
        status_code=404, 
        detail=f"No completed simulation run found for rule: {identifier}"
    )


@router.post("/compare")
async def compare_runs(
    request: ComparisonRequest,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Compare two simulation runs or rules. Resolves IDs automatically.
    """
    user_id = user_data.get("sub")
    
    try:
        baseline_run_id = resolve_to_run_id(db, user_id, request.baseline_run_id)
        refined_run_id = resolve_to_run_id(db, user_id, request.refined_run_id)
        
        logger.info(
            "comparison_requested",
            raw_baseline=request.baseline_run_id,
            resolved_baseline=baseline_run_id,
            raw_refined=request.refined_run_id,
            resolved_refined=refined_run_id
        )
        
        engine = ComparisonEngine(db)
        return engine.compare_runs(baseline_run_id, refined_run_id)
        
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error("comparison_failed", error=str(e))
        raise HTTPException(status_code=500, detail=f"Comparison failed: {str(e)}")


@router.get("/runs/{run_id}/metadata")
async def get_run_metadata(
    run_id: str,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Get metadata for a simulation run.
    """
    user_id = user_data.get("sub")
    
    # Verify ownership
    run = db.query(SimulationRun).filter(
        SimulationRun.run_id == run_id,
        SimulationRun.user_id == user_id
    ).first()
    
    if not run:
         raise HTTPException(
                status_code=404,
                detail=f"Run {run_id} not found or access denied"
            )

    try:
        engine = ComparisonEngine(db)
        metadata = engine.get_run_metadata(run_id)
        
        if not metadata:
            raise HTTPException(
                status_code=404,
                detail=f"Run {run_id} metadata not found"
            )
        
        return metadata
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("metadata_fetch_failed", error=str(e), run_id=run_id)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch metadata: {str(e)}"
        )


@router.get("/diff")
async def compare_runs_legacy(
    baseline_id: str,
    refined_id: str,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Legacy endpoint - use POST /compare instead.
    """
    request = ComparisonRequest(
        baseline_run_id=baseline_id,
        refined_run_id=refined_id
    )
    return await compare_runs(request, user_data, db)
@router.get("/export")
async def export_comparison(
    baseline_run_id: str,
    refined_run_id: str,
    user_data: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Export the full granular difference between two runs as a CSV.
    """
    user_id = user_data.get("sub")
    
    try:
        baseline_id = resolve_to_run_id(db, user_id, baseline_run_id)
        refined_id = resolve_to_run_id(db, user_id, refined_run_id)
        
        engine = ComparisonEngine(db)
        csv_content = engine.export_comparison_csv(baseline_id, refined_id)
        
        from fastapi.responses import Response
        return Response(
            content=csv_content,
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename=comparison_diff_{baseline_run_id[:8]}_vs_{refined_run_id[:8]}.csv"
            }
        )
        
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error("export_failed", error=str(e))
        raise HTTPException(status_code=500, detail=f"Export failed: {str(e)}")


================================================================================
FILE: backend/api/dashboard.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import func
from database import get_db
from models import Transaction, Alert, SimulationRun, ScenarioConfig
from typing import List, Optional
from pydantic import BaseModel
from datetime import datetime
from auth import get_current_user

router = APIRouter(prefix="/api/dashboard", tags=["Dashboard"])

class SimulationSummary(BaseModel):
    run_id: str
    run_type: str
    created_at: datetime
    total_alerts: int
    status: str

class DashboardStats(BaseModel):
    risk_score: str
    active_high_risk_alerts: int
    transactions_scanned: int
    system_coverage: str
    total_simulations: int
    recent_simulations: List[SimulationSummary]

@router.get("/stats", response_model=DashboardStats)
async def get_dashboard_stats(
    user_payload: dict = Depends(get_current_user), # Secure endpoint
    db: Session = Depends(get_db)
):
    user_id = user_payload.get("sub")

    # 1. Transaction Count (Scoped to User)
    from models import DataUpload
    tx_count = db.query(Transaction).join(DataUpload).filter(DataUpload.user_id == user_id).count()
    
    # 2. High Risk Alerts (Scoped to User)
    high_risk_count = db.query(Alert).join(SimulationRun).filter(
        SimulationRun.user_id == user_id,
        Alert.risk_classification == 'HIGH', 
        Alert.alert_status == 'OPN'
    ).count()
    
    # 3. Recent Simulations (Scoped to User)
    total_simulations = db.query(SimulationRun).filter(SimulationRun.user_id == user_id).count()
    
    recent_runs_db = db.query(SimulationRun)\
        .filter(SimulationRun.user_id == user_id)\
        .order_by(SimulationRun.created_at.desc())\
        .limit(5)\
        .all()
        
    recent_runs = [
        SimulationSummary(
            run_id=run.run_id,
            run_type=run.run_type,
            created_at=run.created_at,
            total_alerts=run.total_alerts or 0,
            status=run.status
        ) for run in recent_runs_db
    ]
    
    # 4. Risk Score
    risk_level = "Low"
    if high_risk_count > 50:
        risk_level = "High"
    elif high_risk_count > 10:
        risk_level = "Medium"
        
    # 5. System Coverage (Scoped to User's Scenarios)
    total_scenarios = db.query(ScenarioConfig).filter(ScenarioConfig.user_id == user_id).count()
    enabled_scenarios = db.query(ScenarioConfig).filter(
        ScenarioConfig.user_id == user_id, 
        ScenarioConfig.enabled == True
    ).count()
    
    coverage = "0%"
    if total_scenarios > 0:
        pct = (enabled_scenarios / total_scenarios) * 100
        coverage = f"{pct:.1f}%"
    
    if total_scenarios == 0:
        coverage = "100%" 

    return DashboardStats(
        risk_score=risk_level,
        active_high_risk_alerts=high_risk_count,
        transactions_scanned=tx_count,
        system_coverage=coverage,
        total_simulations=total_simulations,
        recent_simulations=recent_runs
    )


================================================================================
FILE: backend/api/admin.py
================================================================================

"""
Admin API endpoints for manual operations

Provides endpoints for:
- Manual TTL cleanup trigger
- System health checks
- Administrative operations
"""

from fastapi import APIRouter, Depends, HTTPException, Request
from sqlalchemy.orm import Session
from database import get_service_engine
from core.ttl_manager import TTLManager
from auth import get_current_user
from typing import Dict, Any

router = APIRouter(prefix="/api/admin", tags=["Admin"])


from models import AuditLog
import uuid
from core.rate_limiting import limiter
from slowapi import Limiter
from slowapi.util import get_remote_address

# If core.rate_limiting import fails or we want local isolation:
# limiter = Limiter(key_func=get_remote_address) 

@router.post("/cleanup-ttl")
@limiter.limit("5/hour")
async def manual_cleanup(
    request: Request, # Required by slowapi
    dry_run: bool = True,  # Default to dry-run for safety
    current_user: dict = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Manual TTL cleanup trigger.
    
    Allows administrators to manually trigger the TTL cleanup process
    instead of waiting for the scheduled Celery task.
    """
    
    # Optional: Add admin role check
    # In production, check for specific role claim
    if current_user.get("role") not in ["admin", "superuser"]:
        print(f"Unauthorized admin access attempt: {current_user}")
        raise HTTPException(status_code=403, detail="Admin access required")
    
    # Use proper dependency injection if possible, or context manager
    # Here we stick to get_service_engine but ensure closure via final block
    
    sess_gen = get_service_engine()
    db = sess_gen()
    
    try:
        result = TTLManager.cleanup_expired(db, dry_run=dry_run)
        
        # Audit Log
        if not dry_run:
            try:
                audit = AuditLog(
                    log_id=str(uuid.uuid4()),
                    user_id=current_user.get("sub"),
                    action_type="ttl_cleanup_manual",
                    details={"dry_run": dry_run, "result": result}
                )
                db.add(audit)
                db.commit()
            except Exception as e:
                print(f"Failed to write audit log: {e}")
                # Don't fail the main request
        
        return {
            "status": "success",
            "dry_run": dry_run,
            "result": result,
            "message": "Dry run completed - no data modified" if dry_run else "Cleanup completed successfully"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Cleanup failed: {str(e)}"
        )
        
    finally:
        db.close()


@router.get("/ttl-status")
async def get_ttl_status(
    current_user: dict = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Get current TTL status and statistics.
    
    Returns information about:
    - Active uploads and their expiry times
    - Anonymized alert counts
    - Expired data counts
    
    Returns:
        TTL status statistics
    """
    from sqlalchemy import text
    from datetime import datetime, timezone
    
    db = get_service_engine()()
    
    try:
        now = datetime.now(timezone.utc)
        
        # Count active uploads
        active_uploads = db.execute(
            text("SELECT COUNT(*) FROM data_uploads WHERE status = 'active'")
        ).scalar()
        
        # Count expired uploads
        expired_uploads = db.execute(
            text("SELECT COUNT(*) FROM data_uploads WHERE expires_at < :now AND status = 'active'"),
            {"now": now}
        ).scalar()
        
        # Count anonymized alerts
        anonymized_alerts = db.execute(
            text("SELECT COUNT(*) FROM alerts WHERE is_anonymized = true")
        ).scalar()
        
        # Count active alerts
        active_alerts = db.execute(
            text("SELECT COUNT(*) FROM alerts WHERE is_anonymized = false")
        ).scalar()
        
        return {
            "status": "success",
            "timestamp": now.isoformat(),
            "statistics": {
                "active_uploads": active_uploads,
                "expired_uploads_pending_cleanup": expired_uploads,
                "anonymized_alerts": anonymized_alerts,
                "active_alerts": active_alerts,
                "total_alerts": anonymized_alerts + active_alerts
            }
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get TTL status: {str(e)}"
        )
        
    finally:
        db.close()


================================================================================
FILE: backend/api/risk.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from database import get_db
from models import SimulationRun
from core.risk_engine import RiskEngine
from auth import get_current_user
from pydantic import BaseModel
from typing import List, Dict, Any

router = APIRouter(prefix="/api/risk", tags=["Risk Analysis"])

class RiskAnalysisRequest(BaseModel):
    baseline_run_id: str
    refinements: List[Dict[str, Any]] # e.g. [{"type": "event_based", "excluded_events": ["education"]}]

@router.post("/analyze")
async def analyze_risk(
    request: RiskAnalysisRequest, 
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Analyze the risk gap of proposed refinements BEFORE running a simulation.
    """
    user_id = user_payload.get("sub")
    
    # 1. Verify Baseline Run Ownership
    run = db.query(SimulationRun).filter(
        SimulationRun.run_id == request.baseline_run_id,
        SimulationRun.user_id == user_id
    ).first()
    
    if not run:
        raise HTTPException(404, "Baseline run not found or access denied")

    engine = RiskEngine(db)
    
    try:
        # 2. Pass user_id to engine for scoped analysis
        report = engine.analyze_risk_gap(request.refinements, request.baseline_run_id, user_id=user_id)
        return report
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


================================================================================
FILE: backend/api/investigation.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import desc
from typing import Optional, List
from datetime import datetime
from pydantic import BaseModel

from database import get_db
from models import Alert, AlertTransaction, Transaction, Customer
from auth import get_current_user

router = APIRouter(prefix="/api/investigation", tags=["Investigation"])

# --- Schemas ---

class AlertWorkflowUpdate(BaseModel):
    assigned_to: Optional[str] = None
    investigation_status: Optional[str] = None # New, In Progress, Closed
    outcome: Optional[str] = None # False Positive, True Positive, Suspicious
    sar_reference: Optional[str] = None
    investigation_notes: Optional[str] = None

class TraceabilityItem(BaseModel):
    transaction_id: str
    contribution_percentage: float
    amount: float
    date: datetime
    beneficiary: Optional[str]
    description: Optional[str]

class AlertDetailResponse(BaseModel):
    alert_id: str
    scenario_name: str
    risk_score: int
    alert_date: datetime
    customer_name: str
    customer_id: str
    status: str
    # Investigation Fields
    assigned_to: Optional[str]
    investigation_status: str
    outcome: Optional[str]
    sar_reference: Optional[str]
    investigation_notes: Optional[str]
    
    # Traceability
    contributing_transactions: List[TraceabilityItem]
    
    # Raw Metadata
    trigger_details: dict

# --- Endpoints ---

@router.get("/alerts/{alert_id}", response_model=AlertDetailResponse)
async def get_alert_details(
    alert_id: str,
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Get comprehensive details for a specific alert, including:
    - Contributing transactions (Traceability)
    - Investigation status & notes
    - Customer context
    """
    alert = db.query(Alert).options(
        joinedload(Alert.alert_transactions).joinedload(AlertTransaction.transaction),
        joinedload(Alert.customer)
    ).filter(Alert.alert_id == alert_id).first()
    
    if not alert:
        raise HTTPException(404, "Alert not found")
        
    # Build Traceability List
    trace_items = []
    # Optimized Traceability Fetch (Avoids N+1 JSON deserialization)
    from sqlalchemy import text
    
    # Fetch specifics directly via DB to minimize Python overhead
    trace_rows = db.query(
        AlertTransaction.transaction_id,
        AlertTransaction.contribution_percentage,
        Transaction.created_at,
        Transaction.transaction_amount,
        text("transactions.raw_data ->> 'transaction_amount'"),
        text("transactions.raw_data ->> 'beneficiary_name'"),
        text("transactions.raw_data ->> 'transaction_narrative'")
    ).select_from(AlertTransaction)\
     .join(AlertTransaction.transaction)\
     .filter(AlertTransaction.alert_id == alert_id)\
     .all()

    for r in trace_rows:
        # Unpack
        tid, contrib, tdate, amt_col, amt_json, ben_json, narr_json = r
        
        # Logic: Use JSON value if present, else Column
        final_amount = float(amt_json) if amt_json else (float(amt_col) if amt_col else 0.0)
        
        trace_items.append({
            "transaction_id": tid,
            "contribution_percentage": float(contrib) if contrib else 0,
            "amount": final_amount,
            "date": tdate,
            "beneficiary": ben_json or "Unknown",
            "description": narr_json or "N/A"
        })
    
    # sort by contribution desc
    trace_items.sort(key=lambda x: x['contribution_percentage'], reverse=True)

    return {
        "alert_id": alert.alert_id,
        "scenario_name": alert.scenario_name,
        "risk_score": alert.risk_score,
        "alert_date": alert.alert_date,
        "customer_name": alert.customer_name or "Unknown",
        "customer_id": alert.customer_id,
        "status": alert.alert_status,
        
        "assigned_to": alert.assigned_to,
        "investigation_status": alert.investigation_status or "New",
        "outcome": alert.outcome,
        "sar_reference": alert.sar_reference,
        "investigation_notes": alert.investigation_notes,
        
        "contributing_transactions": trace_items,
        "trigger_details": alert.trigger_details or {}
    }

@router.post("/alerts/{alert_id}/workflow")
async def update_investigation_workflow(
    alert_id: str,
    update: AlertWorkflowUpdate,
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Update the investigation state of an alert.
    """
    alert = db.query(Alert).filter(Alert.alert_id == alert_id).first()
    if not alert:
        raise HTTPException(404, "Alert not found")
        
    # Apply updates
    if update.assigned_to is not None:
        alert.assigned_to = update.assigned_to
    
    if update.investigation_status:
        alert.investigation_status = update.investigation_status
        # Sync to main status?
        if update.investigation_status == 'Closed':
            alert.alert_status = 'CLS'
        elif update.investigation_status == 'In Progress':
            alert.alert_status = 'WIP'
            
    if update.outcome:
        alert.outcome = update.outcome
        
    if update.sar_reference:
        alert.sar_reference = update.sar_reference
        
    if update.investigation_notes:
        # Append or Replace? Let's Replace for atomic updates, or append with timestamp
        # Ideally, separate Note model. For now, simple text field.
        # Let's append if existing
        if alert.investigation_notes:
            timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M")
            alert.investigation_notes += f"\n[{timestamp}] {update.investigation_notes}"
        else:
            alert.investigation_notes = update.investigation_notes
            
    # Touch updated_at is automatic via onupdate
    
    db.commit()
    db.refresh(alert)
    
    return {"status": "success", "message": "Workflow updated", "current_status": alert.investigation_status}


================================================================================
FILE: backend/api/data.py
================================================================================

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from sqlalchemy.orm import Session
from sqlalchemy import text, inspect
from datetime import datetime, timezone
import pandas as pd
import io
import json

from database import get_db
from services.data_ingestion import DataIngestionService
from models import Transaction, DataUpload, Alert, SimulationRun, Customer, FieldValueIndex, FieldMetadata, Account, AlertTransaction
from core.upload_validator import UploadValidator
from core.ttl_manager import TTLManager
from auth import get_current_user

router = APIRouter(prefix="/api/data", tags=["Data"])

@router.post("/upload/transactions")
async def upload_transactions(
    file: UploadFile = File(...),
    force_replace: bool = False,
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    user_id = user_payload.get("sub")
    if not file.filename.endswith(('.csv', '.xls', '.xlsx')):
        raise HTTPException(400, "Only CSV and Excel files are supported")
    
    content = await file.read()
    service = DataIngestionService()
    
    try:
        # Changed to unpack 3 values
        valid_records, errors, computed_index = service.process_transactions_csv(content, file.filename)
        
        # [DEBUG]
        print(f"[DEBUG] Upload Transactions File: {file.filename}")
        print(f"[DEBUG] Valid Transaction Records: {len(valid_records)}")
        if valid_records:
            print(f"[DEBUG] First 3 Txn IDs: {[r.get('transaction_id') for r in valid_records[:3]]}")
    except Exception as e:
         raise HTTPException(400, str(e))
    
    if not valid_records:
        raise HTTPException(400, "No valid records found. Please ensure headers match: transaction_id, customer_id, etc.")

    # Convert to DataFrame for validation
    if valid_records:
        df = pd.DataFrame(valid_records)
        
        # SIZE VALIDATION
        validation = UploadValidator.validate_size(df, "transactions")
        if not validation["allowed"]:
            raise HTTPException(413, detail={
                "error": "dataset_too_large",
                "count": validation["count"],
                "max_allowed": validation["max_allowed"],
                "message": validation["message"],
                "recommendation": "connect_external_db"
            })
        
    # ===== UPLOAD ID DECISION LOGIC =====
    upload_id = None
    expires_at = None
    should_merge = False
    
    # CHECK FOR EXISTING DATA
    existing_upload_record = db.query(DataUpload).filter(
        DataUpload.user_id == user_id,
        DataUpload.status == 'active',
        DataUpload.expires_at > datetime.now(timezone.utc)
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if existing_upload_record and not force_replace:
        upload_age = (datetime.now(timezone.utc) - existing_upload_record.upload_timestamp).total_seconds()
        
        # Same file check
        if existing_upload_record.filename == file.filename and \
           abs(existing_upload_record.record_count_transactions - len(valid_records)) < 10:
            TTLManager.extend_ttl(db, existing_upload_record.upload_id, additional_hours=24)
            return {
                "status": "extended",
                "message": "Existing data found. TTL extended by 24 hours.",
                "upload_id": str(existing_upload_record.upload_id),
                "expires_at": (existing_upload_record.expires_at + pd.Timedelta(hours=24)).isoformat(),
                "records_count": existing_upload_record.record_count_transactions,
                "action": "ttl_extended"
            }
        
        # Merge check: customers exist, transactions don't, recent upload
        if existing_upload_record.record_count_customers > 0 and \
           existing_upload_record.record_count_transactions == 0 and \
           upload_age < 300:
            # MERGE MODE
            upload_id = existing_upload_record.upload_id
            expires_at = existing_upload_record.expires_at
            should_merge = True
            
            # Update record
            existing_upload_record.record_count_transactions = len(valid_records)
            existing_upload_record.filename = f"{existing_upload_record.filename}+{file.filename}"
            db.commit()
        else:
            # Conflict
            raise HTTPException(409, detail={
                "error": "existing_data_conflict",
                "message": f"Active data exists ({existing_upload_record.filename}). Use force_replace=true to replace.",
                "existing_upload_id": str(existing_upload_record.upload_id),
                "expires_at": existing_upload_record.expires_at.isoformat(),
                "suggestion": "Add ?force_replace=true to URL"
            })
    
    # CREATE NEW UPLOAD (only if not merging)
    if not should_merge:
        upload_id = TTLManager.create_upload_record(
            db=db,
            user_id=user_id,
            filename=file.filename,
            txn_count=len(valid_records),
            cust_count=0,
            schema_snapshot={"columns": list(df.columns)},
            ttl_hours=48
        )
        expires_at = TTLManager.set_expiry(48)
    
    # ===== DATA INSERTION =====
    # ===== DATA INSERTION =====
    for record in valid_records:
        record['upload_id'] = upload_id
        record['expires_at'] = expires_at

    try:
        # Clear old data (only if NOT merging)
        if not should_merge:
            prev_upload_ids = [u.upload_id for u in db.query(DataUpload.upload_id).filter(
                DataUpload.user_id == user_id,
                DataUpload.upload_id != upload_id
            ).all()]
            prev_run_ids = [r.run_id for r in db.query(SimulationRun.run_id).filter(SimulationRun.user_id == user_id).all()]
            
            prev_alert_ids = [a.alert_id for a in db.query(Alert.alert_id).filter(Alert.run_id.in_(prev_run_ids)).all()]
            
            if prev_alert_ids:
                db.query(AlertTransaction).filter(AlertTransaction.alert_id.in_(prev_alert_ids)).delete(synchronize_session=False)
                db.query(Alert).filter(Alert.run_id.in_(prev_run_ids)).delete(synchronize_session=False)
            
            if prev_upload_ids:
                db.query(Transaction).filter(Transaction.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                db.query(FieldValueIndex).filter(FieldValueIndex.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                db.query(FieldMetadata).filter(FieldMetadata.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
            
            db.commit()  # Commit deletion before insert
        
        # USE UPSERT FOR TRANSACTIONS
        print(f"[UPLOAD] Upserting {len(valid_records)} transactions...")
        
        from sqlalchemy import insert
        
        # USE BATCH UPSERT FOR TRANSACTIONS (much faster!)
        print(f"[UPLOAD] Upserting {len(valid_records)} transactions...")
        
        # Use RAW psycopg2 cursor to bypass SQLAlchemy parameter issues
        connection = db.connection().connection
        cursor = connection.cursor()
        
        # Deduplicate
        unique_txns = {r['transaction_id']: r for r in valid_records}
        valid_records = list(unique_txns.values())
        
        batch_size = 500
        for i in range(0, len(valid_records), batch_size):
            batch = valid_records[i:i+batch_size]
            placeholders = []
            values = []
            
            for record in batch:
                placeholders.append("(%s, %s, %s::uuid, %s::jsonb, %s, %s)")
                values.extend([
                    record['transaction_id'],
                    record.get('customer_id'),
                    str(record['upload_id']),
                    json.dumps(record['raw_data']),
                    record['expires_at'],
                    record.get('created_at', datetime.now(timezone.utc))
                ])
            
            sql = f"""
                INSERT INTO transactions (transaction_id, customer_id, upload_id, raw_data, expires_at, created_at)
                VALUES {','.join(placeholders)}
                ON CONFLICT (transaction_id, upload_id)
                DO UPDATE SET
                    customer_id = EXCLUDED.customer_id,
                    raw_data = EXCLUDED.raw_data,
                    expires_at = EXCLUDED.expires_at,
                    created_at = EXCLUDED.created_at
            """
            cursor.execute(sql, values)
            print(f"[UPLOAD] Processed {min(i+batch_size, len(valid_records))}/{len(valid_records)} transactions")
        
        cursor.close()
        print(f"[UPLOAD] Upserted {len(valid_records)} transactions")
        
        # Save Field Metadata & Index
        print(f"[UPLOAD] Saving {len(computed_index)} field indices...")
        for field_name, data in computed_index.items():
            metadata = data['metadata']
            values = data['values']
            
            # 1. Save Metadata
            db_metadata = FieldMetadata(
                upload_id=upload_id,
                table_name='transactions',
                **metadata
            )
            db.add(db_metadata)
            
            # 2. Save Values
            for val in values:
                db_val = FieldValueIndex(
                    upload_id=upload_id,
                    table_name='transactions',
                    field_name=field_name,
                    **val
                )
                db.add(db_val)
        
        db.commit()
        print(f"[UPLOAD] Successfully committed all data")
        
    except Exception as e:
        print(f"[ERROR] Database insertion failed: {str(e)}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(400, f"Database error: {str(e)}")

    
    return {
        "status": "success",
        "records_uploaded": len(valid_records),
        "errors": len(errors),
        "error_sample": errors[:5] if errors else [],
        "upload_id": str(upload_id) if valid_records else None,
        "expires_at": expires_at.isoformat() if valid_records else None,
        "action": "merged" if should_merge else "new_upload"
    }

@router.get("/schema")
async def get_data_schema(
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    Returns the schema (columns) for Transactions and Customers by extracting field names
    from raw_data JSONB of the user's most recent upload.
    
    This enables schema-agnostic uploads - the system discovers fields dynamically
    from the uploaded CSV data rather than relying on fixed database columns.
    """
    from models import Transaction, Customer, DataUpload
    
    user_id = current_user.get('sub')  # Extract user_id from JWT payload
    
    schema_response = {"transactions": [], "customers": []}
    
    # Helper function to infer type from value
    def infer_type(value):
        if value is None:
            return 'string'
        if isinstance(value, (int, float)):
            return 'number' if isinstance(value, float) else 'integer'
        if isinstance(value, bool):
            return 'boolean'
        # Try to parse as number
        try:
            float(str(value))
            return 'number'
        except:
            pass
        return 'string'
    
    # Get user's most recent upload
    latest_upload = db.query(DataUpload).filter(
        DataUpload.user_id == user_id
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if not latest_upload:
        # Return empty schema if no uploads yet
        return schema_response
    
    # Extract transaction fields from raw_data
    sample_txn = db.query(Transaction).filter(
        Transaction.upload_id == latest_upload.upload_id
    ).first()
    
    if sample_txn and sample_txn.raw_data:
        for field_name, field_value in sample_txn.raw_data.items():
            schema_response["transactions"].append({
                "name": field_name,
                "type": infer_type(field_value),
                "label": field_name.replace('_', ' ').title(),
                "sql_type": infer_type(field_value)
            })
    
    # Extract customer fields from raw_data
    # Extract customer fields from raw_data
    sample_cust = db.query(Customer).filter(
        Customer.upload_id == latest_upload.upload_id
    ).first()
    
    if sample_cust and sample_cust.raw_data:
        for field_name, field_value in sample_cust.raw_data.items():
            schema_response["customers"].append({
                "name": field_name,
                "type": infer_type(field_value),
                "label": field_name.replace('_', ' ').title(),
                "sql_type": infer_type(field_value)
            })
            
    # Fallback: If no customers in current upload, try to find ANY customer to infer schema
    if not schema_response["customers"]:
        any_sample_cust = db.query(Customer).order_by(Customer.created_at.desc()).first()
        if any_sample_cust and any_sample_cust.raw_data:
             for field_name, field_value in any_sample_cust.raw_data.items():
                schema_response["customers"].append({
                    "name": field_name,
                    "type": infer_type(field_value),
                    "label": field_name.replace('_', ' ').title(),
                    "sql_type": infer_type(field_value)
                })

    # Hard Fallback: If still no customer schema, use defaults
    if not schema_response["customers"]:
        schema_response["customers"] = [
            {"name": "customer_type", "type": "string", "label": "Customer Type"},
            {"name": "occupation", "type": "string", "label": "Occupation"},
            {"name": "annual_income", "type": "number", "label": "Annual Income"},
            {"name": "risk_score", "type": "number", "label": "Risk Score"},
            {"name": "customer_id", "type": "string", "label": "Customer ID"}
        ]

    # Fallback to basic schema if no data found (Transactions only)
    if not schema_response["transactions"]:
         schema_response["transactions"] = [
                {"name": "transaction_amount", "type": "number", "label": "Transaction Amount"},
                {"name": "transaction_type", "type": "string", "label": "Transaction Type"},
                {"name": "channel", "type": "string", "label": "Channel"},
                {"name": "debit_credit_indicator", "type": "string", "label": "D/C Indicator"},
                {"name": "transaction_narrative", "type": "string", "label": "Narrative"},
                {"name": "beneficiary_name", "type": "string", "label": "Beneficiary Name"},
                {"name": "beneficiary_bank", "type": "string", "label": "Beneficiary Bank"},
                {"name": "transaction_date", "type": "date", "label": "Date"}
            ]

    return schema_response

@router.post("/upload/customers")
async def upload_customers(
    file: UploadFile = File(...),
    force_replace: bool = False,
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    user_id = user_payload.get("sub")
    
    # 1. Validate file type
    if not file.filename.endswith(('.csv', '.xls', '.xlsx')):
        raise HTTPException(400, "Only CSV and Excel files are supported")
    
    # 2. Process file
    content = await file.read()
    service = DataIngestionService()
    
    try:
        valid_records, errors, computed_index, extracted_accounts = service.process_customers_csv(content, file.filename)
        
        # [DEBUG]
        print(f"[DEBUG] Upload Customers File: {file.filename}")
        print(f"[DEBUG] Valid Customer Records: {len(valid_records)}")
        if valid_records:
            print(f"[DEBUG] First 3 Cust IDs: {[r.get('customer_id') for r in valid_records[:3]]}")
    except Exception as e:
        raise HTTPException(400, str(e))
    
    if not valid_records:
        raise HTTPException(400, "No valid records found. Please ensure headers match customer_id, customer_name, etc.")
    
    # 3. Size validation
    df = pd.DataFrame(valid_records)
    validation = UploadValidator.validate_size(df, "customers")
    
    if not validation['allowed']:
        raise HTTPException(413, detail={
            "error": "dataset_too_large",
            "count": validation['count'],
            "max_allowed": validation['max_allowed'],
            "message": validation['message'],
            "recommendation": "connect_external_db"
        })
    
    # 4. Check for existing data
    existing_upload_record = db.query(DataUpload).filter(
        DataUpload.user_id == user_id,
        DataUpload.status == 'active',
        DataUpload.expires_at > datetime.now(timezone.utc)
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    upload_id = None
    expires_at = None
    should_merge = False
    
    # FORCE REPLACE FIRST
    if existing_upload_record and force_replace:
        try:
            print(f"[FORCE_REPLACE] Deleting existing upload: {existing_upload_record.upload_id}")
            
            # Find all previous upload IDs for this user
            prev_upload_ids = [u.upload_id for u in db.query(DataUpload.upload_id).filter(
                DataUpload.user_id == user_id
            ).all()]
            
            print(f"[FORCE_REPLACE] Found {len(prev_upload_ids)} previous uploads: {prev_upload_ids}")
            
            # Find all previous run IDs
            prev_run_ids = [r.run_id for r in db.query(SimulationRun.run_id).filter(
                SimulationRun.user_id == user_id
            ).all()]
            
            print(f"[FORCE_REPLACE] Found {len(prev_run_ids)} previous runs")
            
            # Delete cascade (in correct order to respect foreign keys)
            if prev_run_ids:
                # 1. Delete AlertTransaction (if exists)
                try:
                    alert_txn_count = db.query(AlertTransaction).filter(
                        AlertTransaction.alert_id.in_(
                            db.query(Alert.alert_id).filter(Alert.run_id.in_(prev_run_ids))
                        )
                    ).delete(synchronize_session=False)
                    print(f"[FORCE_REPLACE] Deleted {alert_txn_count} alert_transactions")
                except:
                    pass  # Table might not exist
                
                # 2. Delete Alerts
                alert_count = db.query(Alert).filter(Alert.run_id.in_(prev_run_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {alert_count} alerts")
                
                # 3. Delete Simulation Runs
                run_count = db.query(SimulationRun).filter(SimulationRun.run_id.in_(prev_run_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {run_count} runs")
            
            if prev_upload_ids:
                # 4. Delete Transactions
                txn_count = db.query(Transaction).filter(Transaction.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {txn_count} transactions")
                
                # 5. Delete Accounts
                acc_count = db.query(Account).filter(Account.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {acc_count} accounts")
                
                # 6. Delete Field Indices
                idx_count = db.query(FieldValueIndex).filter(FieldValueIndex.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {idx_count} field value indices")
                
                meta_count = db.query(FieldMetadata).filter(FieldMetadata.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {meta_count} field metadata")
                
                # 7. Delete Customers (MUST be after Alerts are deleted due to FK)
                cust_count = db.query(Customer).filter(Customer.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {cust_count} customers")
                
                # 8. Delete DataUpload records
                upload_count = db.query(DataUpload).filter(DataUpload.upload_id.in_(prev_upload_ids)).delete(synchronize_session=False)
                print(f"[FORCE_REPLACE] Deleted {upload_count} data uploads")
            
            # CRITICAL: Commit the deletion BEFORE creating new records
            db.commit()
            print(f"[FORCE_REPLACE] Deletion committed successfully")
            
            existing_upload_record = None
            
        except Exception as e:
            print(f"[ERROR] Force replace deletion failed: {str(e)}")
            import traceback
            traceback.print_exc()
            db.rollback()
            raise HTTPException(500, f"Failed to delete old data: {str(e)}")
    
    # 5. Handle existing data (if not force_replace)
    if existing_upload_record and not force_replace:
        upload_age = (datetime.now(timezone.utc) - existing_upload_record.upload_timestamp).total_seconds()
        
        # Same file check (extend TTL)
        if (existing_upload_record.filename == file.filename and 
            abs(existing_upload_record.record_count_customers - len(valid_records)) <= 5):
            
            TTLManager.extend_ttl(db, existing_upload_record.upload_id, additional_hours=24)
            return {
                "status": "extended",
                "message": "Existing data found. TTL extended by 24 hours.",
                "upload_id": str(existing_upload_record.upload_id),
                "expires_at": (existing_upload_record.expires_at + pd.Timedelta(hours=24)).isoformat(),
                "records_count": existing_upload_record.record_count_customers,
                "action": "ttl_extended"
            }
        
        # Merge check (transactions exist, customers don't, recent upload)
        if (existing_upload_record.record_count_transactions > 0 and 
            existing_upload_record.record_count_customers == 0 and 
            upload_age < 300):
            
            upload_id = existing_upload_record.upload_id
            expires_at = existing_upload_record.expires_at
            should_merge = True
            
            # Update record
            existing_upload_record.record_count_customers = len(valid_records)
            existing_upload_record.filename = f"{existing_upload_record.filename}+{file.filename}"
            db.commit()
        else:
            # Conflict - ask user to force replace
            raise HTTPException(409, detail={
                "error": "existing_data_conflict",
                "message": f"Active data exists ({existing_upload_record.filename}). Use force_replace=true to replace.",
                "existing_upload_id": str(existing_upload_record.upload_id),
                "expires_at": existing_upload_record.expires_at.isoformat(),
                "suggestion": "Add ?force_replace=true to URL"
            })
    
    # 6. Create new upload if not merging
    if not should_merge:
        upload_id = TTLManager.create_upload_record(
            db=db,
            user_id=user_id,
            filename=file.filename,
            txn_count=0,
            cust_count=len(valid_records),
            schema_snapshot={"columns": list(df.columns)},
            ttl_hours=48
        )
        expires_at = TTLManager.set_expiry(48)
    
    # VALIDATION: Ensure upload_id is set
    if not upload_id:
        raise HTTPException(500, "Failed to create upload record")
    
    # 7. Add TTL fields to records
    for record in valid_records:
        record['upload_id'] = upload_id
        record['expires_at'] = expires_at
    
    for account in extracted_accounts:
        account['upload_id'] = upload_id
        account['expires_at'] = expires_at
    
    # 8. Insert data
    try:
        print(f"[UPLOAD] Upserting {len(valid_records)} customers...")
        
        # Use RAW psycopg2 cursor (bypasses SQLAlchemy parameter conversion)
        connection = db.connection().connection  # Get raw psycopg2 connection
        cursor = connection.cursor()
        
        batch_size = 500
        for i in range(0, len(valid_records), batch_size):
            batch = valid_records[i:i+batch_size]
            placeholders = []
            values = []
            
            for record in batch:
                placeholders.append("(%s, %s::uuid, %s::jsonb, %s, %s)")
                values.extend([
                    record['customer_id'],
                    str(record['upload_id']),
                    json.dumps(record['raw_data']),
                    record['expires_at'],
                    record.get('created_at', datetime.now(timezone.utc))
                ])
            
            sql = f"""
                INSERT INTO customers (customer_id, upload_id, raw_data, expires_at, created_at)
                VALUES {','.join(placeholders)}
                ON CONFLICT (customer_id, upload_id) 
                DO UPDATE SET
                    raw_data = EXCLUDED.raw_data,
                    expires_at = EXCLUDED.expires_at,
                    created_at = EXCLUDED.created_at
            """
            cursor.execute(sql, values)
            print(f"[UPLOAD] Processed {min(i+batch_size, len(valid_records))}/{len(valid_records)} customers")
        
        cursor.close()
        print(f"[UPLOAD] Upserted {len(valid_records)} customers")
        
        # Insert accounts
        if extracted_accounts:
            print(f"[UPLOAD] Upserting {len(extracted_accounts)} accounts...")
            cursor = db.connection().connection.cursor()
            
            batch_size = 500
            for i in range(0, len(extracted_accounts), batch_size):
                batch = extracted_accounts[i:i+batch_size]
                placeholders = []
                values = []
                
                for account in batch:
                    placeholders.append("(%s, %s, %s::uuid, %s::jsonb, %s, %s)")
                    values.extend([
                        account['account_id'],
                        account['customer_id'],
                        str(account['upload_id']),
                        json.dumps(account.get('raw_data', {})),
                        account['expires_at'],
                        account.get('created_at', datetime.now(timezone.utc))
                    ])
                
                sql = f"""
                    INSERT INTO accounts (account_id, customer_id, upload_id, raw_data, expires_at, created_at)
                    VALUES {','.join(placeholders)}
                    ON CONFLICT (account_id, upload_id) DO UPDATE SET
                        customer_id = EXCLUDED.customer_id,
                        raw_data = EXCLUDED.raw_data,
                        expires_at = EXCLUDED.expires_at,
                        created_at = EXCLUDED.created_at
                """
                cursor.execute(sql, values)
            
            cursor.close()
            print(f"[UPLOAD] Upserted {len(extracted_accounts)} accounts")
        
        # Save field indices
        print(f"[UPLOAD] Saving {len(computed_index)} field indices...")
        for field_name, data in computed_index.items():
            metadata = data['metadata']
            values = data['values']
            
            # Save metadata
            db_metadata = FieldMetadata(
                upload_id=upload_id,
                table_name='customers',
                **metadata
            )
            db.add(db_metadata)
            
            # Save values
            for val in values:
                db_val = FieldValueIndex(
                    upload_id=upload_id,
                    table_name='customers',
                    field_name=field_name,
                    **val
                )
                db.add(db_val)
        
        db.commit()
        print(f"[UPLOAD] Successfully committed all data")
        
    except Exception as e:
        print(f"[ERROR] Database insertion failed: {str(e)}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(400, f"Database error: {str(e)}")
    
    return {
        "status": "success",
        "records_uploaded": len(valid_records),
        "errors": len(errors),
        "error_sample": errors[:5] if errors else [],
        "upload_id": str(upload_id) if valid_records else None,
        "expires_at": expires_at.isoformat() if valid_records else None,
        "action": "merged" if should_merge else "new_upload"
    }

@router.get("/values")
async def get_field_values(
    field: str,
    search: str = "",
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Returns distinct values for a specific field to power UI autocomplete.
    Queries raw_data JSONB from both Transactions and Customers tables.
    """
    user_id = user_payload.get("sub")
    print(f"[VALUES] Searching field='{field}', search='{search}', user_id='{user_id}'")
    potential_tables = ['transactions', 'customers']

    for table in potential_tables:
        try:
            # Query JSONB raw_data directly (schema-agnostic)
            # SQL Injection Fix: Validate table & use params
            if table not in ['transactions', 'customers']:
                continue

            query = text(f"""
                SELECT DISTINCT t.raw_data ->> :field_name
                FROM {table} t
                JOIN data_uploads du ON t.upload_id = du.upload_id
                WHERE du.user_id = :user_id 
                AND t.raw_data ? :field_name
                AND lower(t.raw_data ->> :field_name) LIKE lower(:search)
                LIMIT 20
            """)
            json_result = db.execute(query, {"field_name": field, "search": f"%{search}%", "user_id": user_id})
            json_values = [row[0] for row in json_result.fetchall() if row[0] is not None]

            print(f"[VALUES] Found {len(json_values)} values in {table}: {json_values}")
            
            if json_values:
                return {"values": json_values}
                
        except Exception as e:
            print(f"[VALUES] Error querying {table}: {e}")
            db.rollback()
            continue
            
    print(f"[VALUES] No values found, returning empty array")
    return {"values": []}

@router.post("/ttl/extend")
async def extend_ttl(
    upload_id: str,
    additional_hours: int = 24,
    db: Session = Depends(get_db)
):
    """
    Extend the TTL for uploaded data.
    """
    success = TTLManager.extend_ttl(db, upload_id, additional_hours)
    
    if not success:
        raise HTTPException(404, "Upload not found")
    
    # Get updated expiry
    result = db.execute(
        text("SELECT expires_at FROM data_uploads WHERE upload_id = :id"),
        {"id": upload_id}
    ).fetchone()
    
    return {
        "status": "success",
        "upload_id": str(upload_id),
        "new_expires_at": result[0].isoformat() if result else None,
        "hours_added": additional_hours
    }


================================================================================
FILE: backend/api/validation.py
================================================================================

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import func, Numeric, String
from typing import List, Any, Union
from database import get_db
from models import Transaction, Customer, DataUpload
from auth import get_current_user
from pydantic import BaseModel, validator


router = APIRouter(prefix="/api/validation", tags=["Validation"])


class FilterItem(BaseModel):
    field: str
    operator: str
    value: Union[str, int, float, List[str], List[int], List[float], None]
    
    @validator('value', pre=True)
    def normalize_value(cls, v, values):
        """Normalize value based on operator."""
        operator = values.get('operator')
        
        # For 'in' and 'not_in', ensure value is a list
        if operator in ['in', 'not_in']:
            if isinstance(v, str):
                return [v]
            elif isinstance(v, list):
                return v
            else:
                return [str(v)] if v is not None else []
        
        # For other operators, ensure single value
        if isinstance(v, list):
            return v[0] if v else None
        
        return v


class FilterValidationRequest(BaseModel):
    filters: List[FilterItem]


@router.post("/filters")
async def validate_filters(
    request: FilterValidationRequest,
    user_payload: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    user_id = user_payload.get("sub")
    
    # Get user's active upload
    upload = db.query(DataUpload).filter(
        DataUpload.user_id == user_id,
        DataUpload.status == 'active'
    ).order_by(DataUpload.upload_timestamp.desc()).first()
    
    if not upload:
        raise HTTPException(404, "No active data upload found")
    
    print(f"[VALIDATION] User: {user_id}, Upload: {upload.upload_id}")
    
    # Start with transactions query
    query = db.query(Transaction).filter(Transaction.upload_id == upload.upload_id)
    
    # DETECT CUSTOMER FIELDS AND JOIN
    customer_fields = {'occupation', 'customer_type', 'annual_income', 'account_type', 'risk_score', 'customer_name'}
    needs_customer_join = False
    
    for filter_item in request.filters:
        if filter_item.field in customer_fields:
            needs_customer_join = True
            break
    
    # JOIN customers table if needed
    if needs_customer_join:
        query = query.join(Customer, Transaction.customer_id == Customer.customer_id)
    
    total_before_filter = query.count()
    print(f"[VALIDATION] Total records: {total_before_filter}")
    print(f"[VALIDATION] Filters: {[f.dict() for f in request.filters]}")
    
    # Apply filters
    for idx, filter_item in enumerate(request.filters):
        field = filter_item.field
        operator = filter_item.operator
        value = filter_item.value
        
        # ✅ USE ->> FOR TEXT EXTRACTION (not ->)
        if field in customer_fields:
            # PostgreSQL: raw_data ->> 'field' returns TEXT (no quotes)
            jsonb_field = func.cast(
                func.jsonb_extract_path_text(Customer.raw_data, field),
                String
            )
            print(f"[FILTER {idx}] Customer field: {field} = {value}")
        else:
            jsonb_field = func.cast(
                func.jsonb_extract_path_text(Transaction.raw_data, field),
                String
            )
            print(f"[FILTER {idx}] Transaction field: {field} = {value}")
        
        # Normalize operator
        operator = operator.lower().replace(' ', '_')
        if operator in ['==', 'equals']: operator = '=='
        elif operator in ['!=', 'not_equals']: operator = '!='
        elif operator in ['in', 'in_list']: operator = 'in'
        elif operator in ['not_in', 'not_in_list']: operator = 'not_in'
        elif operator in ['>', 'greater_than']: operator = '>'
        elif operator in ['<', 'less_than']: operator = '<'
        elif operator in ['>=', 'greater_then_equal']: operator = '>='
        elif operator in ['<=', 'less_than_equal']: operator = '<='

        # Apply operator
        if operator == "==":
            query = query.filter(jsonb_field == str(value))
        elif operator == "!=":
            query = query.filter(jsonb_field != str(value))
        elif operator == "in":
            if isinstance(value, list):
                query = query.filter(jsonb_field.in_([str(v) for v in value]))
            else:
                query = query.filter(jsonb_field == str(value))
        elif operator == "not_in":
            if isinstance(value, list):
                query = query.filter(~jsonb_field.in_([str(v) for v in value]))
            else:
                query = query.filter(jsonb_field != str(value))
        elif operator == ">":
            numeric_field = func.cast(
                func.jsonb_extract_path_text(
                    Customer.raw_data if field in customer_fields else Transaction.raw_data,
                    field
                ),
                Numeric
            )
            query = query.filter(numeric_field > float(value))
        elif operator == "<":
            numeric_field = func.cast(
                func.jsonb_extract_path_text(
                    Customer.raw_data if field in customer_fields else Transaction.raw_data,
                    field
                ),
                Numeric
            )
            query = query.filter(numeric_field < float(value))
        elif operator == ">=":
            numeric_field = func.cast(
                func.jsonb_extract_path_text(
                    Customer.raw_data if field in customer_fields else Transaction.raw_data,
                    field
                ),
                Numeric
            )
            query = query.filter(numeric_field >= float(value))
        elif operator == "<=":
            numeric_field = func.cast(
                func.jsonb_extract_path_text(
                    Customer.raw_data if field in customer_fields else Transaction.raw_data,
                    field
                ),
                Numeric
            )
            query = query.filter(numeric_field <= float(value))
        elif operator == "contains":
            query = query.filter(jsonb_field.ilike(f"%{value}%"))
    
    # ✅ PRINT THE ACTUAL SQL QUERY
    try:
        from sqlalchemy.dialects import postgresql
        compiled_query = query.statement.compile(dialect=postgresql.dialect(), compile_kwargs={"literal_binds": True})
        print(f"[DEBUG] SQL Query:\n{compiled_query}")
    except Exception as e:
        print(f"[DEBUG] Could not compile query: {e}")
    
    # Execute query
    matched_transactions = query.all()
    
    # Count distinct customers
    distinct_customers = len(set(txn.customer_id for txn in matched_transactions if txn.customer_id))
    
    # Get total counts
    total_txns = db.query(Transaction).filter(Transaction.upload_id == upload.upload_id).count()
    total_customers = db.query(Customer).filter(Customer.upload_id == upload.upload_id).count()
    
    print(f"[VALIDATION] Matched: {len(matched_transactions)} txns from {distinct_customers} customers")
    
    return {
        "matched_transactions": len(matched_transactions),
        "matched_customers": distinct_customers,
        "total_transactions": total_txns,
        "total_customers": total_customers,
        "match_percentage": round((len(matched_transactions) / total_txns * 100), 2) if total_txns > 0 else 0
    }


================================================================================
FILE: backend/services/comparison_service.py
================================================================================

"""
Comparison Engine - Analyzes two simulation runs to quantify refinement effectiveness

Purpose:
    Compare Baseline vs Refined runs to prove refinements reduce noise
    without missing high-risk alerts.

Key Metrics:
    - Alert reduction percentage
    - Customer-level granular diff
    - Risk analysis of suppressed alerts
    
Business Value:
    Data-driven refinement approval with quantified trade-offs
"""

from typing import Dict, List, Any
from sqlalchemy.orm import Session
from models import Alert, SimulationRun
import structlog
import uuid

logger = structlog.get_logger("comparison_engine")


class ComparisonEngine:
    """
    Compares two simulation runs (Baseline vs Refined) to quantify
    refinement effectiveness and security trade-offs.
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    def compare_runs(
        self, 
        baseline_run_id: str, 
        refined_run_id: str
    ) -> Dict[str, Any]:
        """
        Main comparison method. Persists results to DB.
        """
        # 0. Check for existing comparison
        from models import SimulationComparison
        existing = self.db.query(SimulationComparison).filter(
            SimulationComparison.base_run_id == baseline_run_id,
            SimulationComparison.challenger_run_id == refined_run_id
        ).first()
        
        if existing and existing.comparison_details:
            logger.info("comparison_cache_hit", comparison_id=existing.comparison_id)
            return existing.comparison_details

        logger.info(
            "comparison_started",
            baseline_run_id=baseline_run_id,
            refined_run_id=refined_run_id
        )
        
        # Step 1: Load raw alert sets
        baseline_alerts = self._load_alerts(baseline_run_id)
        refined_alerts = self._load_alerts(refined_run_id)
        
        # Step 2: Calculate high-level summary
        summary = self._calculate_summary(baseline_alerts, refined_alerts)
        
        # Step 3: Granular customer-level diff
        granular_diff = self._calculate_granular_diff(
            baseline_alerts, 
            refined_alerts
        )
        
        # Step 4: Risk analysis (red-teaming)
        risk_analysis = self._analyze_risk(
            baseline_alerts,
            refined_alerts,
            granular_diff
        )
        
        result_json = {
            "summary": summary,
            "granular_diff": granular_diff,
            "risk_analysis": risk_analysis,
            "metadata": {
                "baseline_run_id": baseline_run_id,
                "refined_run_id": refined_run_id,
                "comparison_type": "customer_centric"
            }
        }
        
        # Persist to DB
        try:
            # Efficiency Score calculation (Simple: % reduction * (1 - risk score/100))
            eff_score = summary['percent_reduction'] * (1 - (risk_analysis['risk_score'] / 100))
            
            comparison_record = SimulationComparison(
                comparison_id=str(uuid.uuid4()),
                base_run_id=baseline_run_id,
                challenger_run_id=refined_run_id,
                alerts_delta=summary['net_change'],
                efficiency_score=eff_score,
                overlap_count=summary['refined_alerts'], # Approximation assuming refined is subset
                comparison_details=result_json
            )
            self.db.add(comparison_record)
            self.db.commit()
        except Exception as e:
            logger.error("comparison_persist_failed", error=str(e))
            self.db.rollback()
        
        return result_json
    
    def _load_alerts(self, run_id: str) -> List[Alert]:
        """
        Load all alerts for a given run.
        
        Args:
            run_id: Simulation run ID
            
        Returns:
            List of Alert objects
        """
        alerts = self.db.query(Alert).filter(
            Alert.run_id == run_id
        ).all()
        
        logger.debug(
            "alerts_loaded",
            run_id=run_id,
            count=len(alerts)
        )
        
        return alerts
    
    def _calculate_summary(
        self, 
        baseline_alerts: List[Alert], 
        refined_alerts: List[Alert]
    ) -> Dict[str, Any]:
        """
        Calculate high-level reduction metrics.
        
        Returns:
            {
                "baseline_alerts": int,
                "refined_alerts": int,
                "net_change": int,
                "percent_reduction": float
            }
        """
        baseline_count = len(baseline_alerts)
        refined_count = len(refined_alerts)
        net_change = baseline_count - refined_count
        
        # Handle edge case: no baseline alerts
        if baseline_count == 0:
            percent_reduction = 0.0
        else:
            percent_reduction = (net_change / baseline_count) * 100
        
        return {
            "baseline_alerts": baseline_count,
            "refined_alerts": refined_count,
            "net_change": net_change,
            "percent_reduction": round(percent_reduction, 2)
        }
    
    def _calculate_granular_diff(
        self,
        baseline_alerts: List[Alert],
        refined_alerts: List[Alert],
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Calculate customer-level granular diff with optimized transaction loading.
        """
        from models import Transaction
        
        # Extract customer IDs
        baseline_customers = set(alert.customer_id for alert in baseline_alerts)
        refined_customers = set(alert.customer_id for alert in refined_alerts)
        removed_customers = baseline_customers - refined_customers
        
        # ✅ BATCH LOAD ALL TRANSACTIONS ONCE
        all_transaction_ids = set()
        for alert in baseline_alerts:
            # Use relationship: alert.alert_transactions (list of AlertTransaction objects)
            if alert.alert_transactions:
                for at in alert.alert_transactions:
                    all_transaction_ids.add(at.transaction_id)
        
        # Single DB query for all transactions
        transactions_map = {}
        if all_transaction_ids:
            transactions = self.db.query(Transaction).filter(
                Transaction.transaction_id.in_(all_transaction_ids)
            ).all()
            
            transactions_map = {
                txn.transaction_id: txn for txn in transactions
            }
        
        # Build granular diff
        granular_diff = []
        
        for customer_id in removed_customers:
            customer_alerts = [
                alert for alert in baseline_alerts 
                if alert.customer_id == customer_id
            ]
            
            alert_count = len(customer_alerts)
            
            # ✅ CALCULATE AMOUNT USING PRE-LOADED TRANSACTIONS
            total_amount = 0.0
            for alert in customer_alerts:
                if alert.alert_transactions:
                    for at in alert.alert_transactions:
                        txn_id = at.transaction_id
                        txn = transactions_map.get(txn_id)
                        if txn:
                            try:
                                amount = txn.raw_data.get('transaction_amount', 0)
                                total_amount += float(amount)
                            except (ValueError, TypeError):
                                pass
            
            # Get highest risk score
            max_risk_score = 0
            for alert in customer_alerts:
                if hasattr(alert, 'risk_score') and alert.risk_score:
                    max_risk_score = max(max_risk_score, alert.risk_score)
                else:
                    severity_map = {
                        'Critical': 90,
                        'High': 75,
                        'Medium': 50,
                        'Low': 25
                    }
                    max_risk_score = max(
                        max_risk_score, 
                        severity_map.get(alert.severity, 50)
                    )
            
            granular_diff.append({
                "customer_id": customer_id,
                "status": "removed",
                "alert_count": alert_count,
                "total_amount": round(total_amount, 2),
                "max_risk_score": round(max_risk_score, 2),
                "scenarios": list(set(
                    alert.scenario_id for alert in customer_alerts 
                    if hasattr(alert, 'scenario_id') and alert.scenario_id
                ))
            })
        
        # Sort by risk score (highest first)
        granular_diff.sort(key=lambda x: x["max_risk_score"], reverse=True)
        
        logger.info(
            "granular_diff_calculated",
            removed_customers=len(removed_customers),
            total_diff=len(granular_diff)
        )
        
        if limit:
            return granular_diff[:limit]
        return granular_diff
    
    def _analyze_risk(
        self,
        baseline_alerts: List[Alert],
        refined_alerts: List[Alert],
        granular_diff: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Analyze risk of suppressed alerts (Red-Teaming).
        
        UPDATED: Per user request, ALL alerts are considered "High Risk".
        We no longer filter by risk_score > 70 for the critical count.
        """
        # Count high-risk suppressions (User Request: All alerts are high risk)
        high_risk_suppressions = len(granular_diff)
        
        # Calculate overall risk score
        if not granular_diff:
            risk_score = 0.0
            risk_level = "SAFE"
        else:
            # Average of top 10 risk scores
            top_risks = sorted(
                [item["max_risk_score"] for item in granular_diff],
                reverse=True
            )[:10]
            risk_score = sum(top_risks) / len(top_risks) if top_risks else 0.0
            
            # Classify risk level - stricter since everything is critical
            if high_risk_suppressions > 0:
                risk_level = "CRITICAL" # Any suppression is critical now
            elif risk_score >= 50:
                risk_level = "DANGEROUS"
            elif risk_score >= 25:
                risk_level = "CAUTION"
            else:
                risk_level = "SAFE"
        
        # Generate sample exploits (top 3 suppressions)
        sample_exploits = []
        for item in granular_diff[:3]:
            # Always include, no score threshold
            sample_exploits.append(
                f"Customer {item['customer_id']}: "
                f"${item['total_amount']:,.0f} suppressed "
                f"(score: {item['max_risk_score']})"
            )
        
        logger.info(
            "risk_analysis_completed",
            risk_score=risk_score,
            risk_level=risk_level,
            high_risk_suppressions=high_risk_suppressions
        )
        
        return {
            "risk_score": round(risk_score, 2),
            "risk_level": risk_level,
            "sample_exploits": sample_exploits,
            "high_risk_suppressions": high_risk_suppressions,
            "total_suppressions": len(granular_diff)
        }
    
    def get_run_metadata(self, run_id: str) -> Dict[str, Any]:
        """
        Get metadata for a simulation run.
        
        Args:
            run_id: Simulation run ID
            
        Returns:
            Run metadata (type, date, scenario count, etc.)
        """
        run = self.db.query(SimulationRun).filter(
            SimulationRun.run_id == run_id
        ).first()
        
        if not run:
            return {}
        
        return {
            "run_id": run.run_id,
            "run_type": run.run_type,
            "created_at": run.created_at.isoformat() if run.created_at else None,
            "total_alerts": run.total_alerts,
            "total_transactions": run.total_transactions,
            "status": run.status
        }


================================================================================
FILE: backend/services/simulation_service.py
================================================================================

from sqlalchemy.orm import Session
import pandas as pd
from datetime import datetime
import uuid
from typing import List, Dict, Set, Optional
from decimal import Decimal


# Models
from models import SimulationRun, Alert, Transaction, Customer, ScenarioConfig, AlertExclusionLog
# Core Engines
from core.universal_engine import UniversalScenarioEngine
from core.config_models import ScenarioConfigModel
from core.field_mapper import apply_field_mappings_to_df


class SimulationService:
    """
    Manages the lifecycle of AML simulation execution.
    
    This service orchestrates the entire simulation workflow:
    1. Creating simulation run records
    2. Loading user-scoped data
    3. Executing scenarios via UniversalScenarioEngine
    4. Saving alerts to database
    5. Updating run status
    
    Supports both database-backed and stateless ("pendrive mode") execution.
    
    Attributes:
        db: SQLAlchemy database session for persistence
    
    Example:
        >>> service = SimulationService(db)
        >>> run = service.create_run('ad_hoc', ['rapid-movement'], user_id='user-123')
        >>> service.execute_run(run.run_id)
    """
    
    def __init__(self, db: Session):
        """
        Initialize the simulation service.
        
        Args:
            db: SQLAlchemy database session
        """
        self.db = db
        
    def create_run(self, run_type: str, scenarios: List[str], user_id: str = None) -> SimulationRun:
        """
        Create a new simulation run record.
        
        Args:
            run_type: Type of run ('ad_hoc', 'scheduled', 'comparison')
            scenarios: List of scenario IDs to execute
            user_id: User who initiated the run (for multi-tenancy)
            
        Returns:
            SimulationRun object with status='pending'
            
        Example:
            >>> run = service.create_run(
            ...     run_type='ad_hoc',
            ...     scenarios=['rapid-movement', 'structuring'],
            ...     user_id='user-abc-123'
            ... )
            >>> print(run.run_id)
            'run-def-456'
        """
        run_id = str(uuid.uuid4())
        run = SimulationRun(
            run_id=run_id,
            user_id=user_id,
            run_type=run_type,
            scenarios_run=scenarios,
            status="pending",
            created_at=datetime.utcnow()
        )
        self.db.add(run)
        self.db.commit()
        return run

    def _flatten_raw_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Extracts raw_data JSONB into DataFrame columns.
        """
        if df.empty or 'raw_data' not in df.columns:
            return df
        
        # Extract raw_data JSONB into columns
        meta_df = pd.json_normalize(df['raw_data'])
        
        if meta_df.empty:
            return df.drop(columns=['raw_data'])
        
        # System columns to keep from database query
        system_cols = ['customer_id', 'transaction_id', 'upload_id', 'created_at', 'expires_at']
        df_system = df[[col for col in system_cols if col in df.columns]]
        
        # Combine: system columns (from DB) + user data (from raw_data JSONB)
        result = pd.concat([df_system, meta_df], axis=1)
        
        # ✅ FIX: Parse date columns
        date_columns = ['transaction_date', 'account_opening_date', 'date_of_birth', 'created_date']
        for col in date_columns:
            if col in result.columns:
                result[col] = pd.to_datetime(result[col], errors='coerce', utc=True)
                # Remove timezone for compatibility
                if result[col].dtype == 'datetime64[ns, UTC]':
                    result[col] = result[col].dt.tz_localize(None)
        
        # ✅ FIX: Parse numeric columns
        numeric_columns = ['transaction_amount', 'balance', 'annual_income', 'risk_score']
        for col in numeric_columns:
            if col in result.columns:
                result[col] = pd.to_numeric(result[col], errors='coerce')
        
        print(f"[DATA_FLATTEN] Loaded {len(meta_df.columns)} fields from raw_data")
        
        return result

    def load_simulation_data(self, user_id: str):
        """
        Load transaction and customer data for a specific user.
        
        Fetches data from the database, scoped to the user via DataUpload join,
        and flattens the raw_data JSONB column into DataFrame columns.
        
        Args:
            user_id: User UUID to scope data loading
            
        Returns:
            Tuple of (customers_df, transactions_df) as pandas DataFrames
            
        Raises:
            ValueError: If user has no uploaded data
            
        Example:
            >>> customers_df, transactions_df = service.load_simulation_data('user-123')
            >>> print(f"Loaded {len(transactions_df)} transactions")
            Loaded 5000 transactions
        """
        from models import DataUpload
        
        # We join with DataUpload to ensure we only load data belonging to the specific user
        customers_query = self.db.query(Customer).join(DataUpload).filter(DataUpload.user_id == user_id)
        transactions_query = self.db.query(Transaction).join(DataUpload).filter(DataUpload.user_id == user_id)
        
        customers_df = pd.read_sql(customers_query.statement, self.db.bind)
        transactions_df = pd.read_sql(transactions_query.statement, self.db.bind)
        
        # Flatten raw_data for both
        customers_df = self._flatten_raw_data(customers_df)
        transactions_df = self._flatten_raw_data(transactions_df)
        
        return customers_df, transactions_df

    def execute_run(self, run_id: str, transactions_df: Optional[pd.DataFrame] = None, customers_df: Optional[pd.DataFrame] = None):
        """
        Execute a simulation run against transaction data.
        """
        run = self.db.query(SimulationRun).filter(SimulationRun.run_id == run_id).first()
        if not run:
            return
        
        # Update Status to Running
        run.status = "running"
        self.db.commit()
        
        try:
            # Check mode: Stateless (Pendrive) VS Database (Chunked)
            if transactions_df is not None and customers_df is not None:
                # --- STATELESS MODE (Existing Logic) ---
                self._run_batch(run, run_id, transactions_df, customers_df)
                
                # Finalize
                run.total_alerts = self.db.query(Alert).filter(Alert.run_id == run_id, Alert.excluded == False).count()
                run.total_transactions = len(transactions_df)
                run.status = "completed"
                run.completed_at = datetime.utcnow()
                run.progress_percentage = 100
                self.db.commit()
            
            else:
                # --- DATABASE MODE (Chunked Execution) ---
                if not run.user_id:
                    raise ValueError(f"Run {run_id} has no associated user_id for data loading")
                
                self._execute_db_run_chunked(run, run_id)
            
        except Exception as e:
            # Global Failure Handler
            print(f"Simulation execution failed: {e}")
            import traceback
            traceback.print_exc()
            self.db.rollback()
            run.status = "failed"
            run.metadata_info = {"error": str(e)}
            self.db.commit()
            raise e

    def _execute_db_run_chunked(self, run, run_id):
        """
        Execute run in chunks by customer to ensure aggregation correctness
        while maintaining low memory footprint.
        """
        from models import DataUpload
        from sqlalchemy import func
        
        # 1. Get all customer IDs for this user
        # We process by customer chunks to ensure "Group By Customer" works correctly
        print(f"Fetching customer list for user {run.user_id}...")
        cust_id_query = self.db.query(Customer.customer_id).join(DataUpload).filter(
            DataUpload.user_id == run.user_id
        )
        all_cust_ids = [r[0] for r in cust_id_query.all()]
        total_custs = len(all_cust_ids)
        
        if total_custs == 0:
            print("No customers found.")
            run.status = "completed"
            run.completed_at = datetime.utcnow()
            run.progress_percentage = 100
            self.db.commit()
            return

        BATCH_SIZE = 1000  # Customers per batch
        total_txns_processed = 0
        total_alerts_saved = 0
        
        print(f"Starting chunked execution for {total_custs} customers in batches of {BATCH_SIZE}")
        
        for i in range(0, total_custs, BATCH_SIZE):
            batch_cust_ids = all_cust_ids[i : i + BATCH_SIZE]
            
            # Load Data for this batch
            batch_customers_df, batch_txns_df = self._load_data_for_customers(run.user_id, batch_cust_ids)
            
            if batch_txns_df.empty:
                continue
                
            # Execute Engine on Batch
            self._run_batch(run, run_id, batch_txns_df, batch_customers_df)
            
            # Update Progress
            total_txns_processed += len(batch_txns_df)
            progress = int(((i + len(batch_cust_ids)) / total_custs) * 100)
            
            # Update run occasionally
            run.progress_percentage = progress
            run.total_transactions = total_txns_processed
            self.db.commit()
            
            # Memory Cleanup
            del batch_customers_df
            del batch_txns_df
            import gc
            gc.collect()

        # Finalize
        run.status = "completed"
        run.completed_at = datetime.utcnow()
        run.total_transactions = total_txns_processed
        run.total_alerts = self.db.query(Alert).filter(Alert.run_id == run_id, Alert.excluded == False).count()
        self.db.commit()

    def _load_data_for_customers(self, user_id: str, customer_ids: List[str]):
        """Load transactions and customer details for a specific list of IDs"""
        from models import DataUpload
        
        # Load Customers
        customers_query = self.db.query(Customer).join(DataUpload).filter(
            DataUpload.user_id == user_id,
            Customer.customer_id.in_(customer_ids)
        )
        customers_df = pd.read_sql(customers_query.statement, self.db.bind)
        customers_df = self._flatten_raw_data(customers_df)
        
        # Load Transactions
        transactions_query = self.db.query(Transaction).join(DataUpload).filter(
            DataUpload.user_id == user_id,
            Transaction.customer_id.in_(customer_ids)
        )
        transactions_df = pd.read_sql(transactions_query.statement, self.db.bind)
        transactions_df = self._flatten_raw_data(transactions_df)
        
        return customers_df, transactions_df

    def _run_batch(self, run, run_id, transactions_df, customers_df):
        """
        Process a single batch of data (in-memory) through the engine.
        Refactored from original execute_run.
        """
        # --- Apply Run-Level Dynamic Mappings ---
        if run.metadata_info and 'field_mappings' in run.metadata_info:
            mappings = run.metadata_info['field_mappings']
            if mappings:
                transactions_df = apply_field_mappings_to_df(transactions_df, mappings)
                customers_df = apply_field_mappings_to_df(customers_df, mappings)

        # --- Apply Date Range Filter ---
        if run.metadata_info and 'date_range' in run.metadata_info:
            date_range = run.metadata_info['date_range']
            start_date = pd.to_datetime(date_range.get('start')).tz_localize(None)
            end_date = pd.to_datetime(date_range.get('end')).tz_localize(None)
            
            if start_date and end_date and not transactions_df.empty:
                if 'transaction_date' in transactions_df.columns:
                    transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'], utc=True).dt.tz_localize(None)
                    mask = (transactions_df['transaction_date'] >= start_date) & (transactions_df['transaction_date'] <= end_date)
                    transactions_df = transactions_df.loc[mask]

        if transactions_df.empty:
            return

        # Initialize Engine
        engine = UniversalScenarioEngine(db_session=self.db)
        all_alerts = []
        scenarios_to_run = run.scenarios_run or []
        
        # Execute Each Scenario
        for scenario_id in scenarios_to_run:
            config_record = self.db.query(ScenarioConfig).filter(ScenarioConfig.scenario_id == scenario_id).first()
            
            if not config_record:
                print(f"[ERROR] Scenario {scenario_id} not found in database!")
                continue
            
            if not config_record.config_json:
                print(f"[ERROR] Scenario {scenario_id} has no config_json!")
                continue
            
            try:

                
                conf_data = config_record.config_json.copy()
                conf_data['scenario_id'] = scenario_id
                conf_data['scenario_name'] = config_record.scenario_name
                
                # Check if aggregation exists
                if 'aggregation' not in conf_data:
                    # Valid error check, but removing noisy debug label
                    print(f"[ERROR] No 'aggregation' key in config_json for {scenario_id}!")
                    continue
                
                if 'threshold' not in conf_data:
                    print(f"[WARN] No 'threshold' key in config_json for {scenario_id}")
                
                scenario_config = ScenarioConfigModel(**conf_data)
                
                # Apply Scenario-Specific Mappings
                current_txns = transactions_df.copy()
                current_cust = customers_df.copy()
                
                if config_record.field_mappings:
                    current_txns = apply_field_mappings_to_df(current_txns, config_record.field_mappings)
                    current_cust = apply_field_mappings_to_df(current_cust, config_record.field_mappings)

                # Run Engine
                alerts = engine.execute(scenario_config, current_txns, current_cust, run_id)
                all_alerts.extend(alerts)
                
            except Exception as e:
                print(f"[ERROR] Failed to execute scenario {scenario_id}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Deduplicate Alerts
        seen_keys = set()
        deduplicated_alerts = []
        
        for alert in all_alerts:
            # Convert Series to dict if needed
            if isinstance(alert, pd.Series):
                alert = alert.to_dict()
            
            a_date = alert.get('alert_date', pd.Timestamp.utcnow())
            if isinstance(a_date, datetime):
                a_date = a_date.date()
            
            key = (str(alert.get('customer_id')), str(a_date), str(alert.get('scenario_id')))
            
            if key not in seen_keys:
                seen_keys.add(key)
                deduplicated_alerts.append(alert)
                
        # Persist Results via Bulk Operations
        alert_mappings = []
        exclusion_mappings = []
        trace_mappings = [] # New: Alert Transactions
        
        # Helper index for lookup (Transactions DF is available as transactions_df)
        # Create a quick ID->Amount and ID->UploadID map for performance
        txn_amount_map = {}
        txn_upload_map = {}
        if not transactions_df.empty:
            if 'transaction_amount' in transactions_df.columns:
                txn_amount_map = transactions_df.set_index('transaction_id')['transaction_amount'].to_dict()
            if 'upload_id' in transactions_df.columns:
                txn_upload_map = transactions_df.set_index('transaction_id')['upload_id'].to_dict()

        from models import AlertTransaction

        for alert_data in deduplicated_alerts:
            is_excluded = alert_data.get('excluded', False)
            alert_id = alert_data.get('alert_id') or str(uuid.uuid4())
            
            # ✅ EXTRACT SCALAR VALUES (handle Series)
            def extract_value(data, key, default=None):
                val = data.get(key, default)
                if isinstance(val, pd.Series):
                    return val.iloc[0] if len(val) > 0 else default
                return val
            
            customer_id = extract_value(alert_data, 'customer_id')
            customer_name = extract_value(alert_data, 'customer_name', 'Unknown')
            scenario_id = extract_value(alert_data, 'scenario_id')
            scenario_name = extract_value(alert_data, 'scenario_name')
            alert_date = extract_value(alert_data, 'alert_date', pd.Timestamp.utcnow())
            risk_score = extract_value(alert_data, 'risk_score', 0)
            
            # Determine risk classification
            if risk_score >= 70: risk_classification = 'HIGH'
            elif risk_score >= 40: risk_classification = 'MEDIUM'
            else: risk_classification = 'LOW'
            
            # Prepare Alert Mapping
            alert_mappings.append({
                "alert_id": alert_id,
                "customer_id": str(customer_id),
                "customer_name": str(customer_name) if customer_name else 'Unknown',
                "scenario_id": str(scenario_id),
                "scenario_name": str(scenario_name),
                "scenario_description": f"Generated by {scenario_name}",
                "alert_date": alert_date,
                "alert_status": 'OPN',
                "trigger_details": alert_data.get('trigger_details'),
                "risk_classification": risk_classification,
                "risk_score": risk_score,
                "run_id": run_id,
                "excluded": is_excluded,
                "exclusion_reason": alert_data.get('exclusion_reason'),
                "assigned_to": None,
                "investigation_status": 'New',
                "updated_at": datetime.utcnow()
            })
            
            # Traceability Logic
            involved_ids = alert_data.get('involved_transactions', [])
            if involved_ids:
                total_val = sum([Decimal(str(txn_amount_map.get(tid, 0))) for tid in involved_ids])
                seq = 1
                for tid in involved_ids:
                    amt = Decimal(str(txn_amount_map.get(tid, 0)))
                    pct = (amt / total_val * 100) if total_val > 0 else 0
                    txn_upload = txn_upload_map.get(tid)
                    
                    if txn_upload:
                        trace_mappings.append({
                            "alert_id": alert_id,
                            "transaction_id": tid,
                            "upload_id": txn_upload,
                            "contribution_percentage": round(pct, 2),
                            "is_primary_trigger": False, 
                            "sequence_order": seq
                        })
                    seq += 1
            
            # Prepared Exclusion Log Mapping
            if is_excluded:
                exclusion_mappings.append({
                    "log_id": str(uuid.uuid4()), 
                    "alert_id": alert_id,
                    "exclusion_reason": alert_data.get('exclusion_reason', 'Unknown'),
                    "rule_id": "Refinement",
                    "risk_flags": alert_data.get('trigger_details', {}),
                    "created_at": datetime.utcnow()
                })

        # Execute Bulk Inserts
        if alert_mappings:
            self.db.bulk_insert_mappings(Alert, alert_mappings)
        
        if trace_mappings:
            self.db.bulk_insert_mappings(AlertTransaction, trace_mappings)
            
        if exclusion_mappings:
            self.db.bulk_insert_mappings(AlertExclusionLog, exclusion_mappings)

        self.db.commit()

    def _execute_single_scenario(self, scenario_config: dict, customer_ids: List[str], upload_id: str, run_id: str, user_id: str):
        """
        Execute a single scenario for a sample of customers.
        Used for previewing scenario logic.
        """
        # 1. Load Data for these customers
        customers_df, transactions_df = self._load_data_for_customers(user_id, customer_ids)
        
        if transactions_df.empty:
            return pd.DataFrame()

        # 2. Prepare Scenario Config
        conf_data = scenario_config.get('config_json', {}).copy()
        conf_data['scenario_id'] = scenario_config.get('scenario_id', 'PREVIEW')
        conf_data['scenario_name'] = scenario_config.get('scenario_name', 'Preview')
        
        # 3. Apply Field Mappings if present in the config
        if 'field_mappings' in scenario_config and scenario_config['field_mappings']:
             transactions_df = apply_field_mappings_to_df(transactions_df, scenario_config['field_mappings'])
             customers_df = apply_field_mappings_to_df(customers_df, scenario_config['field_mappings'])

        scenario_model = ScenarioConfigModel(**conf_data)
        
        # 4. Initialize Engine and Run
        engine = UniversalScenarioEngine(db_session=self.db)
        alerts = engine.execute(scenario_model, transactions_df, customers_df, run_id)
        
        return pd.DataFrame(alerts)


================================================================================
FILE: backend/services/data_ingestion.py
================================================================================

import pandas as pd
import numpy as np
from pydantic import BaseModel, ValidationError, validator
from datetime import datetime
from decimal import Decimal
from typing import Optional, List
import io

# IDs are now mandatory again as they are PKs
class TransactionSchema(BaseModel):
    transaction_id: str
    customer_id: str
    account_number: Optional[str] = None
    transaction_date: Optional[datetime] = None
    transaction_amount: Optional[Decimal] = None
    debit_credit_indicator: Optional[str] = None
    transaction_type: Optional[str] = None
    channel: Optional[str] = None
    transaction_narrative: Optional[str] = None
    beneficiary_name: Optional[str] = None
    beneficiary_bank: Optional[str] = None
    raw_data: Optional[dict] = None

class CustomerSchema(BaseModel):
    customer_id: str
    customer_name: Optional[str] = None
    customer_type: Optional[str] = None
    occupation: Optional[str] = None
    annual_income: Optional[Decimal] = None
    account_type: Optional[str] = None
    risk_score: Optional[int] = 0
    raw_data: Optional[dict] = None

class DataIngestionService:
    def _read_file(self, file_content: bytes, filename: str) -> pd.DataFrame:
        if filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(file_content))
        elif filename.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(io.BytesIO(file_content))
        else:
            raise ValueError("Unsupported file format")
            
        # Robust Cleaning: Replace Inf/-Inf with NaN, then all NaN with None
        df = df.replace([np.inf, -np.inf], np.nan)
        # Cast to object to allow None replacement for numeric columns
        df = df.astype(object)
        df = df.where(pd.notnull(df), None)
        return df

    def process_transactions_csv(self, file_content: bytes, filename: str = "data.csv") -> tuple[List[dict], List[dict], dict]:
        df = self._read_file(file_content, filename)
        
        # Best-effort header mapping
        mapping = {
            'id': 'transaction_id',
            'txn_id': 'transaction_id',
            'transaction_id': 'transaction_id',
            'ref': 'transaction_id',
            'date': 'transaction_date',
            'txn_date': 'transaction_date',
            'transaction_date': 'transaction_date',
            'amount': 'transaction_amount',
            'txn_amt': 'transaction_amount',
            'transaction_amount': 'transaction_amount',
            'cust_id': 'customer_id',
            'customer_id': 'customer_id',
            'client_id': 'customer_id',
            'indicator': 'debit_credit_indicator',
            'dc': 'debit_credit_indicator',
            'd/c': 'debit_credit_indicator',
        }
        
        valid_records = []
        errors = []
        
        for idx, row in enumerate(df.to_dict(orient='records')):
            try:
                # ✅ STEP 1: Build raw_data with ALL original fields
                raw_data = {}
                for k, v in row.items():
                    clean_k = str(k).lower().strip().replace(' ', '_')
                    if v is not None:  # Skip None/NaN
                        # Convert types for JSON serialization
                        if isinstance(v, (np.integer, np.floating)):
                            raw_data[clean_k] = float(v) if isinstance(v, np.floating) else int(v)
                        elif isinstance(v, pd.Timestamp):
                            raw_data[clean_k] = v.isoformat()
                        elif isinstance(v, Decimal):
                            raw_data[clean_k] = float(v)
                        else:
                            raw_data[clean_k] = str(v)
                
                # ✅ STEP 2: Extract required system fields for model validation
                processed_row = {
                    'raw_data': raw_data  # ✅ ALL FIELDS STORED HERE
                }
                
                # Extract required PKs
                for k, v in row.items():
                    clean_k = str(k).lower().strip().replace(' ', '_')
                    target_k = mapping.get(clean_k, clean_k)
                    
                    if target_k == 'transaction_id':
                        processed_row['transaction_id'] = str(v)
                    elif target_k == 'customer_id':
                        processed_row['customer_id'] = str(v)
                
                # Validate required fields exist
                if 'transaction_id' not in processed_row or 'customer_id' not in processed_row:
                    raise ValueError(f"Missing required fields: transaction_id or customer_id")
                
                valid_records.append(processed_row)
                
            except Exception as e:
                errors.append({"row": idx + 2, "error": str(e)})
        
        # Build field index from raw_data
        computed_index = self._build_field_index(valid_records, 'transactions')
        
        return valid_records, errors, computed_index
        
    def process_customers_csv(self, file_content: bytes, filename: str = "data.csv") -> tuple[List[dict], List[dict], dict, List[dict]]:
        df = self._read_file(file_content, filename)
        
        mapping = {
            'cust_id': 'customer_id',
            'customer_id': 'customer_id',
            'client_id': 'customer_id',
            'id': 'customer_id',
        }
        
        valid_records = []
        errors = []
        
        for idx, row in enumerate(df.to_dict(orient='records')):
            try:
                # ✅ Build raw_data with ALL fields
                raw_data = {}
                for k, v in row.items():
                    clean_k = str(k).lower().strip().replace(' ', '_')
                    if v is not None:
                        if isinstance(v, (np.integer, np.floating)):
                            raw_data[clean_k] = float(v) if isinstance(v, np.floating) else int(v)
                        elif isinstance(v, pd.Timestamp):
                            raw_data[clean_k] = v.isoformat()
                        elif isinstance(v, Decimal):
                            raw_data[clean_k] = float(v)
                        else:
                            raw_data[clean_k] = str(v)
                
                # Extract customer_id
                processed_row = {
                    'raw_data': raw_data
                }
                
                for k, v in row.items():
                    clean_k = str(k).lower().strip().replace(' ', '_')
                    target_k = mapping.get(clean_k, clean_k)
                    if target_k == 'customer_id':
                        processed_row['customer_id'] = str(v)
                        break
                
                if 'customer_id' not in processed_row:
                    raise ValueError("Missing customer_id")
                
                valid_records.append(processed_row)
                
            except Exception as e:
                errors.append({"row": idx + 2, "error": str(e)})
        
        computed_index = self._build_field_index(valid_records, 'customers')
        extracted_accounts = self._extract_accounts_from_customers(valid_records)
        
        return valid_records, errors, computed_index, extracted_accounts

    def _extract_accounts_from_customers(self, customer_records: List[dict]) -> List[dict]:
        """
        Generates master Account records from Customer data.
        """
        accounts = []
        import uuid
        from datetime import datetime
        
        for cust in customer_records:
            # We assume 1 customer row = 1 primary account for now
            # In real world, one customer can have multiple accounts (master-detail),
            # but usually the CSV is flattened.
            
            # Generate or derive Account ID
            # If account_number exists in raw_data, use it to seed ID? 
            # ideally we want consistent IDs if re-uploaded.
            # But for now, new UUIDs are safer to avoid collisions unless we have a strict key.
            
            raw = cust.get('raw_data', {})
            acc_num = raw.get('account_number')
            
            # Basic Account Dict
            account_data = {
                "account_id": str(uuid.uuid4()),
                "customer_id": cust['customer_id'],
                "account_number": acc_num, # Might be None
                "account_type": cust.get('account_type', 'Savings'), # Default
                "account_status": 'Active',
                "currency_code": raw.get('currency', 'GBP'),
                "account_open_date": datetime.utcnow(), # Default to now if not provided
                "risk_rating": 'LOW', # Default
                "is_pep": False,
                "current_balance": 0,
                "raw_data": {} # Storing extra account props here
            }
            
            # Try to populate more fields from raw_data if available
            if 'open_date' in raw:
                try: account_data['account_open_date'] = pd.to_datetime(raw['open_date'])
                except: pass
            
            if 'balance' in raw:
                try: account_data['current_balance'] = Decimal(str(raw['balance']))
                except: pass
                
            if 'risk_rating' in raw:
                account_data['risk_rating'] = raw['risk_rating']
                
            if 'status' in raw:
                account_data['account_status'] = raw['status']
                
            accounts.append(account_data)
            
        return accounts

    def _build_field_index(self, records: List[dict], table_name: str) -> dict:
        """
        Extract all unique field values and build searchable index stats.
        Returns a dict structure to be used for creating FieldMetadata and FieldValueIndex.
        """
        from collections import Counter
        
        # Group records by field
        field_values = {}  # {field_name: {value: count}}
        field_samples = {} # {field_name: [sample_values]}
        
        for record in records:
            # We index data from 'raw_data' (dynamic fields) AND specific core columns users filter on.
            # Core columns to index:
            core_cols = []
            if table_name == 'transactions':
                core_cols = ['transaction_type', 'channel', 'debit_credit_indicator', 'beneficiary_bank']
            elif table_name == 'customers':
                core_cols = ['customer_type', 'occupation', 'account_type']
            
            # Combine raw_data and core columns
            data_to_index = record.get('raw_data', {}).copy()
            for col in core_cols:
                if record.get(col):
                    data_to_index[col] = record.get(col)
            
            for field_name, field_value in data_to_index.items():
                if field_name not in field_values:
                    field_values[field_name] = Counter()
                
                # Convert to string for indexing, but keep None as None
                str_value = str(field_value) if field_value is not None else None
                if str_value:
                    field_values[field_name][str_value] += 1
        
        # Build Index Result
        index_result = {}
        total_records = len(records)
        
        for field_name, value_counts in field_values.items():
            field_type = self._infer_field_type(list(value_counts.keys()))
            distinct_count = len(value_counts)
            
            metadata = {
                "field_name": field_name,
                "field_type": field_type,
                "total_records": total_records,
                "non_null_count": sum(value_counts.values()),
                "null_count": total_records - sum(value_counts.values()),
                "distinct_count": distinct_count,
                "recommended_operators": self._get_recommended_operators(field_type),
                "sample_values": list(value_counts.keys())[:10]
            }
            
            values = []
            # Build value index (only if distinct values < 1000)
            if distinct_count < 1000:  # Don't index high-cardinality fields
                for value, count in value_counts.items():
                    percentage = (count / total_records) * 100
                    values.append({
                        "field_value": value,
                        "value_count": count,
                        "value_percentage": round(percentage, 2)
                    })
            
            index_result[field_name] = {
                "metadata": metadata,
                "values": values
            }
            
        return index_result
    
    def _infer_field_type(self, sample_values: List[str]) -> str:
        """Infer field type from values"""
        samples = sample_values[:100]
        if not samples: return 'text'
        
        # Check if all values are numeric
        try:
            [float(v) for v in samples]
            return 'numeric'
        except:
            pass
        
        # Check if all values are dates (simple check)
        # Using pandas to datetime is robust but slow for many checks. 
        # For ingestion, reliability > speed here.
        try:
            # Only checking if *valid* samples parse. If it fails, it's text.
             # We assume if 90% parse, it's date? No, be strict for now.
            [pd.to_datetime(v) for v in samples]
            return 'date'
        except:
            pass
        
        # Check if boolean
        unique_values = set(str(v).lower() for v in samples)
        if unique_values.issubset({'true', 'false', '1', '0', 'yes', 'no'}):
            return 'boolean'
        
        return 'text'
    
    def _get_recommended_operators(self, field_type: str) -> list:
        """Get operators that make sense for this field type"""
        if field_type == 'numeric':
            return ['equals', 'not_equals', 'greater_than', 'less_than', 'greater_or_equal', 'less_or_equal', 'between']
        elif field_type == 'date':
            return ['equals', 'greater_than', 'less_than', 'between']
        elif field_type == 'boolean':
            return ['equals']
        else:  # text
            return ['equals', 'not_equals', 'in', 'contains', 'starts_with', 'ends_with']


================================================================================
FILE: backend/services/__init__.py
================================================================================



================================================================================
FILE: backend/services/data_quality_service.py
================================================================================

from sqlalchemy.orm import Session
from sqlalchemy import func
import pandas as pd
import uuid
from datetime import datetime
from models import DataQualityMetric, Transaction, Customer, DataUpload

class DataQualityService:
    def __init__(self, db: Session):
        self.db = db

    def check_upload_quality(self, upload_id: str) -> dict:
        """
        Runs comprehensive data quality checks on a specific upload.
        """
        # 1. Load Data
        upload = self.db.query(DataUpload).filter(DataUpload.upload_id == upload_id).first()
        if not upload:
            raise ValueError("Upload not found")
        
        # Load raw data into DF for analysis
        # Using raw_data column from transactions
        txns = self.db.query(Transaction.raw_data).filter(Transaction.upload_id == upload_id).all()
        if not txns:
             return {"error": "No transaction data found for analysis"}
             
        df = pd.DataFrame([t[0] for t in txns])
        total_rows = len(df)
        
        if total_rows == 0:
             return {"error": "Empty dataset"}

        # 2. Metric: Completeness (% of non-nulls in critical fields)
        critical_fields = ['transaction_id', 'transaction_amount', 'transaction_date', 'customer_id']
        field_issues = {}
        
        null_counts = df[critical_fields].isnull().sum() if set(critical_fields).issubset(df.columns) else df.isnull().sum()
        total_fields_checked = len(null_counts)
        total_possible_values = total_rows * total_fields_checked
        total_nulls = null_counts.sum()
        
        completeness_score = ((total_possible_values - total_nulls) / total_possible_values) * 100
        
        # 3. Metric: Validity (Negative amounts, future dates)
        invalid_count = 0
        if 'transaction_amount' in df.columns:
            # Check for negative amounts (assuming standard AML data shouldn't have them unless reversals, but let's flag)
            # Convert to numeric first
            nums = pd.to_numeric(df['transaction_amount'], errors='coerce')
            invalid_count += (nums < 0).sum()
            
            # Record field issue
            field_issues['transaction_amount'] = {
                "nulls": int(null_counts.get('transaction_amount', 0)),
                "negatives": int((nums < 0).sum())
            }

        validity_score = ((total_rows - invalid_count) / total_rows) * 100
        if validity_score < 0: validity_score = 0
        
        # 4. Metric: Uniqueness (Duplicate Transaction IDs)
        uniqueness_score = 100.0
        if 'transaction_id' in df.columns:
            dupes = df['transaction_id'].duplicated().sum()
            uniqueness_score = ((total_rows - dupes) / total_rows) * 100
        
        # 5. Persist
        metric_id = str(uuid.uuid4())
        metric = DataQualityMetric(
            metric_id=metric_id,
            upload_id=upload_id,
            completeness_score=completeness_score,
            validity_score=validity_score,
            uniqueness_score=uniqueness_score,
            field_level_issues=field_issues,
            row_level_issues={}, # Placeholder for sample bad rows
            created_at=datetime.utcnow()
        )
        
        # Clear old metrics for this upload if any
        self.db.query(DataQualityMetric).filter(DataQualityMetric.upload_id == upload_id).delete()
        
        self.db.add(metric)
        self.db.commit()
        
        return {
            "metric_id": metric_id,
            "scores": {
                "completeness": completeness_score,
                "validity": validity_score,
                "uniqueness": uniqueness_score
            }
        }


================================================================================
FILE: backend/services/beneficiary_service.py
================================================================================

from sqlalchemy.orm import Session
from sqlalchemy import func
import uuid
from datetime import datetime
from decimal import Decimal
import pandas as pd

from models import Transaction, BeneficiaryHistory

class BeneficiaryService:
    def __init__(self, db: Session):
        self.db = db
        
    def build_history_for_upload(self, upload_id: str):
        """
        Scans transactions for a specific upload and builds/updates beneficiary profiles.
        This is an expensive operation and should be run asynchronously.
        """
        # 1. Fetch all transactions with beneficiaries
        # We need raw_data -> beneficiary_name
        # OR use aggregation queries if possible. 
        # Since raw_data is JSONB, we can query it.
        
        # Aggregate by Beneficiary Name (Normalized)
        # Using Pandas mostly because JSONB aggregation in SQL can be complex to standardize names
        # But for performance on 10L rows, SQL is better.
        # Let's try hybrid: Fetch essential cols
        
        try:
            # Delete existing history for this upload to allow re-runs
            self.db.query(BeneficiaryHistory).filter(BeneficiaryHistory.upload_id == upload_id).delete()
            
            # Fetch Data
            # Note: We rely on 'raw_data' having 'beneficiary_name' or similar
            # Ideally we extract this during ingestion into a column, but if not, we do it here.
            # Assuming schema-agnostic means we trust the mapping in search/indexing
            
            # Let's pull relevant columns from Transaction
            query = self.db.query(Transaction.raw_data, Transaction.created_at).filter(Transaction.upload_id == upload_id)
            
            # Use pandas for heavy lifting of normalization
            df = pd.read_sql(query.statement, self.db.bind)
            
            if df.empty:
                return {"status": "no_data"}
            
            # Extract beneficiary
            def get_ben(row):
                raw = row['raw_data']
                return raw.get('beneficiary_name') or raw.get('beneficiary') or raw.get('counterparty')
                
            def get_amount(row):
                raw = row['raw_data']
                val = raw.get('transaction_amount') or raw.get('amount') or 0
                try: return float(val)
                except: return 0.0
                
            df['beneficiary_name'] = df.apply(get_ben, axis=1)
            df['amount'] = df.apply(get_amount, axis=1)
            df['date'] = pd.to_datetime(df['created_at']) # or transaction_date from raw_data
            
            # Filter nulls
            df = df.dropna(subset=['beneficiary_name'])
            
            # Group by Beneficiary
            # Stats: total_txns, total_amt, first_seen, last_seen, std_dev, avg_days
            stats = df.groupby('beneficiary_name').agg({
                'amount': ['count', 'sum', 'std'],
                'date': ['min', 'max']
            })
            
            # Flatten columns
            stats.columns = ['_'.join(col).strip() for col in stats.columns.values]
            stats = stats.reset_index()
            
            # Prepare Objects
            history_records = []
            for _, row in stats.iterrows():
                name = row['beneficiary_name']
                count = row['amount_count']
                total_amt = row['amount_sum']
                std_dev = row['amount_std'] if not pd.isna(row['amount_std']) else 0
                first = row['date_min']
                last = row['date_max']
                
                # Avg Days between txns (simple heuristic: duration / count-1)
                duration = (last - first).days
                avg_days = 0
                if count > 1:
                    avg_days = duration / (count - 1)
                
                record = {
                    "history_id": str(uuid.uuid4()),
                    "beneficiary_name": str(name)[:255],
                    "upload_id": upload_id,
                    "total_transactions": int(count),
                    "total_amount": Decimal(str(total_amt)),
                    "first_seen": first,
                    "last_seen": last,
                    "std_dev_amount": float(std_dev),
                    "avg_days_between_txns": float(avg_days)
                }
                history_records.append(record)
                
            # Bulk Insert
            if history_records:
                self.db.bulk_insert_mappings(BeneficiaryHistory, history_records)
                self.db.commit()
                
            return {"status": "success", "count": len(history_records)}
            
        except Exception as e:
            self.db.rollback()
            print(f"Beneficiary analysis failed: {e}")
            raise e

